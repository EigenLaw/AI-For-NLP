{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Assignment-02,-Probability-Model-A-First-Look:-An-Introduction-of-Language-Model\" data-toc-modified-id=\"Assignment-02,-Probability-Model-A-First-Look:-An-Introduction-of-Language-Model-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Assignment-02, Probability Model A First Look: An Introduction of Language Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Assignment\" data-toc-modified-id=\"Assignment-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Assignment</a></span></li><li><span><a href=\"#Review-the-course-online-programming-code.\" data-toc-modified-id=\"Review-the-course-online-programming-code.-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Review the course online programming code.</a></span><ul class=\"toc-item\"><li><span><a href=\"#Use-Requests-and-Regular-expression\" data-toc-modified-id=\"Use-Requests-and-Regular-expression-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Use Requests and Regular expression</a></span></li><li><span><a href=\"#Language-Model\" data-toc-modified-id=\"Language-Model-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Language Model</a></span></li><li><span><a href=\"#Get-the-frequences-of-words\" data-toc-modified-id=\"Get-the-frequences-of-words-1.2.3\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>Get the frequences of words</a></span></li><li><span><a href=\"#Unigram-Model\" data-toc-modified-id=\"Unigram-Model-1.2.4\"><span class=\"toc-item-num\">1.2.4&nbsp;&nbsp;</span>Unigram Model</a></span></li><li><span><a href=\"#2-Gram-Model\" data-toc-modified-id=\"2-Gram-Model-1.2.5\"><span class=\"toc-item-num\">1.2.5&nbsp;&nbsp;</span>2-Gram Model</a></span></li><li><span><a href=\"#Review-the-problem-using-2-gram\" data-toc-modified-id=\"Review-the-problem-using-2-gram-1.2.6\"><span class=\"toc-item-num\">1.2.6&nbsp;&nbsp;</span>Review the problem using 2-gram</a></span></li></ul></li><li><span><a href=\"#Review-the-main-points-of-this-lesson.\" data-toc-modified-id=\"Review-the-main-points-of-this-lesson.-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Review the main points of this lesson.</a></span></li><li><span><a href=\"#Using-Wikipedia-dataset-to-finish-the-language-model.\" data-toc-modified-id=\"Using-Wikipedia-dataset-to-finish-the-language-model.-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Using Wikipedia dataset to finish the language model.</a></span><ul class=\"toc-item\"><li><span><a href=\"#Read-all-separated-wiki-files\" data-toc-modified-id=\"Read-all-separated-wiki-files-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>Read all separated wiki files</a></span></li><li><span><a href=\"#繁体转简体并分词\" data-toc-modified-id=\"繁体转简体并分词-1.4.2\"><span class=\"toc-item-num\">1.4.2&nbsp;&nbsp;</span>繁体转简体并分词</a></span></li><li><span><a href=\"#读取之前保存的分词结果\" data-toc-modified-id=\"读取之前保存的分词结果-1.4.3\"><span class=\"toc-item-num\">1.4.3&nbsp;&nbsp;</span>读取之前保存的分词结果</a></span></li><li><span><a href=\"#Get-the-frequences-of-words\" data-toc-modified-id=\"Get-the-frequences-of-words-1.4.4\"><span class=\"toc-item-num\">1.4.4&nbsp;&nbsp;</span>Get the frequences of words</a></span></li><li><span><a href=\"#Step3---Unigram-Model\" data-toc-modified-id=\"Step3---Unigram-Model-1.4.5\"><span class=\"toc-item-num\">1.4.5&nbsp;&nbsp;</span>Step3 - Unigram Model</a></span></li><li><span><a href=\"#2-Gram-Model\" data-toc-modified-id=\"2-Gram-Model-1.4.6\"><span class=\"toc-item-num\">1.4.6&nbsp;&nbsp;</span>2-Gram Model</a></span></li><li><span><a href=\"#Step4-Test-the-model\" data-toc-modified-id=\"Step4-Test-the-model-1.4.7\"><span class=\"toc-item-num\">1.4.7&nbsp;&nbsp;</span>Step4 Test the model</a></span></li><li><span><a href=\"#Step5\" data-toc-modified-id=\"Step5-1.4.8\"><span class=\"toc-item-num\">1.4.8&nbsp;&nbsp;</span>Step5</a></span></li><li><span><a href=\"#Compared-to-the-previous-learned-parsing-and-pattern-match-problems.-What's-the-advantage-and-disavantage-of-Probability-Based-Methods?\" data-toc-modified-id=\"Compared-to-the-previous-learned-parsing-and-pattern-match-problems.-What's-the-advantage-and-disavantage-of-Probability-Based-Methods?-1.4.9\"><span class=\"toc-item-num\">1.4.9&nbsp;&nbsp;</span>Compared to the previous learned parsing and pattern match problems. What's the advantage and disavantage of Probability Based Methods?</a></span></li></ul></li><li><span><a href=\"#(Optional)--How-to-solve-OOV-problem?\" data-toc-modified-id=\"(Optional)--How-to-solve-OOV-problem?-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>(Optional)  How to solve <em>OOV</em> problem?</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment-02, Probability Model A First Look: An Introduction of Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "1. Review the course online programming code; \n",
    "2. Review the main questions; \n",
    "3. Using wikipedia corpus to build a language model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review the course online programming code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In this part, you should re-code the programming task in our online course.*\n",
    "\n",
    "> \n",
    "> \n",
    "\n",
    "> \n",
    "> \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Requests and Regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2551353482.jpg',\n",
       " 'https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2551249211.jpg',\n",
       " 'https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2552522615.jpg',\n",
       " 'https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2551172384.jpg',\n",
       " 'https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2549234765.jpg']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "url = \"https://movie.douban.com/\"\n",
    "total = requests.get(url).text\n",
    "pattern1 = re.compile(\"https://movie.douban.com/subject/\\d+/\\?from=showing\")\n",
    "pattern2 = re.compile(\"https://img3.doubanio.com/view/photo/s_ratio_poster/public/p\\d+.jpg\")\n",
    "pattern2.findall(total)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2541240741.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 正则表达式的练习在线网址: https://regexone.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of text: 33338928\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def token(string):\n",
    "    \"\"\"\n",
    "    return only words and numbers in string\n",
    "    \"\"\"\n",
    "    return ''.join(re.findall('[\\w|\\d+]',string))\n",
    "\n",
    "mydata = pd.read_csv(\"mydata/sqlResult_1558435.csv\", encoding = \"gb18030\")\n",
    "\n",
    "all_articles = [token(str(a)) for a in mydata.content.tolist()]\n",
    "\n",
    "text =''\n",
    "for a in all_articles:\n",
    "    text += a\n",
    "print('length of text: {}'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "def cut(string): return list(jieba.cut(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEXT = text\n",
    "ALL_TOKENS = cut(TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('ALL_TOKENS.txt','w') as file:\n",
    "#     file.write(str(ALL_TOKENS))\n",
    "# with open('ALL_TOKENS.txt','r') as file:\n",
    "#     ALL_TOKENS=file.read()\n",
    "\n",
    "# ALL_TOKENS=ALL_TOKENS.replace(\"'\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17526006\n",
      "17147550\n"
     ]
    }
   ],
   "source": [
    "all_articles = [token(str(a)) for a in mydata.content.tolist()]\n",
    "\n",
    "valida_tokens = [t for t in ALL_TOKENS if t.strip() and t!= 'n']\n",
    "print(len(ALL_TOKENS))\n",
    "print(len(valida_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the frequences of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "words_count = Counter(valida_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 1, 1: 3, 2: 3, 3: 1})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([1, 1, 2, 2, 2, 3, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('的', 703715),\n",
       " ('在', 263597),\n",
       " ('月', 189330),\n",
       " ('日', 166267),\n",
       " ('新华社', 142462),\n",
       " ('和', 134061),\n",
       " ('年', 123105),\n",
       " ('了', 121939),\n",
       " ('是', 100912),\n",
       " ('１', 88187)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frequences = [f for w, f in words_count.most_common(100)]\n",
    "x = [i for i in range(len(frequences[:100]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x19f03cc0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb610f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(x, frequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1bbadbe0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xd0XOWd//H3d2Y06pLVbdmWZRvj\ngsEFUUw1CQGHUFP2FyAbJ8A65BdOsvvbFJLsSbZllyV7djebkBCHvksoIUAcIPRiiimyMca9YWNZ\nliVbtiSrj+b5/TFjIuwZSZY0HunO53WOj+Y+c+/M954LHz16bnnMOYeIiKQOX7ILEBGR40vBLyKS\nYhT8IiIpRsEvIpJiFPwiIilGwS8ikmIU/CIiKUbBLyKSYhT8IiIpJpDsAmIpLi52lZWVyS5DRGTU\nWLly5T7nXMlA1h2RwV9ZWUl1dXWyyxARGTXMbOdA19VQj4hIilHwi4ikGAW/iEiK6Tf4zewuM6s3\ns7W92v7JzNaY2Woze9bMyuNs2xNdZ7WZLRvOwkVEZHAG0uO/B1h0RNtPnXOnOOfmAk8AP4qzbbtz\nbm703+VDqFNERIZJv8HvnFsONB7R1txrMRvQbC4iIqPEoMf4zewnZrYLuJb4Pf4MM6s2szfN7Mp+\nPm9JdN3qhoaGwZYlIiL9GHTwO+d+6JybCNwP3BRntQrnXBVwDfBfZja1j89b6pyrcs5VlZQM6B6E\no/z3C1t4ZbN+aYiI9GU4rur5LfC5WG8452qjP7cDLwPzhuH74vr1K9t4ZZOCX0SkL4MKfjOb1mvx\ncmBjjHUKzCw9+roYOBtYP5jvG6i8zDRaOroT+RUiIqNev49sMLMHgIVAsZnVAD8GLjGz6UAY2Anc\nGF23CrjROXcDMBP4tZmFifyCucU5l9Dgz80I0NIRSuRXiIiMev0Gv3Pu6hjNd8ZZtxq4Ifr6DeDk\nIVV3jHIz0mjpVI9fRKQvnrpzVz1+EZH+eSz40xT8IiL98FjwB3RyV0SkH54L/mb1+EVE+uSp4M/L\nSKMrFKYz1JPsUkRERixPBX9uRuQiJY3zi4jEp+AXEUkx3gr+9DQAmtt1gldEJB5vBb96/CIi/fJY\n8Ed6/LqkU0QkPo8Fv3r8IiL98VTw50V7/M3q8YuIxOWp4M9Rj19EpF+eCn6/z8gO+hX8IiJ98FTw\ngyZjERHpj+eCX49mFhHpmweDX5OxiIj0ZUDBb2Z3mVm9ma3t1fZPZrbGzFab2bNmVh5n28VmtiX6\nb/FwFR6PevwiIn0baI//HmDREW0/dc6d4pybCzwB/OjIjcyskMgcvWcApwM/NrOCwZfbP03GIiLS\ntwEFv3NuOdB4RFtzr8VswMXY9GLgOedco3PuAPAcR/8CGVaajEVEpG/9TrbeFzP7CfBloAm4IMYq\n44FdvZZrom0Jo8lYRET6NqSTu865HzrnJgL3AzfFWMVibRbrs8xsiZlVm1l1Q0PDoGs6PBlLR7cm\nYxERiWW4rur5LfC5GO01wMReyxOA2lgf4Jxb6pyrcs5VlZSUDLoQPa9HRKRvgw5+M5vWa/FyYGOM\n1Z4BLjKzguhJ3YuibQnz5+DXOL+ISCwDGuM3sweAhUCxmdUQuVLnEjObDoSBncCN0XWrgBudczc4\n5xrN7J+Ad6If9Y/OucajvmAYHZ6MRT1+EZHYBhT8zrmrYzTfGWfdauCGXst3AXcNqrpB0FCPiEjf\nPHnnLmioR0QkHg8Gv3r8IiJ98VzwazIWEZG+eS74NRmLiEjfPBf8moxFRKRvngt+OPygNg31iIjE\n4sngz8vUo5lFROLxZPBrMhYRkfg8Gvzq8YuIxOPR4NdkLCIi8Xg0+AM0t2uoR0QkFs8Gv3r8IiKx\neTL48zLS6OrRZCwiIrF4Mvj1vB4Rkfg8Hvwa5xcROZI3g1+TsYiIxOXN4NdQj4hIXB4Nfk3GIiIS\nT7/Bb2Z3mVm9ma3t1fZTM9toZmvM7DEzGxNn2x1m9r6ZrTaz6uEsvC/q8YuIxDeQHv89wKIj2p4D\nZjvnTgE2A9/vY/sLnHNznXNVgyvx2GkyFhGR+PoNfufccqDxiLZnnXOHu9NvAhMSUNugaTIWEZH4\nhmOM/zrgT3Hec8CzZrbSzJYMw3cNiCZjERGJLzCUjc3sh0AIuD/OKmc752rNrBR4zsw2Rv+CiPVZ\nS4AlABUVFUMpC4AxWUEaWzuH/DkiIl4z6B6/mS0GLgWudc65WOs452qjP+uBx4DT432ec26pc67K\nOVdVUlIy2LI+MqUkmy31h4b8OSIiXjOo4DezRcD3gMudc21x1sk2s9zDr4GLgLWx1k2EGWNz2VJ/\niJ5wzN9JIiIpayCXcz4ArACmm1mNmV0P/ALIJTJ8s9rMbo+uW25mT0U3LQNeM7P3gLeBJ51zTydk\nL2KYPjaPrlCYHftbj9dXioiMCv2O8Tvnro7RfGecdWuBS6KvtwNzhlTdEEwvywVgU10LU0tyklWG\niMiI48k7dwGmleXgs0jwi4jIn3k2+DPS/FQWZSv4RUSO4NngBzixLJdNexX8IiK9eTr4p4/NZcf+\nVtq7NBOXiMhhng7+GWNzcQ626np+EZGPeDr4TxwbubJnY11zkisRERk5PB38lUXZpAd8OsErItKL\np4Pf7zOmleXoBK+ISC+eDn6A6WV56vGLiPTi/eAfm0N9SycHWruSXYqIyIiQAsGfB6DhHhGRKM8H\n/4yxf35mj4iIpEDwl+amk5+Zpks6RUSiPB/8ZsZplYU8t34vHd26g1dExPPBD/DVsyvZd6iLZatr\nk12KiEjSpUTwnzW1iBljc7njte3EmSVSRCRlpETwmxk3nDuFzXsP8eqWfckuR0QkqQYy9eJdZlZv\nZmt7tf3UzDaa2Roze8zMxsTZdpGZbTKzrWZ283AWfqwumzOOktx07nztg2SWISKSdAPp8d8DLDqi\n7TlgtnPuFGAz8P0jNzIzP3Ab8GlgFnC1mc0aUrVDkB7ws3jBJF7Z3MBmXdMvIims3+B3zi0HGo9o\ne9Y5F4ouvglMiLHp6cBW59x251wX8CBwxRDrHZJrzphEesDHXer1i0gKG44x/uuAP8VoHw/s6rVc\nE21LmsLsIFfMLWfZe7W0dob630BExIOGFPxm9kMgBNwf6+0YbXEvqTGzJWZWbWbVDQ0NQymrT1+o\nmkhbVw9/WluXsO8QERnJBh38ZrYYuBS41sW+RrIGmNhreQIQ90J659xS51yVc66qpKRksGX1q2pS\nAZOKsnhk5a7+VxYR8aBBBb+ZLQK+B1zunGuLs9o7wDQzm2xmQeCLwLLBlTl8zIzPz5/Am9sb2dUY\nr3QREe8ayOWcDwArgOlmVmNm1wO/AHKB58xstZndHl233MyeAoie/L0JeAbYADzsnFuXoP04JlfN\nj5xqeHTV7iRXIiJy/NlIvJO1qqrKVVdXJ/Q7rvnNm9QcaOeV7yzELNbpCBGR0cPMVjrnqgaybkrc\nuRvL50+dwIeNbbyz40CySxEROa5SNvgXzR5LdtDPQ+/oJK+IpJaUDf6sYIDPnzqB36+q4eFqhb+I\npI5AsgtIph98Zibb97Vy8+/XkBX0c+kp5ckuSUQk4VK2xw+R5/cs/csqqiYV8tcPruaFDXuTXZKI\nSMKldPADZAb93PmVKmaV53HTb99l5/7WZJckIpJQKR/8ALkZadz+pVMJ+Izv/G4NPeGRd4mriMhw\nUfBHlY/J5MeXn8TbOxq5+3U9vVNEvEvB38vn5o/nwpml3PrMJrbW65n9IuJNCv5ezIx/+ezJZAX9\n3HBvNT95cj0PV+/SxC0i4ikpfTlnLKW5Gfzsi/O45U8buXfFTrpCYQI+442bP0FpXkayyxMRGTIF\nfwznn1jC+SeW0BN2vPXBfq75zVs8v6Gea86oSHZpIiJDpqGePvh9xoIpRVQUZvG8rvEXEY9Q8PfD\nzLhwZhmvbd1HW5emaxSR0U/BPwAXziqlKxTm1S37kl2KiMiQKfgH4LTKQvIyAjy/XsM9IjL6KfgH\nIM3v44IZpby4sV539YrIqKfgH6ALZ5axv7WLdz/UxC0iMroNZM7du8ys3szW9mr7gpmtM7OwmcWd\n6svMdpjZ+9F5eRM7l2KCnT+9hDS/8Zyu7hGRUW4gPf57gEVHtK0FPgssH8D2Fzjn5g50LsiRKi8j\njTOnFGmcX0RGvX6D3zm3HGg8om2Dc25TwqoaoS6cWca2hlY21jUnuxQRkUFL9Bi/A541s5VmtiTB\n35Vwl80pJz3g474VO5NdiojIoCU6+M92zs0HPg18w8zOi7eimS0xs2ozq25oaEhwWYNTmB3kyrnj\neXRVDU1t3ckuR0RkUBIa/M652ujPeuAx4PQ+1l3qnKtyzlWVlJQksqwhWXxWJR3dYR6q/jDZpYiI\nDErCgt/Mss0s9/Br4CIiJ4VHtVnleZw+uZD7VuzUNf0iMioN5HLOB4AVwHQzqzGz683sKjOrARYA\nT5rZM9F1y83sqeimZcBrZvYe8DbwpHPu6cTsxvH11bMqqTnQrsnZRWRU6vexzM65q+O89ViMdWuB\nS6KvtwNzhlTdCPWpWWWU52dwzxs7uOiksckuR0TkmOjO3UEI+H18acEk3ti2n1++vJVQTzjZJYmI\nDJiCf5AWL6jk4pPKuPXpTVz5y9dZX6tr+0VkdFDwD1J2eoDbv3Qqv7x2PnVNHVz2i9f4yt1v8+iq\nGg516rn9IjJymXMj78qUqqoqV109eh7tc7Cti18v386y1bXsPthORpqPe756OmdOKUp2aSKSIsxs\n5UAfjaMe/zAYkxXke4tm8Op3L+D3X19AUXY6//b0RkbiL1UREQX/MPL5jFMnFfL1hVN598ODvLFt\nf7JLEhE5ioI/AT5/6gTK8tL5+Ytbkl2KiMhRFPwJkJHmZ8l5U3lzeyPv7GjsfwMRkeNIwZ8g15xe\nQVF2kJ+/uDXZpYiIfIyCP0Eyg35uOHcKyzc38Mrmkfm0URFJTQr+BPrLBZOYVJTFV+5+m3/44zra\nunR9v4gkX7/P6pHBy0kP8OQ3z+XWpzdy9+s7eH7DXv7i1ImcUJrDCaU5TCrKJhjQ714ROb50A9dx\n8tb2/fx42To21rV81BbwGRVFWUwtyeG6syezYKpu+BKRwTmWG7gU/MdZa2eI7Q2tbG1oYVt9K9sa\nDvHOjgP4ffDKdy4gI82f7BJFZBQ6luDXUM9xlp0e4OQJ+Zw8If+jtje27uOaO97id9W7+MsFlckr\nTkRSggaYR4AFU4uomlTAL1/eRmeoJ9nliIjHKfhHADPjm5+cxp6mDh5ZWZPsckTE4xT8I8S504qZ\nO3EMv3xpG10hTewiIokzkDl37zKzejNb26vtC2a2zszCZhb3ZIKZLTKzTWa21cxuHq6ivcjM+NYn\np7H7YDv//uwmnllXx6tbGtiwp5mObg3/iMjwGcjJ3XuAXwD39WpbC3wW+HW8jczMD9wGfAqoAd4x\ns2XOufWDrtbjFk4v4YzJhSxdvv1j7T6DiYVZnH9iCT/8zEzSA7ryR0QGbyCTrS83s8oj2jZApJfa\nh9OBrdFJ1zGzB4ErAAV/HGbG/95wBh82ttHe1UNbVw97mzvYUn+ITXXN3LdiJ5v3trD0y1XkZaQl\nu1wRGaUSeTnneGBXr+Ua4Ix4K5vZEmAJQEVFRQLLGtnS/D6mluTEfO/xd3fz7d+9x1/cvoJ7rzud\nsryM41ydiHhBIoM/1p8Dce8Wc84tBZZC5AauRBU1ml05bzyF2UFu/N+VnH3Li+RmBMgKBijOTefi\nk8q47JRyJhZmJbtMERnhEhn8NcDEXssTgNoEfl9KOO/EEn7/9bN4fPVu2jojw0HbGg5x69ObuPXp\nTZxWWcC/fvZkTijNTXapIjJCJTL43wGmmdlkYDfwReCaBH5fypg5Lo+Z4/I+1rarsY0n1uzhjle3\nc/kvXucnV83mqnkTklShiIxkA7mc8wFgBTDdzGrM7Hozu8rMaoAFwJNm9kx03XIzewrAORcCbgKe\nATYADzvn1iVqR1LdxMIsvr5wKk9961xmj8/nbx56j+8/ukaXgorIUfSQNg8K9YT5z+c3c9tL25gx\nNpfbrp0f94SxiHjDsTykTXfuelDA7+M7F8/gnq+ext7mDi77+Ws8uqqGUI/uCBYR9fg9b09TO998\n4F3e2XGArKCfk8fnM39SAZ+dN55pZToBLOIVeh6/fEyoJ8xTa+tYtfMA7+46yPraJrp7HOefWML1\n50zm3GnF/d2MJyIjnIJf+tTY2sVv39rJvSt20tDSybVnVPDPV85W+IuMYhrjlz4VZge56RPTeO17\nF/BX507m/rc+5O+XrWMkdgJEZPhpBq4Ulh7w84NLZgLwm1c/wOczfnTpLPX8RTxOwZ/izIwfXDKT\nUNhx9+s7uPv1Hfh9RprfuHBmGX/3mVmMzdczgUS8RMEvmEV6+rPL8/mwsY1QOMzBtm4eWVnDSxvr\n+ZtPncjisypJ82tkUMQLFPwCRML/c6d+/BEPXztvKn//x3X885Mb2Ly3hVs/PydJ1YnIcFIXTuKq\nKMrizsVVfPXsSh5ZWcPW+kPJLklEhoGCX/pkZtx0wQlkpPn52Qtbkl2OiAwDBb/0qygnna+cVckT\na2rZVNeS7HJEZIgU/DIgf3XuFLKDAf7r+c3JLkVEhkjBLwNSkB3kunMm86e1dayrbUp2OSIyBAp+\nGbDrz5lMbkaAb/9uDbUH25NdjogMkoJfBiw/M43//uI8djW2cdnPX+PN7fuTXZKIDIKCX47JBTNK\nefwbZ5Oflca1d7zF0uXb6AnrGT8io8lApl68y8zqzWxtr7ZCM3vOzLZEfxbE2bbHzFZH/y0bzsIl\neU4ozeEP3zibC2eW8i9PbeTK217n/RqN+4uMFv0+ltnMzgMOAfc552ZH224FGp1zt5jZzUCBc+57\nMbY95Jw75jn/9Fjm0cE5x1Pv1/H3f1zH/kOdXDannKrKQk4Zn8/0sblkpPmTXaJIyjiWxzL3+8gG\n59xyM6s8ovkKYGH09b3Ay8BRwS/eZmZ85pRxnDOtmP94dhPL3qvlD6tro+/BuLwMJhZmMS4/gzS/\nj4DfR1F2kKvPqGD8mMwkVy+SugY0EUs0+J/o1eM/6Jwb0+v9A865o4Z7zCwErAZCwC3OuccHUpR6\n/KOTc47dB9t5v6aJjXUt7Gps48PGNupbOgn1hOnqcRxo68Jn8Ln5E7jx/KlUFmcnu2wRTxjWHv8Q\nVTjnas1sCvCimb3vnNsWa0UzWwIsAaioqEhwWZIIZsaEgiwmFGTx6ZPHxVxn98F2bn95Gw9V7+LB\nd3ZRmpvOzHF5nFSex+Vzy5kxNu84Vy2Segbb498ELHTO7TGzccDLzrnp/XzGPdHPeKS/71OP3/v2\nNnfwx/dqWb+nmY17Wti8t4VQ2DG/YgzXnDGJq+aNx+/ThDAiA3U8evzLgMXALdGff4hRRAHQ5pzr\nNLNi4Gzg1kF+n3hMWV4GN5w75aPlxtYuHl1Vw2/f/pBv/+49tuxt4fvR2cFEZHgN5HLOB4AVwHQz\nqzGz64kE/qfMbAvwqegyZlZlZndEN50JVJvZe8BLRMb41ydiJ2T0K8wOcsO5U3jh/53PtWdU8Ovl\n23liTW2yyxLxpAEN9RxvGupJbV2hMFf/5k3W1zbz2DfO0ri/yAAcy1CP7tyVEScY8PGra+eTmxFg\nyX0rOdDaleySRDxFwS8jUmleBr/60qnUNXXwxaVvUtfUkeySRDxDwS8j1qmTCrjnutPYfbCdz/3q\nDbY1aOpHkeGg4JcR7aypxTy45Ew6unv4wu0ruPO1D9hU18JIPDclMlro5K6MCh/sa+X/3r+KDXua\nASjJTWfhiSVcfNJYzplWrOcCSco7lpO7Cn4ZVWoOtPHG1v0s39LAK5saaOkMkRX0U1mUTXa6n6xg\ngKklOXxiRimnTy4kGNAftZIaFPySErpCYd7cvp/nN+yl9mAHbV0hDnWG2FjXQlcoTHbQz2Vzyvnb\ni6ZTkpue7HJFEmokPatHJGGCAR/nnVjCeSeWfKy9rSvEG1v38+z6On6/qoYn1+zhWxdO48sLKvUX\ngAjq8YvHbW84xD8+sZ6XNzXg9xlZaX4ygn7Gj8nksjnlXDZnHKW5GckuU2TINNQjcoSXN9Xz9geN\ndHSHae8O8f7uJtbubsZnkctGK4uyqSjM4oTSHM6ZVkxuRlqySxY5JhrqETnCwumlLJxe+rG2rfUt\nPP5uLSu27+eVzQ3Ut3QCEPT7OGdaMRfMKGVsXgaF2WkUZAUpyk4nLzOAmZ4aKqObgl9S1gmluXz7\n4j8/Tbyju4c1NU08s66Op9fW8eLG+qO2CfiMguwgmWl+MtJ8ZAYDkb8USnKYUpLN+IJMxuZlUJqb\nTsCv8wkyMmmoRyQG5xw1B9o50NZFY2sXB9q62H+oi/2tXRxo7aK9u4eO7h5aO3vYsb+V3Qfb6f2/\nks9gzsQxfGpWGRfNKmNycY7mF5CE0hi/yHHW3hX5BbCnqZ26pk5qDrTx6pZ9vL+76aN1Aj4jPeAj\nOz1AXmYa+ZlpjMlMoyA7SEFWGjnpaaQFjKDfR0luOmdOKaIsTyeeZWAU/CIjxJ6mdl7a2EBDSyed\noR46Q2FaO0M0tXfT1N7Nwbbuj/6q6AyFj9p+akk2Z59QzAXTS1kwtUh3KEtcCn6RUagn7OjuCdPd\nE2bn/jZWbNvP69v28db2Rtq7e0gP+JhfUUBZXjrFOemU5WVQWZzN5OLIFUm6RyG1KfhFPKSju4e3\nP2jkxY31rN51kP2tnexriZxnOCzgM04qz2NeRQFVlQWcO62E/ExdkppKFPwiKaCprZsP9rfywb5D\nbKo7xKoPD7Cm5iAd3WECPmPB1CIunFnGtNIcysdkMjY/Q0NFHjbs1/Gb2V3ApUC9c252tK0QeAio\nBHYAf+GcOxBj28XA30UX/9k5d+9AvlNE+paflcbcrDHMnTjmo7bunjBrapp4dn0dz67by4+XrfvY\nNkXZQcbmZzAuP5MLZpRw+Zxy3ayWggbU4zez84BDwH29gv9WoNE5d4uZ3QwUOOe+d8R2hUA1UAU4\nYCVwaqxfEL2pxy8ydIcvSd11oI09BzuoPdjOnuYO9hxsZ8f+Nj7Y10pW0M/lc8q5at54qioLdcnp\nKDbsPX7n3HIzqzyi+QpgYfT1vcDLwPeOWOdi4DnnXGO0sOeARcADA/leERk8M2NiYRYTC7OOes85\nx+pdB3ng7Q/5w+paHnxnF8U56Vx8UhlzJ46hNC+Dsrx0CrKC5KQHyAr6dceyhwzlzt0y59weAOfc\nHjMrjbHOeGBXr+WaaJuIJJGZMa+igHkVBfzospN4cWM9T6/dw6OrdnP/Wx/GWD/yKAufGWbgM8Nn\n4PMZAZ/vo7kQinOCnDqpgNMqC5lXMYasoB4OMBIl+qjE6iLEHFsysyXAEoCKiopE1iQiveSkB7h8\nTjmXzymnM9TD3qZO9rZ0sLe5g6b2bg51ROY56AqFcUA47Ag7CDuHc46unjBtXZG7mGsOtPGzF7bg\nHKQHfFw5dzyLz6pkVnlesndTehlK8O81s3HR3v444OgHm0R6+At7LU8gMiR0FOfcUmApRMb4h1CX\niAxSesBPRVEWFUVHDw8NVHNHN6t2HuCZdXU89u5uHqrexZwJ+UwpyaE0L52SnHQyg37SA5HnHRVk\nBSnMDlKUHSQ/K430gK48SrQBX84ZHeN/otfJ3Z8C+3ud3C10zn33iG0KiZzQnR9tWkXk5G5jX9+l\nk7si3tDU1s3D1bt4Zl0ddc0d1Dd30tVz9B3KvWWm+cnPTCPgt4+15WQEyM1I4xPTS7jmjEm6Ye0I\nw34dv5k9QKTnXgzsBX4MPA48DFQAHwJfcM41mlkVcKNz7obottcBP4h+1E+cc3f3930KfhFvcs7R\n3B6iI9RDZ3eY9u6ejx5Zsb+1i6a2ro8eZdFzOJsctHf3cKgzRH1zJ5v2tlBZlMV3F83g07PH6qRz\nlG7gEhFPcs7x8uYG/vWpDWzee+hj7507rZhfXD2f/KzUvC9BwS8inhbqCfPHNbXs2NeGA9o6Q9y7\nYgcVhVnc89XTY17C6nWagUtEPC3g93HVvAkfa/vkzDK+9j/VXPXL1/m7z8wiPyuNoN9HRpqP/Mw0\n8jIij8BO0wQ56vGLiHdsrW/hK3e/Q82B9pjvBwM+5leMYcGUYk6ZkE9Gmp9gwMhJT2NqSfaonjVN\nQz0ikrLau3rY1nCI7p4wobCjrauHpvZumtu7+WBfK29u38/6Pc0cGX0ZaT5ml+czZ+IY5kwcw7yJ\nY5hQkDlqTh5rqEdEUlZm0M/s8fl9rtPU1s2W+ha6esJ09zgOtHaxpqaJ92oO8r9v7uTO1z4AoDgn\nnUtPGcf/OW0iM8d55yY09fhFRHrp7gmzqa6F1bsO8sa2fTy/vp6unjCzxuUxNj8Dn0Wm0Jw/qYCL\nTypjQsHIOJGsoR4RkWFyoLWLx97dzdPr6mjrCtEThrauEDv3twEwe3weJ5bmUpgdpDAnSG56gMxg\ngMw0P+kBHwF/ZB7lgN+H3wd+n4+cdD/FOenkZ6YN21CSgl9EJMF27GvlmXV1vLChnt0H22ls/fis\naAOR5jfyMyNXHwUDPkpzM3j4xgWDqkdj/CIiCVZZnM3Xzp/K186f+lFbe1fkDuP2rh7aukN0hyIP\nsQv1hOkJO0JhRygcpqUjRENLJ/sOddHc0U1XKExXKExW8Pg8p0jBLyIyTDKDfjKPU3gPxei9aFVE\nRAZFwS8ikmIU/CIiKUbBLyKSYhT8IiIpRsEvIpJiFPwiIilGwS8ikmJG5CMbzKwB2DnIzYuBfcNY\nzmiQivsMqbnfqbjPkJr7faz7PMk5VzKQFUdk8A+FmVUP9HkVXpGK+wypud+puM+QmvudyH3WUI+I\nSIpR8IuIpBgvBv/SZBeQBKm4z5Ca+52K+wypud8J22fPjfGLiEjfvNjjFxGRPngm+M1skZltMrOt\nZnZzsutJFDObaGYvmdkGM1tnZt+Kthea2XNmtiX6syDZtQ43M/Ob2btm9kR0ebKZvRXd54fMLJjs\nGoebmY0xs0fMbGP0mC/w+rE2s7+J/re91sweMLMMLx5rM7vLzOrNbG2vtpjH1iL+O5pva8xs/lC+\n2xPBb2Z+4Dbg08As4Gozm5VuLcxgAAAC/klEQVTcqhImBPytc24mcCbwjei+3gy84JybBrwQXfaa\nbwEbei3/G/Cf0X0+AFyflKoS62fA0865GcAcIvvv2WNtZuOBbwJVzrnZgB/4It481vcAi45oi3ds\nPw1Mi/5bAvxqKF/sieAHTge2Oue2O+e6gAeBK5JcU0I45/Y451ZFX7cQCYLxRPb33uhq9wJXJqfC\nxDCzCcBngDuiywZ8AngkuooX9zkPOA+4E8A51+WcO4jHjzWRmQEzzSwAZAF78OCxds4tBxqPaI53\nbK8A7nMRbwJjzGzcYL/bK8E/HtjVa7km2uZpZlYJzAPeAsqcc3sg8ssBKE1eZQnxX8B3gXB0uQg4\n6JwLRZe9eMynAA3A3dEhrjvMLBsPH2vn3G7g34EPiQR+E7AS7x/rw+Id22HNOK8Ev8Vo8/TlSmaW\nA/we+GvnXHOy60kkM7sUqHfOrezdHGNVrx3zADAf+JVzbh7QioeGdWKJjmlfAUwGyoFsIsMcR/La\nse7PsP737pXgrwEm9lqeANQmqZaEM7M0IqF/v3Pu0Wjz3sN/+kV/1iervgQ4G7jczHYQGcb7BJG/\nAMZEhwPAm8e8Bqhxzr0VXX6EyC8CLx/rC4EPnHMNzrlu4FHgLLx/rA+Ld2yHNeO8EvzvANOiZ/6D\nRE4GLUtyTQkRHdu+E9jgnPuPXm8tAxZHXy8G/nC8a0sU59z3nXMTnHOVRI7ti865a4GXgM9HV/PU\nPgM45+qAXWY2Pdr0SWA9Hj7WRIZ4zjSzrOh/64f32dPHupd4x3YZ8OXo1T1nAk2Hh4QGxTnniX/A\nJcBmYBvww2TXk8D9PIfIn3hrgNXRf5cQGfN+AdgS/VmY7FoTtP8LgSeir6cAbwNbgd8B6cmuLwH7\nOxeojh7vx4ECrx9r4B+AjcBa4H+AdC8ea+ABIucxuon06K+Pd2yJDPXcFs2394lc9TTo79aduyIi\nKcYrQz0iIjJACn4RkRSj4BcRSTEKfhGRFKPgFxFJMQp+EZEUo+AXEUkxCn4RkRTz/wGP1IB+whTm\nTAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19ed52b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, np.log(frequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.831736895358229e-08"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequences_all = [f for w, f in words_count.most_common()]\n",
    "frequences_sum = sum(frequences_all)\n",
    "1 / frequences_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0011652976664304814"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_prob(word): \n",
    "    esp = 1 / frequences_sum\n",
    "    if word in words_count: \n",
    "        return words_count[word] / frequences_sum\n",
    "    else:\n",
    "        return esp\n",
    "    \n",
    "get_prob('我们')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "def product(numbers):\n",
    "    return reduce(lambda n1, n2: n1 * n2, numbers)\n",
    "\n",
    "product([1, 2, 3, 4, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def language_model_one_gram(string):\n",
    "    words = cut(string)\n",
    "    return product([get_prob(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\CHRIST~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.118 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是一个比较正常的句子 6.951281725653128e-22\n",
      "这个一个比较罕见的句子 1.8027974155493477e-22\n",
      "小明毕业于清华大学 2.73868286524123e-18\n",
      "小明毕业于秦华大学 5.256325018795704e-24\n"
     ]
    }
   ],
   "source": [
    "sentences = \"\"\"\n",
    "这是一个比较正常的句子\n",
    "这个一个比较罕见的句子\n",
    "小明毕业于清华大学\n",
    "小明毕业于秦华大学\n",
    "\"\"\".split()\n",
    "\n",
    "for s in sentences:\n",
    "    print(s, language_model_one_gram(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "明天晚上请你吃大餐，我们一起吃苹果 is more possible\n",
      "---- 今天晚上请你吃大餐，我们一起吃日料 with probility 6.326552988102628e-50\n",
      "---- 明天晚上请你吃大餐，我们一起吃苹果 with probility 5.1598305491460503e-48\n",
      "真是一只好看的小猫 is more possible\n",
      "---- 真事一只好看的小猫 with probility 2.649334414108739e-25\n",
      "---- 真是一只好看的小猫 with probility 1.1304372411181555e-21\n",
      "今晚我去吃火锅 is more possible\n",
      "---- 我去吃火锅，今晚 with probility 6.918567934898121e-26\n",
      "---- 今晚我去吃火锅 with probility 1.1863648959206229e-18\n"
     ]
    }
   ],
   "source": [
    "need_compared = [\n",
    "    \"今天晚上请你吃大餐，我们一起吃日料 明天晚上请你吃大餐，我们一起吃苹果\",\n",
    "    \"真事一只好看的小猫 真是一只好看的小猫\",\n",
    "    \"我去吃火锅，今晚 今晚我去吃火锅\"\n",
    "]\n",
    "\n",
    "for s in need_compared:\n",
    "    s1, s2 = s.split()\n",
    "    p1, p2 = language_model_one_gram(s1), language_model_one_gram(s2)\n",
    "    \n",
    "    better = s1 if p1 > p2 else s2\n",
    "    \n",
    "    print('{} is more possible'.format(better))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s1, p1))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s2, p2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Pr(S) = Pr(w_1\\cdot w_2 \\cdots w_n) \\sim Pr(w_1) \\cdot Pr(w_2 | w_1) \\cdots Pr(w_n | w_{n-1})$$\n",
    "\n",
    "$$ Pr(w_2 | w_1) =  \\frac { Pr(w_1 \\cdot w_2) }{Pr(w_1)} $$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_tokens = [str(t) for t in valida_tokens] \n",
    "all_2_grams_words = [''.join(valid_tokens[i:i+2]) for i in range(len(valid_tokens[:-2]))]\n",
    "_2_gram_sum = len(all_2_grams_words)\n",
    "_2_gram_counter = Counter(all_2_grams_words)\n",
    "\n",
    "def get_combination_prob(w1, w2):\n",
    "    if w1 + w2 in _2_gram_counter: return _2_gram_counter[w1+w2] / _2_gram_sum\n",
    "    else:\n",
    "        return 1 / _2_gram_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8078386484178378e-06"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_combination_prob('去', '北京')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.749521272662424e-07"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_combination_prob( '北京','去')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0007037298498485233\n",
      "0.004363125069060844\n"
     ]
    }
   ],
   "source": [
    "def get_prob_2_gram(w1, w2):\n",
    "    return get_combination_prob(w1, w2) / get_prob(w1)\n",
    "\n",
    "print(get_prob_2_gram('去', '沈阳'))\n",
    "print(get_prob_2_gram('去', '北京'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from icecream import ic\n",
    "#调Bug常用工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def langauge_model_of_2_gram(sentence):\n",
    "    sentence_probability = 1\n",
    "    \n",
    "    words = cut(sentence)\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        if i == 0: \n",
    "            prob = get_prob(word)\n",
    "        else:\n",
    "            previous = words[i-1]\n",
    "            prob = get_prob_2_gram(previous, word)\n",
    "        sentence_probability *= prob\n",
    "    \n",
    "    return sentence_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.119929526793368e-12\n",
      "8.369467292866351e-15\n"
     ]
    }
   ],
   "source": [
    "print(langauge_model_of_2_gram('农民工坐火车去上海'))\n",
    "print(langauge_model_of_2_gram('农民工去上海坐火车'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the problem using 2-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天晚上请你吃大餐，我们一起吃日料 is more possible\n",
      "---- 今天晚上请你吃大餐，我们一起吃日料 with probility 6.733510460213917e-28\n",
      "---- 明天晚上请你吃大餐，我们一起吃苹果 with probility 5.386808368171131e-28\n",
      "真是一只好看的小猫 is more possible\n",
      "---- 真事一只好看的小猫 with probility 1.7745055189894016e-19\n",
      "---- 真是一只好看的小猫 with probility 3.451412831879709e-16\n",
      "今晚我去吃火锅 is more possible\n",
      "---- 今晚我去吃火锅 with probility 6.958414780242525e-14\n",
      "---- 今晚火锅去吃我 with probility 1.4057576096877225e-15\n",
      "养乐多绿来一杯 is more possible\n",
      "---- 洋葱奶昔来一杯 with probility 1.0846719865912803e-12\n",
      "---- 养乐多绿来一杯 with probility 5.8317382557246745e-08\n"
     ]
    }
   ],
   "source": [
    "need_compared = [\n",
    "    \"今天晚上请你吃大餐，我们一起吃日料 明天晚上请你吃大餐，我们一起吃苹果\",\n",
    "    \"真事一只好看的小猫 真是一只好看的小猫\",\n",
    "    \"今晚我去吃火锅 今晚火锅去吃我\",\n",
    "    \"洋葱奶昔来一杯 养乐多绿来一杯\"\n",
    "]\n",
    "\n",
    "for s in need_compared:\n",
    "    s1, s2 = s.split()\n",
    "    p1, p2 = langauge_model_of_2_gram(s1), langauge_model_of_2_gram(s2)\n",
    "    \n",
    "    better = s1 if p1 > p2 else s2\n",
    "    \n",
    "    print('{} is more possible'.format(better))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s1, p1))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s2, p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "grammar = \"\"\"\n",
    "sentence => noun_phrase verb_phrase \n",
    "noun_phrase => Article Adj* noun belong \n",
    "belong => de property\n",
    "de => 的\n",
    "property => 眼睛 | 裙子 | 胳膊 | 尾巴\n",
    "Adj* => null | Adj Adj*\n",
    "verb_phrase => verb noun_phrase\n",
    "Article =>  一个 | 这个\n",
    "noun =>   女人 |  篮球 | 桌子 | 小猫\n",
    "verb => 看着   |  坐在 |  听着 | 看见\n",
    "Adj =>   蓝色的 |  好看的 | 小小的\n",
    "\"\"\"\n",
    "\n",
    "def parse_grammar(grammar_str, sep='=>'):\n",
    "    grammar = {}\n",
    "    for line in grammar_str.split('\\n'): \n",
    "        line = line.strip()\n",
    "        if not line: continue\n",
    "        \n",
    "        target, rules = line.split(sep)\n",
    "        \n",
    "        grammar[target.strip()] = [r.split() for r in rules.split('|')]\n",
    "    \n",
    "    return grammar\n",
    "\n",
    "def gene(grammar_parsed, target='sentence'):\n",
    "    if target not in grammar_parsed: return target\n",
    "    \n",
    "    rule = random.choice(grammar_parsed[target])\n",
    "    return ''.join(gene(grammar_parsed, target=r) for r in rule if r!='null')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['一个女人的眼睛看见一个女人的胳膊',\n",
       " '这个小猫的尾巴坐在一个女人的眼睛',\n",
       " '这个女人的眼睛看着这个桌子的胳膊',\n",
       " '一个女人的胳膊看见一个桌子的裙子',\n",
       " '一个小小的女人的眼睛看见一个女人的胳膊',\n",
       " '这个女人的裙子看见一个篮球的眼睛',\n",
       " '一个女人的尾巴坐在这个桌子的胳膊',\n",
       " '一个桌子的裙子看见一个桌子的眼睛',\n",
       " '这个小小的桌子的尾巴坐在一个女人的眼睛',\n",
       " '这个小猫的眼睛看着这个桌子的胳膊',\n",
       " '这个篮球的胳膊看着这个女人的尾巴',\n",
       " '这个篮球的尾巴看着这个女人的裙子',\n",
       " '这个小猫的眼睛看见这个小小的女人的眼睛',\n",
       " '一个桌子的眼睛坐在这个女人的尾巴',\n",
       " '这个女人的眼睛坐在这个小猫的裙子',\n",
       " '一个小小的女人的胳膊听着一个女人的尾巴',\n",
       " '这个小小的小猫的眼睛看着这个篮球的眼睛',\n",
       " '一个篮球的眼睛看见一个小猫的胳膊',\n",
       " '这个小猫的尾巴坐在这个小猫的胳膊',\n",
       " '一个篮球的胳膊看见一个桌子的裙子',\n",
       " '这个女人的裙子看着一个篮球的尾巴',\n",
       " '一个小猫的眼睛听着一个小猫的眼睛',\n",
       " '一个桌子的胳膊听着一个桌子的胳膊',\n",
       " '一个篮球的胳膊坐在一个小猫的裙子',\n",
       " '一个篮球的裙子坐在一个篮球的胳膊',\n",
       " '一个桌子的眼睛坐在这个篮球的尾巴',\n",
       " '这个篮球的裙子坐在一个篮球的尾巴',\n",
       " '这个女人的眼睛听着这个篮球的尾巴',\n",
       " '这个桌子的胳膊看着一个小小的桌子的裙子',\n",
       " '这个女人的尾巴听着一个小小的篮球的裙子',\n",
       " '一个小猫的眼睛听着这个篮球的裙子',\n",
       " '这个小猫的裙子坐在这个小小的篮球的尾巴',\n",
       " '一个女人的眼睛看着一个好看的女人的胳膊',\n",
       " '这个桌子的眼睛坐在一个蓝色的小猫的眼睛',\n",
       " '一个蓝色的桌子的眼睛听着一个女人的眼睛',\n",
       " '一个女人的胳膊坐在这个蓝色的桌子的尾巴',\n",
       " '一个好看的小猫的胳膊听着一个女人的裙子',\n",
       " '一个桌子的眼睛坐在这个好看的桌子的眼睛',\n",
       " '一个蓝色的桌子的裙子看见这个小猫的胳膊',\n",
       " '这个小小的蓝色的小猫的胳膊看见这个女人的裙子',\n",
       " '这个小小的蓝色的女人的胳膊坐在这个女人的尾巴',\n",
       " '一个桌子的裙子看见一个蓝色的桌子的胳膊',\n",
       " '这个蓝色的桌子的尾巴看着一个女人的尾巴',\n",
       " '一个小猫的裙子坐在这个小小的蓝色的女人的胳膊',\n",
       " '一个小小的小猫的眼睛看着这个蓝色的桌子的胳膊',\n",
       " '这个蓝色的小猫的裙子坐在一个小猫的尾巴',\n",
       " '一个篮球的胳膊听着一个好看的桌子的眼睛',\n",
       " '一个好看的桌子的胳膊听着一个篮球的眼睛',\n",
       " '这个小小的女人的尾巴听着一个小小的小小的篮球的尾巴',\n",
       " '一个小小的好看的篮球的胳膊看见一个篮球的胳膊',\n",
       " '这个蓝色的蓝色的女人的胳膊看着这个女人的眼睛',\n",
       " '这个好看的女人的尾巴听着这个篮球的裙子',\n",
       " '一个小小的好看的篮球的眼睛坐在一个小猫的尾巴',\n",
       " '这个小猫的胳膊听着一个蓝色的小猫的裙子',\n",
       " '一个小猫的胳膊坐在这个小小的蓝色的桌子的尾巴',\n",
       " '一个小小的好看的篮球的尾巴听着一个桌子的裙子',\n",
       " '一个蓝色的小猫的胳膊听着这个小小的桌子的裙子',\n",
       " '这个桌子的眼睛看着这个小小的蓝色的小小的小猫的眼睛',\n",
       " '一个蓝色的篮球的胳膊看见这个蓝色的女人的眼睛',\n",
       " '这个蓝色的小小的桌子的胳膊看见这个篮球的裙子',\n",
       " '这个蓝色的蓝色的篮球的眼睛看见这个女人的裙子',\n",
       " '一个好看的蓝色的桌子的胳膊坐在这个小猫的眼睛',\n",
       " '一个桌子的胳膊看见一个小小的好看的蓝色的小猫的眼睛',\n",
       " '一个桌子的胳膊听着这个小小的小小的蓝色的桌子的胳膊',\n",
       " '一个小小的蓝色的小猫的尾巴坐在一个好看的篮球的眼睛',\n",
       " '一个好看的小猫的眼睛听着一个蓝色的女人的尾巴',\n",
       " '一个蓝色的好看的小猫的裙子看见这个篮球的裙子',\n",
       " '这个蓝色的好看的篮球的胳膊听着这个小猫的胳膊',\n",
       " '这个小猫的裙子听着这个小小的蓝色的好看的女人的裙子',\n",
       " '这个小小的小小的好看的好看的小猫的裙子看见这个女人的尾巴',\n",
       " '一个好看的蓝色的小猫的尾巴看见这个好看的篮球的眼睛',\n",
       " '一个好看的小小的篮球的眼睛看着这个好看的桌子的裙子',\n",
       " '一个好看的小小的女人的眼睛看着一个蓝色的小猫的尾巴',\n",
       " '这个小小的蓝色的小小的女人的眼睛看着一个好看的女人的尾巴',\n",
       " '这个小小的小小的好看的蓝色的桌子的胳膊坐在一个篮球的尾巴',\n",
       " '一个小小的篮球的眼睛坐在这个好看的小小的好看的女人的裙子',\n",
       " '这个好看的篮球的眼睛坐在一个蓝色的小小的篮球的尾巴',\n",
       " '这个桌子的裙子看着这个好看的蓝色的好看的篮球的尾巴',\n",
       " '这个小小的小小的篮球的眼睛看着一个好看的好看的篮球的尾巴',\n",
       " '这个蓝色的蓝色的好看的小猫的胳膊听着这个女人的胳膊',\n",
       " '一个好看的蓝色的好看的女人的裙子听着一个小猫的裙子',\n",
       " '这个小小的蓝色的蓝色的篮球的胳膊坐在一个蓝色的桌子的胳膊',\n",
       " '一个小小的好看的好看的小小的小小的小猫的眼睛看见这个小猫的胳膊',\n",
       " '一个好看的小小的好看的小猫的眼睛坐在一个蓝色的小猫的眼睛',\n",
       " '一个好看的桌子的胳膊看见一个蓝色的小小的蓝色的女人的尾巴',\n",
       " '这个桌子的胳膊听着这个小小的蓝色的蓝色的好看的篮球的胳膊',\n",
       " '这个好看的女人的胳膊看见一个蓝色的蓝色的蓝色的桌子的裙子',\n",
       " '这个小小的小小的好看的蓝色的篮球的尾巴坐在这个好看的桌子的裙子',\n",
       " '一个好看的好看的蓝色的女人的尾巴听着这个蓝色的篮球的胳膊',\n",
       " '一个蓝色的好看的小猫的胳膊看见这个小小的小小的小小的蓝色的桌子的裙子',\n",
       " '一个小猫的眼睛看见这个小小的蓝色的好看的小小的小小的蓝色的小猫的裙子',\n",
       " '一个小小的好看的小小的蓝色的小小的好看的小猫的眼睛看见这个小小的女人的裙子',\n",
       " '一个好看的蓝色的好看的蓝色的女人的裙子坐在一个好看的小猫的胳膊',\n",
       " '这个蓝色的小小的好看的篮球的胳膊看见这个蓝色的好看的小猫的尾巴',\n",
       " '一个小小的好看的蓝色的蓝色的蓝色的蓝色的桌子的尾巴坐在一个小小的蓝色的女人的眼睛',\n",
       " '这个小小的好看的蓝色的好看的蓝色的小小的好看的女人的裙子看见这个好看的女人的裙子',\n",
       " '这个好看的蓝色的好看的小猫的尾巴听着一个蓝色的蓝色的小小的好看的女人的眼睛',\n",
       " '这个蓝色的蓝色的小小的小猫的眼睛听着一个小小的蓝色的蓝色的小小的好看的小小的篮球的尾巴',\n",
       " '一个蓝色的蓝色的蓝色的篮球的眼睛看见一个好看的小小的好看的小小的好看的蓝色的小猫的裙子',\n",
       " '一个蓝色的小小的好看的蓝色的女人的尾巴看见这个小小的好看的好看的蓝色的好看的蓝色的好看的桌子的胳膊']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = parse_grammar(grammar)\n",
    "random_generated = [gene(g) for _ in range(100)]\n",
    "#\"|\" == > 'or'\n",
    "sorted(random_generated, key=langauge_model_of_2_gram, reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review the main points of this lesson. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. How to Github and Why do we use Jupyter and Pycharm; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ans:__\n",
    "\n",
    "* [Official Reference](https://guides.github.com/activities/hello-world/)\n",
    "\n",
    "git init   #在工程文件夹下，对文件初始化\n",
    "\n",
    "git remote add origin https://github.com/EigenLaw/AI-For-NLP   #将本地文件和远端关联\n",
    "\n",
    "git status   #查看本地操作的变化\n",
    "\n",
    "git add *    #将本地更改生效\n",
    "\n",
    "git commit -m ‘first_commit’   #将本地更改生效\n",
    "\n",
    "git push origin master - - force    #将本地更改推送到远端\n",
    "\n",
    "* We can use Jupyter to do anything we want, and Pycharm is more functional as a professional engineer tool.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. What's the Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ans:__\n",
    "\n",
    "[Probability Model - From Wiki Pedia](https://en.wikipedia.org/wiki/Statistical_model)\n",
    "> A probability(statistical) model is a mathematical model that embodies a set of statistical assumptions concerning the generation of sample data (and similar data from a larger population). A statistical model represents, often in considerably idealized form, the data-generating process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Can you came up with some sceneries at which we could use Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ans:__\n",
    "\n",
    "> Y(X)的经典sceneries\n",
    "\n",
    "* X:名义变量 Y:名义变量 - 列联表分析模型 example:吸烟是否会导致癌症\n",
    "\n",
    "* X:名义变量 Y:数值变量 - Dummy Regressional Model example:节假日是否会导致超市销量增加\n",
    "\n",
    "* X:数值变量 Y:名义变量 - Logistic Regressional Model example: 候选人财富值是否会导致选举成功\n",
    "\n",
    "* X:数值变量 Y:数值变量 - (Multi)Linear Regressional Model example: numerous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ans:__\n",
    "\n",
    "[From Wiki Pedia](https://en.m.wikipedia.org/wiki/Probabilistic_context-free_grammar)\n",
    "\n",
    ">(1) Why do we use probability?\n",
    "\n",
    "- aim to understand the structure of natural languages\n",
    "- for large problems it is convenient to learn parameters of probability model via machine learning\n",
    "\n",
    ">(2) What's the difficult points for programming based on parsing and pattern match?\n",
    "\n",
    "- Grammar parsing algorithms have various time and memory requirements\n",
    "- The model has to weigh factors of scalability and generality\n",
    "- Issues such as grammar ambiguity must be resolved and the grammar design affects results accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. What's the Language Model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ans:__\n",
    "\n",
    "[From Wiki Pedia](https://en.m.wikipedia.org/wiki/Language_model)\n",
    "\n",
    "> A statistical language model is a probability distribution over sequences of words. Given such a sequence, say of length m, it assigns a probability $ P(w_{1},w_{2},\\dots ,w_{m}) $ to the whole sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  6. Can you came up with some sceneraies at which we could use Language Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ans:__\n",
    "\n",
    "统计语言模型是一个单词序列上的概率分布，对于一个给定长度为m的序列，它可以为整个序列产生一个概率$ P(w_{1},w_{2},\\dots,w_{m})$。其实就是想办法找到一个概率分布，它可以表示任意一个句子或序列出现的概率。\n",
    "目前在自然语言处理相关应用非常广泛，如语音识别(speech recognition) , 机器翻译(machine translation), 词性标注(part-of-speech tagging), 句法分析(parsing)等。\n",
    "\n",
    "\n",
    "\n",
    "[原文](https://blog.csdn.net/huanghaocs/article/details/77935556 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7. What's the 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ans:__\n",
    "\n",
    "Unigram models也即一元文法模型，它是一种上下文无关模型。该模型仅仅考虑当前词本身出现的概率，而不考虑当前词的上下文环境。概率形式为\n",
    "$$ P(w_1,w_2,\\dots,w_m)=P(w_1)\\cdot P(w_2)\\cdot \\cdots \\cdot P(w_m) $$即一个句子出现的概率等于句子中每个单词概率乘积。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. What's the disadvantages and advantages of 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ans:__\n",
    "\n",
    "> __Cons:__\n",
    "\n",
    "* 无法判断句子的连贯性与合理性\n",
    "* 受极端词（两个极端：常用的词和罕见的词）的影响较大\n",
    "* 短句概率大于长句概率\n",
    "\n",
    "> __Pros:__\n",
    "\n",
    "* 简单快速地得出一个句子的概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9.  What't the 2-gram models; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ans:__\n",
    "\n",
    "二元文法模型局部地考虑到上下文环境，即考虑前一个词和后一个词的关联性。先计算在$w_{i}$词出现条件下($w_{i}$ $w_{i+1}$)词组出现的概率$p_{i}$，再计算所有$p_{i}$的乘积，概率形式为：\n",
    "\n",
    "$$ Pr(S) = Pr(w_1\\cdot w_2 \\cdots w_n) \\sim Pr(w_1) \\cdot Pr(w_2 | w_1) \\cdots Pr(w_n | w_{n-1})$$\n",
    "\n",
    "$$ Pr(w_2 | w_1) =  \\frac { Pr(w_1 \\cdot w_2) }{Pr(w_1)} $$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10. what's the web crawler, and can you implement a simple crawler? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ans:__\n",
    "\n",
    "Web crawler is designed rule to crawl information in given or redirected website\n",
    "\n",
    "> for example:爬取妹子图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christopher_luole\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\Christopher_luole\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "#-*- coding: utf-8 -*-\n",
    "#encoding=utf-8\n",
    "\n",
    "import urllib.request\n",
    "import urllib\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import string\n",
    "table=str.maketrans({key:None for key in string.punctuation})#remove ? / , .\n",
    "\n",
    "def getAllImageLink():\n",
    "    for i in range(0,2):#可以设为100页或更高\n",
    "        if i==0:\n",
    "            url=\"http://www.dbmeinv.com\"\n",
    "        else:\n",
    "            url=\"https://www.dbmeinv.com/?pager_offset=\"+str(i+1)\n",
    "        html = urllib.request .urlopen(url).read()\n",
    "        soup = BeautifulSoup(html)\n",
    "        liResult = soup.findAll('li',attrs={\"class\":\"span3\"})\n",
    "        \n",
    "\n",
    "        for li in liResult:\n",
    "            imageEntityArray = li.findAll('img')\n",
    "            for image in imageEntityArray:\n",
    "                link = image.get('src')\n",
    "                imageName = image.get('title')\n",
    "                imageName=imageName.translate(table)\n",
    "                filesavepath = \"%s.png\" % imageName\n",
    "                urllib.request.urlretrieve(link,filesavepath)\n",
    "                # print filesavepath\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    getAllImageLink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11.  There may be some issues to make our crwaler programming difficult, what are these, and how do we solve them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ans:__\n",
    "\n",
    "__difficulties and solutions__\n",
    "\n",
    "* 识别来访者浏览器等信息从而做出限制 - 通过武装post header来伪装指定身份\n",
    "* 验证码输入门槛 - 通过抓取验证码，Image2Words自动识别验证码，或者手动输入验证码\n",
    "* 目标信息隐藏（比如B站弹幕） - 查看事件流上css, js, xhr等变化，找到目标信息的来源\n",
    "* 多网页爬取 - 通过有序规律改写url，或者模仿点击“下一页”跳转页面\n",
    "* 限制点击频率 - 通过time.sleep来减缓点击频率\n",
    "* and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12. What't the Regular Expression and how to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ans:__\n",
    "\n",
    "- [c,m,f]an: can, man, fan; [^b]og to skip bog\n",
    "- [^a-z]\\w+ skip lower case begined string; \\w means [A-Za-z0-9]; \\d means [0-9]\n",
    "- z{3} match z three times: uzzz, wuzzzz; .{2,6} match string with length of 2-6\n",
    "- \\? match ?\n",
    "- whitespace: space (␣), the tab (\\t), the new line (\\n) and the carriage return (\\r) \n",
    "- \\s will match any of the specific whitespaces above\n",
    "- \\D represents any non-digit character, \\S any non-whitespace character, and \\W any non-alphanumeric\n",
    "- ^Mission: successful\\$ ^为字符串开始 and $为字符串结尾\n",
    "- ^(file_\\w+) can match file_record_transcript in file_record_transcript.pdf\n",
    "- ^([A-Z]\\w{2} (\\d{4})) 括号中为提取的信息，此处不但提取Jan 1987，还提取1987\n",
    "- ^I love cats|I love dogs\\$ match \"I love cats\"或\"I love dogs\"\n",
    "- ^The.* match string starting with \"The\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Wikipedia dataset to finish the language model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: You need to download the corpus from wikipedis:\n",
    "> https://dumps.wikimedia.org/zhwiki/20190401/\n",
    "\n",
    "Step 2: You may need the help of wiki-extractor:\n",
    "\n",
    "> https://github.com/attardi/wikiextractor\n",
    "\n",
    "Step 3: Using the technologies and methods to finish the language model; \n",
    "> \n",
    "\n",
    "Step 4: Try some interested sentence pairs, and check if your model could fit them\n",
    "\n",
    "> \n",
    "\n",
    "Step 5: If we need to solve following problems, how can language model help us? \n",
    "\n",
    "+ Voice Recognization.\n",
    "+ Sogou *pinyin* input.\n",
    "+ Auto correction in search engine. \n",
    "+ Abnormal Detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read all separated wiki files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_doc(string):\n",
    "    return \"<doc\" in str(string) or \"</doc\" in str(string)\n",
    "\n",
    "def read_one_file(path):\n",
    "    \"\"\"\n",
    "    input: file path of the text, like \"mydata/text/AA/wiki_00\"\n",
    "    output: DataFrame of one column \"line\" \n",
    "    remark: \"<doc id ...>\" and \"</doc>\" is removed\n",
    "    \"\"\"\n",
    "    wikidata = pd.read_table(path)\n",
    "    wikidata = wikidata.rename(columns = {wikidata.columns[0]:\"line\"})#rename the columns\n",
    "    wikidata[wikidata.line.apply(find_doc)] = \"\"#remove \"<doc ...\"\n",
    "    return wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token(string):\n",
    "    \"\"\"\n",
    "    return only words and numbers in string\n",
    "    \"\"\"\n",
    "    return ''.join(re.findall('[\\w|\\d+]',string))\n",
    "\n",
    "def gather_all_text_together(dataframe):\n",
    "    \"\"\"\n",
    "    input: dataframe with columns \"line\"\n",
    "    output: a string with all text in dataframe.line\n",
    "    \"\"\"\n",
    "    all_articles = [token(str(a)) for a in dataframe.line.tolist()]# only keep words and digit\n",
    "    text = ''\n",
    "    for a in all_articles:\n",
    "        text += a\n",
    "    return text\n",
    "    print('Add length of text: {}'.format(len(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 繁体转简体并分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#繁体转简体\n",
    "from langconv import *\n",
    "def Traditional2Simplified(sentence):\n",
    "    return Converter('zh-hans').convert(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "def cut(string): return list(jieba.cut(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('ALL_TOKENS.txt','w') as file:\n",
    "#     file.write(str(ALL_TOKENS))\n",
    "# with open('ALL_TOKENS.txt','r') as file:\n",
    "#     ALL_TOKENS=file.read()\n",
    "\n",
    "# ALL_TOKENS=ALL_TOKENS.replace(\"'\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn all text file into Tokens\n",
    "import os\n",
    "path_AA = os.getcwd()+\"\\\\mydata\\\\text\\\\AA\"\n",
    "path_AB = os.getcwd()+\"\\\\mydata\\\\text\\\\AB\"\n",
    "\n",
    "def get_file_name(path):\n",
    "    \"\"\"\n",
    "    input: string, file path, like'C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\text\\\\AA'\n",
    "    output: list, with all visible file name under the path\n",
    "    \"\"\"\n",
    "    list_of_file_name = []\n",
    "    for file_name in os.listdir(path):\n",
    "        if not file_name.startswith('.'):\n",
    "            list_of_file_name.append(file_name) \n",
    "    return list_of_file_name\n",
    "\n",
    "AA = get_file_name(path_AA)\n",
    "AB = get_file_name(path_AB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\CHRIST~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.007 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total spend 14.77033019065857 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_13.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 13.588124752044678 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_14.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 38.4126181602478 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_15.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 38.659611225128174 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_16.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 13.378539800643921 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_17.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 41.292723417282104 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_18.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 40.86196422576904 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_19.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 36.19674587249756 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_20.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 13.312470436096191 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_21.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 39.12149119377136 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_22.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 42.87830400466919 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_23.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 15.949992418289185 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_24.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 17.320066690444946 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_25.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 16.365787506103516 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_26.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 15.906627655029297 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_27.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 16.331289529800415 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_28.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 44.66955780982971 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_29.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 44.941246032714844 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_30.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 16.399768114089966 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_31.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 16.2052264213562 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_32.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 16.501627683639526 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_33.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 40.2341365814209 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_34.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 38.15386652946472 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_35.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 40.955793619155884 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_36.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 16.51813244819641 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_37.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 16.601542472839355 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_38.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 16.71907091140747 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_39.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 43.069231033325195 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_40.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 16.421570301055908 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_41.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 16.318694353103638 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_42.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 16.638160943984985 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_43.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 42.33439087867737 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_44.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 16.006529331207275 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_45.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 49.03442072868347 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_46.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 43.10454177856445 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_47.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 17.260127544403076 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_48.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 46.9154999256134 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_49.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 16.991185665130615 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_50.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 46.79626202583313 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_51.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 45.60814619064331 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_52.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 45.21523571014404 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_53.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 43.02296209335327 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_54.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 36.30607986450195 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_55.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 37.599515438079834 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_56.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 13.13669753074646 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_57.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 13.501533269882202 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_58.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 35.08693218231201 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_59.txt' mode='w' encoding='utf-8'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total spend 39.08711886405945 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_60.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 13.376901626586914 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_61.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 12.774177074432373 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_62.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 13.144969701766968 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_63.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 15.613511085510254 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_64.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 42.97290062904358 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_65.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 44.51757454872131 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_66.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 16.694689750671387 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_67.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 16.64474654197693 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_68.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 17.713836193084717 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_69.txt' mode='w' encoding='utf-8'>\n",
      "Total spend 3.774008274078369 seconds to finish <_io.TextIOWrapper name='C:\\\\Users\\\\Christopher_luole\\\\Git_Project\\\\mydata\\\\Tokens\\\\AB\\\\wiki_70.txt' mode='w' encoding='utf-8'>\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "\n",
    "for file in AB:\n",
    "    #if file ==\"wiki_00\":\n",
    "        start=time()\n",
    "        \n",
    "        wikidata = read_one_file(path_AB+\"\\\\\"+file)\n",
    "        text = gather_all_text_together(wikidata)\n",
    "        text = Traditional2Simplified(text)#繁转简\n",
    "        ALL_TOKENS = cut(text)#分词\n",
    "        with open(path_AB[:-7]+\"Tokens\\\\AB\\\\\"+file+\".txt\",'w',encoding=\"utf-8\") as file:\n",
    "            file.write(str(ALL_TOKENS))\n",
    "        T1=time()-start\n",
    "        print('Total spend {} seconds to finish {}'.format(T1,file))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取之前保存的分词结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read all Tokens files into ALL_TOKENS\n",
    "def Read_Tokens(path):\n",
    "    \"\"\"\n",
    "    input: path of Tokens files\n",
    "    output: list of valid tokens\n",
    "    \"\"\"\n",
    "    with open(path,'r',encoding=\"utf-8\") as file:\n",
    "        ALL_TOKENS = file.read()\n",
    "        ALL_TOKENS = ALL_TOKENS.replace(\"'\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").split(', ')\n",
    "        valid_tokens = [t for t in ALL_TOKENS if t.strip()]\n",
    "    return valid_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add wiki_00 in AA, and Length of valid_tokens increased to 167700, spend 0.0740041732788086 seconds\n",
      "Add wiki_01 in AA, and Length of valid_tokens increased to 331493, spend 0.06800389289855957 seconds\n",
      "Add wiki_02 in AA, and Length of valid_tokens increased to 490316, spend 0.0686030387878418 seconds\n",
      "Add wiki_03 in AA, and Length of valid_tokens increased to 657965, spend 0.06240034103393555 seconds\n",
      "Add wiki_04 in AA, and Length of valid_tokens increased to 825021, spend 0.07800006866455078 seconds\n",
      "Add wiki_05 in AA, and Length of valid_tokens increased to 990946, spend 0.07680177688598633 seconds\n",
      "Add wiki_06 in AA, and Length of valid_tokens increased to 1161184, spend 0.07799983024597168 seconds\n",
      "Add wiki_07 in AA, and Length of valid_tokens increased to 1327844, spend 0.07800054550170898 seconds\n",
      "Add wiki_08 in AA, and Length of valid_tokens increased to 1491714, spend 0.060800790786743164 seconds\n",
      "Add wiki_09 in AA, and Length of valid_tokens increased to 1656636, spend 0.07800006866455078 seconds\n",
      "Add wiki_10 in AA, and Length of valid_tokens increased to 1823848, spend 0.07140064239501953 seconds\n",
      "Add wiki_11 in AA, and Length of valid_tokens increased to 1985265, spend 0.08440136909484863 seconds\n",
      "Add wiki_12 in AA, and Length of valid_tokens increased to 2151685, spend 0.06900405883789062 seconds\n",
      "Add wiki_13 in AA, and Length of valid_tokens increased to 2322059, spend 0.07800459861755371 seconds\n",
      "Add wiki_14 in AA, and Length of valid_tokens increased to 2483443, spend 0.052201032638549805 seconds\n",
      "Add wiki_15 in AA, and Length of valid_tokens increased to 2651466, spend 0.09300112724304199 seconds\n",
      "Add wiki_16 in AA, and Length of valid_tokens increased to 2818202, spend 0.07900428771972656 seconds\n",
      "Add wiki_17 in AA, and Length of valid_tokens increased to 2983882, spend 0.08300495147705078 seconds\n",
      "Add wiki_18 in AA, and Length of valid_tokens increased to 3141625, spend 0.06900382041931152 seconds\n",
      "Add wiki_19 in AA, and Length of valid_tokens increased to 3308837, spend 0.07500433921813965 seconds\n",
      "Add wiki_20 in AA, and Length of valid_tokens increased to 3474602, spend 0.08800506591796875 seconds\n",
      "Add wiki_21 in AA, and Length of valid_tokens increased to 3637351, spend 0.07500433921813965 seconds\n",
      "Add wiki_22 in AA, and Length of valid_tokens increased to 3807228, spend 0.07700419425964355 seconds\n",
      "Add wiki_23 in AA, and Length of valid_tokens increased to 3972019, spend 0.09100532531738281 seconds\n",
      "Add wiki_24 in AA, and Length of valid_tokens increased to 4141938, spend 0.07900428771972656 seconds\n",
      "Add wiki_25 in AA, and Length of valid_tokens increased to 4307051, spend 0.0870051383972168 seconds\n",
      "Add wiki_26 in AA, and Length of valid_tokens increased to 4477681, spend 0.08600473403930664 seconds\n",
      "Add wiki_27 in AA, and Length of valid_tokens increased to 4643186, spend 0.06840085983276367 seconds\n",
      "Add wiki_28 in AA, and Length of valid_tokens increased to 4808802, spend 0.07800006866455078 seconds\n",
      "Add wiki_29 in AA, and Length of valid_tokens increased to 4976473, spend 0.06980109214782715 seconds\n",
      "Add wiki_30 in AA, and Length of valid_tokens increased to 5138990, spend 0.07800030708312988 seconds\n",
      "Add wiki_31 in AA, and Length of valid_tokens increased to 5298176, spend 0.06440019607543945 seconds\n",
      "Add wiki_32 in AA, and Length of valid_tokens increased to 5445662, spend 0.0742027759552002 seconds\n",
      "Add wiki_33 in AA, and Length of valid_tokens increased to 5607527, spend 0.07800412178039551 seconds\n",
      "Add wiki_34 in AA, and Length of valid_tokens increased to 5773374, spend 0.08500504493713379 seconds\n",
      "Add wiki_35 in AA, and Length of valid_tokens increased to 5939840, spend 0.09500527381896973 seconds\n",
      "Add wiki_36 in AA, and Length of valid_tokens increased to 6107104, spend 0.06900405883789062 seconds\n",
      "Add wiki_37 in AA, and Length of valid_tokens increased to 6270671, spend 0.06900405883789062 seconds\n",
      "Add wiki_38 in AA, and Length of valid_tokens increased to 6436996, spend 0.06760311126708984 seconds\n",
      "Add wiki_39 in AA, and Length of valid_tokens increased to 6604966, spend 0.06640028953552246 seconds\n",
      "Add wiki_40 in AA, and Length of valid_tokens increased to 6769799, spend 0.10080313682556152 seconds\n",
      "Add wiki_41 in AA, and Length of valid_tokens increased to 6939675, spend 0.08800506591796875 seconds\n",
      "Add wiki_42 in AA, and Length of valid_tokens increased to 7102162, spend 0.0740041732788086 seconds\n",
      "Add wiki_43 in AA, and Length of valid_tokens increased to 7265857, spend 0.08400487899780273 seconds\n",
      "Add wiki_44 in AA, and Length of valid_tokens increased to 7432981, spend 0.06960320472717285 seconds\n",
      "Add wiki_45 in AA, and Length of valid_tokens increased to 7596452, spend 0.06680107116699219 seconds\n",
      "Add wiki_46 in AA, and Length of valid_tokens increased to 7766095, spend 0.09360027313232422 seconds\n",
      "Add wiki_47 in AA, and Length of valid_tokens increased to 7929579, spend 0.07800006866455078 seconds\n",
      "Add wiki_48 in AA, and Length of valid_tokens increased to 8097435, spend 0.06440019607543945 seconds\n",
      "Add wiki_49 in AA, and Length of valid_tokens increased to 8262482, spend 0.08420300483703613 seconds\n",
      "Add wiki_50 in AA, and Length of valid_tokens increased to 8428585, spend 0.06220197677612305 seconds\n",
      "Add wiki_51 in AA, and Length of valid_tokens increased to 8595114, spend 0.07800006866455078 seconds\n",
      "Add wiki_52 in AA, and Length of valid_tokens increased to 8761925, spend 0.11200213432312012 seconds\n",
      "Add wiki_53 in AA, and Length of valid_tokens increased to 8933717, spend 0.07700443267822266 seconds\n",
      "Add wiki_54 in AA, and Length of valid_tokens increased to 9100543, spend 0.07200407981872559 seconds\n",
      "Add wiki_55 in AA, and Length of valid_tokens increased to 9271781, spend 0.07700443267822266 seconds\n",
      "Add wiki_56 in AA, and Length of valid_tokens increased to 9438787, spend 0.06900382041931152 seconds\n",
      "Add wiki_57 in AA, and Length of valid_tokens increased to 9608297, spend 0.06240034103393555 seconds\n",
      "Add wiki_58 in AA, and Length of valid_tokens increased to 9779682, spend 0.07800006866455078 seconds\n",
      "Add wiki_59 in AA, and Length of valid_tokens increased to 9946158, spend 0.09140181541442871 seconds\n",
      "Add wiki_60 in AA, and Length of valid_tokens increased to 10113981, spend 0.07800006866455078 seconds\n",
      "Add wiki_61 in AA, and Length of valid_tokens increased to 10274969, spend 0.062399864196777344 seconds\n",
      "Add wiki_62 in AA, and Length of valid_tokens increased to 10446512, spend 0.07680177688598633 seconds\n",
      "Add wiki_63 in AA, and Length of valid_tokens increased to 10615127, spend 0.06240034103393555 seconds\n",
      "Add wiki_64 in AA, and Length of valid_tokens increased to 10781772, spend 0.0874013900756836 seconds\n",
      "Add wiki_65 in AA, and Length of valid_tokens increased to 10951128, spend 0.07800483703613281 seconds\n",
      "Add wiki_66 in AA, and Length of valid_tokens increased to 11111630, spend 0.06600379943847656 seconds\n",
      "Add wiki_67 in AA, and Length of valid_tokens increased to 11275563, spend 0.12000656127929688 seconds\n",
      "Add wiki_68 in AA, and Length of valid_tokens increased to 11441602, spend 0.08500504493713379 seconds\n",
      "Add wiki_69 in AA, and Length of valid_tokens increased to 11606946, spend 0.08200478553771973 seconds\n",
      "Add wiki_70 in AA, and Length of valid_tokens increased to 11772716, spend 0.07900428771972656 seconds\n",
      "Add wiki_71 in AA, and Length of valid_tokens increased to 11939710, spend 0.07300400733947754 seconds\n",
      "Add wiki_72 in AA, and Length of valid_tokens increased to 12107043, spend 0.07000398635864258 seconds\n",
      "Add wiki_73 in AA, and Length of valid_tokens increased to 12272269, spend 0.06020164489746094 seconds\n",
      "Add wiki_74 in AA, and Length of valid_tokens increased to 12441000, spend 0.0688011646270752 seconds\n",
      "Add wiki_75 in AA, and Length of valid_tokens increased to 12606864, spend 0.08140110969543457 seconds\n",
      "Add wiki_76 in AA, and Length of valid_tokens increased to 12776598, spend 0.11100649833679199 seconds\n",
      "Add wiki_77 in AA, and Length of valid_tokens increased to 12944312, spend 0.07800459861755371 seconds\n",
      "Add wiki_78 in AA, and Length of valid_tokens increased to 13109834, spend 0.07700419425964355 seconds\n",
      "Add wiki_79 in AA, and Length of valid_tokens increased to 13270370, spend 0.07000398635864258 seconds\n",
      "Add wiki_80 in AA, and Length of valid_tokens increased to 13440125, spend 0.0890049934387207 seconds\n",
      "Add wiki_81 in AA, and Length of valid_tokens increased to 13606017, spend 0.0870053768157959 seconds\n",
      "Add wiki_82 in AA, and Length of valid_tokens increased to 13774864, spend 0.08800482749938965 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add wiki_83 in AA, and Length of valid_tokens increased to 13944632, spend 0.07900428771972656 seconds\n",
      "Add wiki_84 in AA, and Length of valid_tokens increased to 14112035, spend 0.07500457763671875 seconds\n",
      "Add wiki_85 in AA, and Length of valid_tokens increased to 14280133, spend 0.07000398635864258 seconds\n",
      "Add wiki_86 in AA, and Length of valid_tokens increased to 14446079, spend 0.11700654029846191 seconds\n",
      "Add wiki_87 in AA, and Length of valid_tokens increased to 14611306, spend 0.07100415229797363 seconds\n",
      "Add wiki_88 in AA, and Length of valid_tokens increased to 14778989, spend 0.07700419425964355 seconds\n",
      "Add wiki_89 in AA, and Length of valid_tokens increased to 14947993, spend 0.0740041732788086 seconds\n",
      "Add wiki_90 in AA, and Length of valid_tokens increased to 15113963, spend 0.07100415229797363 seconds\n",
      "Add wiki_91 in AA, and Length of valid_tokens increased to 15282530, spend 0.0760045051574707 seconds\n",
      "Add wiki_92 in AA, and Length of valid_tokens increased to 15449361, spend 0.07120227813720703 seconds\n",
      "Add wiki_93 in AA, and Length of valid_tokens increased to 15618686, spend 0.06780123710632324 seconds\n",
      "Add wiki_94 in AA, and Length of valid_tokens increased to 15784241, spend 0.08260416984558105 seconds\n",
      "Add wiki_95 in AA, and Length of valid_tokens increased to 15951603, spend 0.07460331916809082 seconds\n",
      "Add wiki_96 in AA, and Length of valid_tokens increased to 16120194, spend 0.07540082931518555 seconds\n",
      "Add wiki_97 in AA, and Length of valid_tokens increased to 16288663, spend 0.11820054054260254 seconds\n",
      "Add wiki_98 in AA, and Length of valid_tokens increased to 16455775, spend 0.07800030708312988 seconds\n",
      "Add wiki_99 in AA, and Length of valid_tokens increased to 16619124, spend 0.06780123710632324 seconds\n",
      "Add wiki_00 in AB, and Length of valid_tokens increased to 16788911, spend 0.06240034103393555 seconds\n",
      "Add wiki_01 in AB, and Length of valid_tokens increased to 16956283, spend 0.062399864196777344 seconds\n",
      "Add wiki_02 in AB, and Length of valid_tokens increased to 17121046, spend 0.07800006866455078 seconds\n",
      "Add wiki_03 in AB, and Length of valid_tokens increased to 17290351, spend 0.08140110969543457 seconds\n",
      "Add wiki_04 in AB, and Length of valid_tokens increased to 17453425, spend 0.06240034103393555 seconds\n",
      "Add wiki_05 in AB, and Length of valid_tokens increased to 17614297, spend 0.07800006866455078 seconds\n",
      "Add wiki_06 in AB, and Length of valid_tokens increased to 17781686, spend 0.06180095672607422 seconds\n",
      "Add wiki_07 in AB, and Length of valid_tokens increased to 17953505, spend 0.07800030708312988 seconds\n",
      "Add wiki_08 in AB, and Length of valid_tokens increased to 18123836, spend 0.07940077781677246 seconds\n",
      "Add wiki_09 in AB, and Length of valid_tokens increased to 18292078, spend 0.07300424575805664 seconds\n",
      "Add wiki_10 in AB, and Length of valid_tokens increased to 18452052, spend 0.12540364265441895 seconds\n",
      "Add wiki_11 in AB, and Length of valid_tokens increased to 18617749, spend 0.07800459861755371 seconds\n",
      "Add wiki_12 in AB, and Length of valid_tokens increased to 18784371, spend 0.07200407981872559 seconds\n",
      "Add wiki_13 in AB, and Length of valid_tokens increased to 18952617, spend 0.07100415229797363 seconds\n",
      "Add wiki_14 in AB, and Length of valid_tokens increased to 19116587, spend 0.05660247802734375 seconds\n",
      "Add wiki_15 in AB, and Length of valid_tokens increased to 19282382, spend 0.0840003490447998 seconds\n",
      "Add wiki_16 in AB, and Length of valid_tokens increased to 19445618, spend 0.063201904296875 seconds\n",
      "Add wiki_17 in AB, and Length of valid_tokens increased to 19611014, spend 0.0688014030456543 seconds\n",
      "Add wiki_18 in AB, and Length of valid_tokens increased to 19781258, spend 0.07800030708312988 seconds\n",
      "Add wiki_19 in AB, and Length of valid_tokens increased to 19950682, spend 0.062399864196777344 seconds\n",
      "Add wiki_20 in AB, and Length of valid_tokens increased to 20118948, spend 0.07540106773376465 seconds\n",
      "Add wiki_21 in AB, and Length of valid_tokens increased to 20289857, spend 0.07800006866455078 seconds\n",
      "Add wiki_22 in AB, and Length of valid_tokens increased to 20456216, spend 0.062400102615356445 seconds\n",
      "Add wiki_23 in AB, and Length of valid_tokens increased to 20623776, spend 0.0764007568359375 seconds\n",
      "Add wiki_24 in AB, and Length of valid_tokens increased to 20787150, spend 0.12480020523071289 seconds\n",
      "Add wiki_25 in AB, and Length of valid_tokens increased to 20953663, spend 0.08160400390625 seconds\n",
      "Add wiki_26 in AB, and Length of valid_tokens increased to 21118621, spend 0.06100344657897949 seconds\n",
      "Add wiki_27 in AB, and Length of valid_tokens increased to 21284528, spend 0.062400102615356445 seconds\n",
      "Add wiki_28 in AB, and Length of valid_tokens increased to 21453167, spend 0.07800006866455078 seconds\n",
      "Add wiki_29 in AB, and Length of valid_tokens increased to 21620652, spend 0.07520270347595215 seconds\n",
      "Add wiki_30 in AB, and Length of valid_tokens increased to 21786915, spend 0.062400102615356445 seconds\n",
      "Add wiki_31 in AB, and Length of valid_tokens increased to 21952735, spend 0.062399864196777344 seconds\n",
      "Add wiki_32 in AB, and Length of valid_tokens increased to 22118861, spend 0.07540106773376465 seconds\n",
      "Add wiki_33 in AB, and Length of valid_tokens increased to 22286295, spend 0.07280158996582031 seconds\n",
      "Add wiki_34 in AB, and Length of valid_tokens increased to 22454337, spend 0.07520222663879395 seconds\n",
      "Add wiki_35 in AB, and Length of valid_tokens increased to 22621307, spend 0.06760334968566895 seconds\n",
      "Add wiki_36 in AB, and Length of valid_tokens increased to 22789509, spend 0.07800006866455078 seconds\n",
      "Add wiki_37 in AB, and Length of valid_tokens increased to 22956955, spend 0.07420229911804199 seconds\n",
      "Add wiki_38 in AB, and Length of valid_tokens increased to 23126290, spend 0.07300424575805664 seconds\n",
      "Add wiki_39 in AB, and Length of valid_tokens increased to 23291944, spend 0.06900382041931152 seconds\n",
      "Add wiki_40 in AB, and Length of valid_tokens increased to 23454913, spend 0.13180088996887207 seconds\n",
      "Add wiki_41 in AB, and Length of valid_tokens increased to 23624443, spend 0.07440066337585449 seconds\n",
      "Add wiki_42 in AB, and Length of valid_tokens increased to 23789431, spend 0.062400102615356445 seconds\n",
      "Add wiki_43 in AB, and Length of valid_tokens increased to 23957834, spend 0.07800006866455078 seconds\n",
      "Add wiki_44 in AB, and Length of valid_tokens increased to 24125608, spend 0.07780194282531738 seconds\n",
      "Add wiki_45 in AB, and Length of valid_tokens increased to 24291397, spend 0.07800030708312988 seconds\n",
      "Add wiki_46 in AB, and Length of valid_tokens increased to 24458496, spend 0.08440113067626953 seconds\n",
      "Add wiki_47 in AB, and Length of valid_tokens increased to 24625712, spend 0.08400487899780273 seconds\n",
      "Add wiki_48 in AB, and Length of valid_tokens increased to 24793120, spend 0.06480097770690918 seconds\n",
      "Add wiki_49 in AB, and Length of valid_tokens increased to 24960011, spend 0.06240034103393555 seconds\n",
      "Add wiki_50 in AB, and Length of valid_tokens increased to 25128143, spend 0.07340049743652344 seconds\n",
      "Add wiki_51 in AB, and Length of valid_tokens increased to 25294650, spend 0.07800006866455078 seconds\n",
      "Add wiki_52 in AB, and Length of valid_tokens increased to 25462682, spend 0.07800006866455078 seconds\n",
      "Add wiki_53 in AB, and Length of valid_tokens increased to 25629379, spend 0.060801029205322266 seconds\n",
      "Add wiki_54 in AB, and Length of valid_tokens increased to 25790276, spend 0.07800030708312988 seconds\n",
      "Add wiki_55 in AB, and Length of valid_tokens increased to 25954769, spend 0.07240056991577148 seconds\n",
      "Add wiki_56 in AB, and Length of valid_tokens increased to 26117871, spend 0.06680107116699219 seconds\n",
      "Add wiki_57 in AB, and Length of valid_tokens increased to 26285539, spend 0.07800006866455078 seconds\n",
      "Add wiki_58 in AB, and Length of valid_tokens increased to 26453868, spend 0.14040040969848633 seconds\n",
      "Add wiki_59 in AB, and Length of valid_tokens increased to 26621250, spend 0.08040118217468262 seconds\n",
      "Add wiki_60 in AB, and Length of valid_tokens increased to 26789541, spend 0.062400102615356445 seconds\n",
      "Add wiki_61 in AB, and Length of valid_tokens increased to 26954883, spend 0.062400102615356445 seconds\n",
      "Add wiki_62 in AB, and Length of valid_tokens increased to 27117852, spend 0.0764009952545166 seconds\n",
      "Add wiki_63 in AB, and Length of valid_tokens increased to 27280091, spend 0.062400102615356445 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add wiki_64 in AB, and Length of valid_tokens increased to 27443932, spend 0.062399864196777344 seconds\n",
      "Add wiki_65 in AB, and Length of valid_tokens increased to 27608882, spend 0.07480192184448242 seconds\n",
      "Add wiki_66 in AB, and Length of valid_tokens increased to 27776510, spend 0.07799983024597168 seconds\n",
      "Add wiki_67 in AB, and Length of valid_tokens increased to 27943190, spend 0.15600061416625977 seconds\n",
      "Add wiki_68 in AB, and Length of valid_tokens increased to 28110080, spend 0.06580114364624023 seconds\n",
      "Add wiki_69 in AB, and Length of valid_tokens increased to 28274597, spend 0.062400102615356445 seconds\n",
      "Add wiki_70 in AB, and Length of valid_tokens increased to 28328816, spend 0.031199932098388672 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "valid_tokens = []\n",
    "for file in AA:\n",
    "    start=time()\n",
    "    valid_tokens += (Read_Tokens(path_AA[:-7]+\"Tokens\\\\AA\\\\\"+file+\".txt\"))\n",
    "    T1=time()-start\n",
    "    print(\"Add {} in AA, and Length of valid_tokens increased to {}, spend {} seconds\".format(file, len(valid_tokens),T1))\n",
    "\n",
    "for file in AB:\n",
    "    start=time()\n",
    "    valid_tokens += (Read_Tokens(path_AB[:-7]+\"Tokens\\\\AB\\\\\"+file+\".txt\"))\n",
    "    T1=time()-start\n",
    "    print(\"Add {} in AB, and Length of valid_tokens increased to {}, spend {} seconds\".format(file, len(valid_tokens),T1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the frequences of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('的', 1645202),\n",
       " ('在', 433010),\n",
       " ('年', 353279),\n",
       " ('是', 305301),\n",
       " ('和', 256064),\n",
       " ('了', 213869),\n",
       " ('为', 193551),\n",
       " ('与', 153842),\n",
       " ('月', 151892),\n",
       " ('有', 143411)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "words_count = Counter(valid_tokens)\n",
    "\n",
    "words_count.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.529974567239238e-08"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequences_all = [f for w, f in words_count.most_common()]\n",
    "frequences_sum = sum(frequences_all)\n",
    "1 / frequences_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00016862688507701838 5.542060070565604e-06 0.00021797592952702295 0.0003077784825175892\n"
     ]
    }
   ],
   "source": [
    "def get_prob(word): \n",
    "    esp = 1 / frequences_sum\n",
    "    if word in words_count: \n",
    "        return words_count[word] / frequences_sum\n",
    "    else:\n",
    "        return esp\n",
    "    \n",
    "print(get_prob('数学'),get_prob('蛋糕'),get_prob('罗马'),get_prob('我们'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3 - Unigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "def product(numbers):\n",
    "    return reduce(lambda n1, n2: n1 * n2, numbers)\n",
    "\n",
    "def language_model_one_gram(string):\n",
    "    words = cut(string)\n",
    "    return product([get_prob(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "秦始皇统一中国 1.3608996758020246e-11\n",
      "秦始皇一统中国 4.037140238126656e-13\n",
      "明天我们一起回家吃饭 8.094004689864576e-23\n",
      "未来人类共同面对饥饿 5.8928632566104e-21\n"
     ]
    }
   ],
   "source": [
    "sentences = \"\"\"\n",
    "秦始皇统一中国\n",
    "秦始皇一统中国\n",
    "明天我们一起回家吃饭\n",
    "未来人类共同面对饥饿\n",
    "\"\"\".split()\n",
    "\n",
    "for s in sentences:\n",
    "    print(s, language_model_one_gram(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "三角学等初等数学，已大体完备 is more possible\n",
      "---- 三角学等初等数学，已大体完备 with probility 2.3095409633743076e-35\n",
      "---- 三角学这么容易学习，已经快要完成 with probility 2.9149061159514537e-38\n",
      "真是一只好看的小猫 is more possible\n",
      "---- 真事一只好看的小猫 with probility 4.773456491380672e-26\n",
      "---- 真是一只好看的小猫 with probility 4.295269206738948e-23\n",
      "今晚我去吃火锅 is more possible\n",
      "---- 我去吃火锅，今晚 with probility 4.785413704056803e-28\n",
      "---- 今晚我去吃火锅 with probility 1.3556510430610363e-20\n"
     ]
    }
   ],
   "source": [
    "need_compared = [\n",
    "    \"三角学等初等数学，已大体完备 三角学这么容易学习，已经快要完成\",\n",
    "    \"真事一只好看的小猫 真是一只好看的小猫\",\n",
    "    \"我去吃火锅，今晚 今晚我去吃火锅\"\n",
    "]\n",
    "\n",
    "for s in need_compared:\n",
    "    s1, s2 = s.split()\n",
    "    p1, p2 = language_model_one_gram(s1), language_model_one_gram(s2)\n",
    "    \n",
    "    better = s1 if p1 > p2 else s2\n",
    "    \n",
    "    print('{} is more possible'.format(better))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s1, p1))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s2, p2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time()\n",
    "\n",
    "valid_tokens = [str(t) for t in valid_tokens] \n",
    "all_2_grams_words = [''.join(valid_tokens[i:i+2]) for i in range(len(valid_tokens[:-2]))]\n",
    "_2_gram_sum = len(all_2_grams_words)\n",
    "_2_gram_counter = Counter(all_2_grams_words)\n",
    "\n",
    "T1=time()-start\n",
    "print(\"spend {} seconds\".format(T1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_combination_prob(w1, w2):\n",
    "    if w1 + w2 in _2_gram_counter: return _2_gram_counter[w1+w2] / _2_gram_sum\n",
    "    else:\n",
    "        return 1 / _2_gram_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.118942077843428e-07 1.4119899265814658e-07\n"
     ]
    }
   ],
   "source": [
    "print(get_combination_prob('去', '北京'), get_combination_prob( '北京','去'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00010883762196337576\n",
      "0.0025032653051576424\n"
     ]
    }
   ],
   "source": [
    "def get_prob_2_gram(w1, w2):\n",
    "    return get_combination_prob(w1, w2) / get_prob(w1)\n",
    "\n",
    "print(get_prob_2_gram('去', '沈阳'))\n",
    "print(get_prob_2_gram('去', '北京'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def langauge_model_of_2_gram(sentence):\n",
    "    sentence_probability = 1\n",
    "    \n",
    "    words = cut(sentence)\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        if i == 0: \n",
    "            prob = get_prob(word)\n",
    "        else:\n",
    "            previous = words[i-1]\n",
    "            prob = get_prob_2_gram(previous, word)\n",
    "        sentence_probability *= prob\n",
    "    \n",
    "    return sentence_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4 Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5367763669493123e-11\n",
      "1.1909038601400628e-14\n"
     ]
    }
   ],
   "source": [
    "print(langauge_model_of_2_gram('农民工坐火车去上海'))\n",
    "print(langauge_model_of_2_gram('农民工去上海坐火车'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天晚上请你吃大餐，我们一起吃日料 is more possible\n",
      "---- 今天晚上请你吃大餐，我们一起吃日料 with probility 2.7148988815952096e-28\n",
      "---- 明天晚上请你吃大餐，我们一起吃苹果 with probility 5.42979776319042e-29\n",
      "真是一只好看的小猫 is more possible\n",
      "---- 真事一只好看的小猫 with probility 1.7283995385208607e-20\n",
      "---- 真是一只好看的小猫 with probility 5.681248882024758e-17\n",
      "今晚我去吃火锅 is more possible\n",
      "---- 今晚我去吃火锅 with probility 2.2123617951232717e-14\n",
      "---- 今晚火锅去吃我 with probility 1.093013140430263e-15\n",
      "三角学等初等数学，已大体完备 is more possible\n",
      "---- 三角学等初等数学，已大体完备 with probility 7.636130291674986e-19\n",
      "---- 三角学这么容易学习，已经快要完成 with probility 8.828483440669797e-23\n"
     ]
    }
   ],
   "source": [
    "need_compared = [\n",
    "    \"今天晚上请你吃大餐，我们一起吃日料 明天晚上请你吃大餐，我们一起吃苹果\",\n",
    "    \"真事一只好看的小猫 真是一只好看的小猫\",\n",
    "    \"今晚我去吃火锅 今晚火锅去吃我\",\n",
    "    \"三角学等初等数学，已大体完备 三角学这么容易学习，已经快要完成\"\n",
    "]\n",
    "\n",
    "for s in need_compared:\n",
    "    s1, s2 = s.split()\n",
    "    p1, p2 = langauge_model_of_2_gram(s1), langauge_model_of_2_gram(s2)\n",
    "    \n",
    "    better = s1 if p1 > p2 else s2\n",
    "    \n",
    "    print('{} is more possible'.format(better))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s1, p1))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s2, p2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "grammar = \"\"\"\n",
    "sentence => noun_phrase verb_phrase \n",
    "noun_phrase => Article Adj* noun belong \n",
    "belong => de property\n",
    "de => 的\n",
    "property =>  蜜桔 | 脐橙 | 陶瓷\n",
    "Adj* => null | Adj Adj*\n",
    "verb_phrase => verb noun_phrase\n",
    "Article =>  目前 | 历史上\n",
    "noun =>   南丰 |  赣南 | 景德镇\n",
    "verb =>   有名于 | 好于\n",
    "Adj =>   香的 |  咸的 | 甜的\n",
    "\"\"\"\n",
    "\n",
    "def parse_grammar(grammar_str, sep='=>'):\n",
    "    grammar = {}\n",
    "    for line in grammar_str.split('\\n'): \n",
    "        line = line.strip()\n",
    "        if not line: continue\n",
    "        \n",
    "        target, rules = line.split(sep)\n",
    "        \n",
    "        grammar[target.strip()] = [r.split() for r in rules.split('|')]\n",
    "    \n",
    "    return grammar\n",
    "\n",
    "def gene(grammar_parsed, target='sentence'):\n",
    "    if target not in grammar_parsed: return target\n",
    "    \n",
    "    rule = random.choice(grammar_parsed[target])\n",
    "    return ''.join(gene(grammar_parsed, target=r) for r in rule if r!='null')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['历史上赣南的蜜桔好于目前南丰的陶瓷',\n",
       " '历史上赣南的陶瓷好于目前南丰的陶瓷',\n",
       " '目前南丰的蜜桔好于历史上南丰的陶瓷',\n",
       " '历史上赣南的脐橙好于目前赣南的蜜桔',\n",
       " '历史上赣南的脐橙好于历史上南丰的陶瓷',\n",
       " '目前南丰的脐橙好于历史上景德镇的蜜桔',\n",
       " '历史上南丰的蜜桔好于目前南丰的脐橙',\n",
       " '目前景德镇的脐橙有名于目前南丰的陶瓷',\n",
       " '目前赣南的脐橙有名于目前赣南的陶瓷',\n",
       " '目前咸的赣南的脐橙好于目前景德镇的陶瓷',\n",
       " '历史上赣南的脐橙有名于目前景德镇的陶瓷',\n",
       " '目前景德镇的陶瓷有名于目前赣南的脐橙',\n",
       " '目前咸的景德镇的脐橙好于目前赣南的脐橙',\n",
       " '目前景德镇的脐橙有名于目前南丰的脐橙',\n",
       " '历史上香的南丰的脐橙好于目前赣南的陶瓷',\n",
       " '历史上景德镇的脐橙有名于目前南丰的陶瓷',\n",
       " '目前南丰的脐橙好于目前甜的景德镇的蜜桔',\n",
       " '历史上南丰的脐橙有名于目前赣南的陶瓷',\n",
       " '历史上赣南的脐橙有名于历史上景德镇的陶瓷',\n",
       " '历史上景德镇的蜜桔有名于历史上赣南的陶瓷',\n",
       " '目前南丰的陶瓷有名于目前赣南的脐橙',\n",
       " '目前赣南的蜜桔有名于目前南丰的蜜桔',\n",
       " '目前咸的赣南的蜜桔好于目前南丰的脐橙',\n",
       " '目前南丰的蜜桔好于历史上香的南丰的脐橙',\n",
       " '目前南丰的脐橙好于目前甜的赣南的蜜桔',\n",
       " '目前南丰的陶瓷有名于历史上赣南的脐橙',\n",
       " '历史上赣南的脐橙好于历史上甜的景德镇的脐橙',\n",
       " '目前赣南的脐橙好于历史上咸的南丰的脐橙',\n",
       " '历史上景德镇的脐橙有名于历史上南丰的蜜桔',\n",
       " '历史上景德镇的蜜桔有名于历史上南丰的蜜桔',\n",
       " '历史上南丰的陶瓷好于目前咸的赣南的脐橙',\n",
       " '历史上赣南的陶瓷有名于历史上南丰的脐橙',\n",
       " '历史上赣南的蜜桔好于历史上甜的南丰的脐橙',\n",
       " '目前香的南丰的蜜桔好于历史上南丰的蜜桔',\n",
       " '目前甜的赣南的脐橙有名于目前景德镇的陶瓷',\n",
       " '目前香的赣南的蜜桔有名于目前景德镇的陶瓷',\n",
       " '目前景德镇的脐橙有名于历史上香的赣南的脐橙',\n",
       " '目前甜的景德镇的蜜桔好于历史上香的赣南的脐橙',\n",
       " '目前咸的赣南的脐橙有名于历史上景德镇的陶瓷',\n",
       " '目前甜的南丰的陶瓷有名于目前景德镇的脐橙',\n",
       " '历史上景德镇的陶瓷有名于目前甜的景德镇的脐橙',\n",
       " '历史上甜的咸的景德镇的脐橙好于目前赣南的脐橙',\n",
       " '目前甜的赣南的陶瓷有名于目前赣南的蜜桔',\n",
       " '历史上景德镇的脐橙好于历史上香的咸的赣南的脐橙',\n",
       " '目前赣南的蜜桔有名于目前香的赣南的蜜桔',\n",
       " '历史上咸的赣南的蜜桔好于目前香的景德镇的脐橙',\n",
       " '历史上景德镇的蜜桔有名于目前甜的赣南的蜜桔',\n",
       " '目前香的赣南的蜜桔有名于目前南丰的蜜桔',\n",
       " '历史上景德镇的陶瓷好于历史上咸的甜的南丰的陶瓷',\n",
       " '历史上赣南的陶瓷有名于目前甜的南丰的脐橙',\n",
       " '历史上咸的赣南的蜜桔有名于目前赣南的蜜桔',\n",
       " '历史上咸的南丰的陶瓷有名于历史上南丰的陶瓷',\n",
       " '历史上景德镇的脐橙好于历史上甜的香的南丰的蜜桔',\n",
       " '历史上南丰的蜜桔有名于历史上咸的南丰的蜜桔',\n",
       " '历史上甜的赣南的陶瓷好于历史上咸的赣南的脐橙',\n",
       " '历史上咸的咸的咸的景德镇的脐橙好于目前赣南的陶瓷',\n",
       " '历史上甜的景德镇的蜜桔好于目前咸的咸的景德镇的蜜桔',\n",
       " '目前甜的赣南的蜜桔有名于目前香的景德镇的脐橙',\n",
       " '目前咸的南丰的蜜桔有名于历史上香的南丰的陶瓷',\n",
       " '历史上甜的咸的景德镇的脐橙有名于目前赣南的蜜桔',\n",
       " '目前南丰的蜜桔有名于目前香的香的赣南的陶瓷',\n",
       " '历史上香的景德镇的陶瓷有名于目前香的赣南的脐橙',\n",
       " '目前景德镇的陶瓷好于历史上香的咸的香的赣南的蜜桔',\n",
       " '历史上香的咸的景德镇的脐橙有名于历史上南丰的蜜桔',\n",
       " '目前咸的咸的甜的景德镇的蜜桔好于历史上南丰的脐橙',\n",
       " '目前香的咸的景德镇的脐橙有名于历史上南丰的脐橙',\n",
       " '历史上甜的景德镇的陶瓷有名于历史上咸的赣南的陶瓷',\n",
       " '目前赣南的陶瓷有名于目前甜的香的赣南的蜜桔',\n",
       " '目前咸的咸的赣南的脐橙有名于历史上景德镇的蜜桔',\n",
       " '历史上甜的香的南丰的脐橙好于目前甜的景德镇的蜜桔',\n",
       " '目前南丰的脐橙有名于目前香的咸的赣南的蜜桔',\n",
       " '历史上甜的南丰的脐橙好于历史上咸的香的赣南的蜜桔',\n",
       " '历史上香的甜的甜的赣南的脐橙有名于目前南丰的陶瓷',\n",
       " '目前景德镇的蜜桔好于目前咸的甜的香的咸的赣南的脐橙',\n",
       " '目前咸的香的香的景德镇的蜜桔有名于目前南丰的蜜桔',\n",
       " '目前景德镇的陶瓷好于目前甜的香的香的甜的赣南的蜜桔',\n",
       " '历史上景德镇的脐橙好于目前香的咸的香的香的南丰的陶瓷',\n",
       " '目前甜的甜的咸的咸的南丰的脐橙好于历史上南丰的陶瓷',\n",
       " '目前香的赣南的蜜桔有名于历史上咸的咸的景德镇的脐橙',\n",
       " '目前香的赣南的陶瓷有名于目前香的咸的赣南的陶瓷',\n",
       " '目前甜的南丰的陶瓷有名于历史上咸的甜的景德镇的蜜桔',\n",
       " '历史上咸的香的甜的南丰的蜜桔有名于目前南丰的脐橙',\n",
       " '目前咸的咸的南丰的陶瓷有名于历史上咸的赣南的蜜桔',\n",
       " '目前景德镇的蜜桔有名于目前香的甜的咸的香的景德镇的蜜桔',\n",
       " '目前甜的咸的香的香的景德镇的蜜桔有名于历史上南丰的陶瓷',\n",
       " '历史上香的赣南的陶瓷好于目前香的香的香的甜的景德镇的陶瓷',\n",
       " '历史上咸的甜的咸的咸的甜的景德镇的陶瓷好于历史上南丰的陶瓷',\n",
       " '目前咸的香的赣南的脐橙好于目前咸的甜的甜的南丰的脐橙',\n",
       " '历史上咸的南丰的陶瓷好于目前咸的甜的香的甜的赣南的陶瓷',\n",
       " '历史上赣南的陶瓷有名于历史上咸的香的咸的香的南丰的陶瓷',\n",
       " '历史上咸的香的甜的景德镇的脐橙有名于历史上甜的赣南的蜜桔',\n",
       " '历史上香的景德镇的蜜桔有名于历史上香的咸的甜的咸的赣南的陶瓷',\n",
       " '目前南丰的陶瓷好于目前甜的咸的香的甜的咸的咸的南丰的陶瓷',\n",
       " '历史上甜的香的景德镇的脐橙有名于历史上咸的香的咸的景德镇的蜜桔',\n",
       " '目前香的甜的香的景德镇的陶瓷有名于目前香的香的赣南的脐橙',\n",
       " '历史上甜的香的咸的香的咸的赣南的脐橙好于历史上甜的景德镇的脐橙',\n",
       " '目前香的咸的赣南的脐橙有名于目前香的香的甜的赣南的蜜桔',\n",
       " '历史上咸的赣南的脐橙有名于目前甜的香的香的甜的南丰的蜜桔',\n",
       " '目前咸的香的咸的甜的赣南的陶瓷有名于历史上甜的甜的南丰的蜜桔',\n",
       " '历史上香的南丰的脐橙有名于目前咸的咸的咸的咸的甜的香的赣南的陶瓷']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = parse_grammar(grammar)\n",
    "random_generated = [gene(g) for _ in range(100)]\n",
    "#\"|\" == > 'or'\n",
    "sorted(random_generated, key=langauge_model_of_2_gram, reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 如上实验所示，Language Model通过给定语料库得出两词或多词搭配顺序的合理性（概率），\n",
    "从而在语音识别、搜狗输入拼音、自动校正搜索、异常检测等方面可以利用Language Model来判断输入文段的合理性，\n",
    "并重组输入预料顺序之后给出最优预料顺序。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compared to the previous learned parsing and pattern match problems. What's the advantage and disavantage of Probability Based Methods? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ans: __\n",
    "\n",
    "* __Advantage:__ 在parsing and pattern match基础上，Probability Based Methods还可以判断出每句话的合理性，从而让随机数出的语句按合理性排序输出。\n",
    "* __Disadvantage:__ 由于多义词或者不同场景下出现词组概率的变化等语言复杂性，一般的Probability Based Methods也无法解决。而且在n-Gram Model中随着n增加，n个词组会很难匹配到，判断前后语句合理性的功能就受限了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional)  How to solve *OOV* problem?\n",
    "\n",
    "If some words are not in our dictionary or corpus. When we using language model, we need to overcome this `out-of-vocabulary`(OOV) problems. There are so many intelligent man to solve this probelm. \n",
    "\n",
    "-- \n",
    "\n",
    "The first question is: \n",
    "\n",
    "**Q1: How did you solve this problem in your programming task?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ans: __\n",
    "\n",
    "在上面项目中将OOV设为频率出现为1的词来处理。同时我们也有以下解决思路（尚未实施）：\n",
    "\n",
    "导入中文近义词库，将输入的test语句用近义词替换出所有可能test1, test2,..., testn。再分别做测试，选出最优结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the sencond question is: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2: Read about the 'Turing-Good Estimator', can explain the main points about this method, and may implement this method in your programming task**\n",
    "\n",
    "Reference: \n",
    "+ https://www.wikiwand.com/en/Good%E2%80%93Turing_frequency_estimation\n",
    "+ https://github.com/Computing-Intelligence/References/blob/master/NLP/Natural-Language-Processing.pdf, Page-37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> coding in here\n",
    "\n",
    "pending..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
