{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f77Fyc3CQh9C"
   },
   "source": [
    "## 第一部分 背景与问题描述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SlfUVjd2Qh9E"
   },
   "source": [
    "## Environment\n",
    "+ jupyter notebook (测试、试验环境)\n",
    "+ Pycharm （开发环境）\n",
    "+ python3.6\n",
    "+ networkx\n",
    "+ jieba\n",
    "+ numpy, pandas, matplotlib\n",
    "+ gensim\n",
    "+ (optional) bottle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n1yJ9NxtQh9E"
   },
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gjEYPHWsQh9F"
   },
   "source": [
    "在自然语言处理中，有一个常见的问题就是对客户的评价进行分析。 这些用户评论中，包含了大量的有用信息，例如情感分析，或者相关事实描述。 例如，\n",
    "> `“味道不错的面馆，性价比也相当之高，分量很足～女生吃小份，胃口小的，可能吃不完呢。环境在面馆来说算是好的，至少看上去堂子很亮，也比较干净，一般苍蝇馆子还是比不上这个卫生状况的。中午饭点的时候，人很多，人行道上也是要坐满的，隔壁的冒菜馆子，据说是一家，有时候也会开放出来坐吃面的人。“`\n",
    "\n",
    "首先情感是正向的，除此之外我们还能够进行知道这个的几个事实描述：1. 性价比比较高； 2. 装修比较好； 3. 分量足。 \n",
    "\n",
    "这些信息是非常重要宝贵的，不论是对于公司进行商业分析或者要建立一个搜索引擎排序，这些信息都是重要的参考因素。 那么在这个时候，我们就需要进行文本的情感分类了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-adL2vz9Qh9G"
   },
   "source": [
    "**注**: 此次问题的来源数据集来源于大众点评， 这个数据集也被用作 AI 全球挑战赛的数据集。 而且细粒度情感分类这个问题其实在目前而言是一个 *未解之谜*， 人类现在对这个问题并没有什么特别好的方法， 因为人的情感表达真的是很有变化的，例如“我不认为这个地方不好是不对的”。所以这个问题也需要大家在之后做出来基础模型之后，大家多想想办法，*八仙过海，各显神通*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3t3jrBjSQh9H"
   },
   "source": [
    "所以，我们这一次是要使用深度学习的方法，建立一个模型，这个模型能够将一句话进行分类判断，判断出来这句话到底表达了什么重要信息。 说实话，这个看似简单的问题，曾经是困扰了科学家数十年的问题，就算是现在，深度学习，人工智能有了很大的进步，其效果也达不到人们预期的那么好,但是比起前些年已经好多了:)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s98yXhcWQh9I"
   },
   "source": [
    "这个问题我们希望的是，输入一句话，输出是这句话对于以下6大类，20小类进行打标，对于每个小类而言，都会有<** 正面情感, 中性情感, 负面情感, 情感倾向未提及 > ** 这4个类别。 \n",
    "\n",
    "总得来说，我们现在这6大类，20小类的类别如下：\n",
    "\n",
    "+ 位置: location\n",
    "    + 交通是否便利(traffic convenience)\n",
    "    + 距离商圈远近(distance from business district)\n",
    "    + 是否容易寻找(easy to find)\n",
    "+ 服务(service)\t\n",
    "    + 排队等候时间(wait time)\n",
    "    + 服务人员态度(waiter’s attitude)\n",
    "    + 是否容易停车(parking convenience)\n",
    "    + 点菜/上菜速度(serving speed)\n",
    "+ 价格(price)\t\n",
    "    + 价格水平(price level)\n",
    "    + 性价比(cost-effective)\n",
    "    + 折扣力度(discount)\n",
    "+ 环境(environment)\t\n",
    "    + 装修情况(decoration)\n",
    "    + 嘈杂情况(noise)\n",
    "    + 就餐空间(space)\n",
    "    + 卫生情况(cleaness)\n",
    "+ 菜品(dish)\t\n",
    "    + 分量(portion)\n",
    "    + 口感(taste)\n",
    "    + 外观(look)\n",
    "    + 推荐程度(recommendation)\n",
    "+ 其他(others)\t\n",
    "    + 本次消费感受(overall experience)\n",
    "    + 再次消费的意愿(willing to consume again)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ms42fqicQh9J"
   },
   "source": [
    "而为了方便训练数据的标标注，训练数据中，<** 正面情感, 中性情感, 负面情感, 情感倾向未提及 > ** 分别对应与 (1, 0, -1, -2). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1LAFhh28Qh9J"
   },
   "source": [
    "例如说，\n",
    "> `“味道不错的面馆，性价比也相当之高，分量很足～女生吃小份，胃口小的，可能吃不完呢。环境在面馆来说算是好的，至少看上去堂子很亮，也比较干净，一般苍蝇馆子还是比不上这个卫生状况的。中午饭点的时候，人很多，人行道上也是要坐满的，隔壁的冒菜馆子，据说是一家，有时候也会开放出来坐吃面的人。“`\n",
    "\n",
    "这句话在训练数据中的标签就是：\n",
    "\n",
    "+ 交通是否便利(traffic convenience)\t-2 \n",
    "+ 距离商圈远近(distance from business district)\t-2\n",
    "+ 是否容易寻找(easy to find)\t-2\n",
    "+ 排队等候时间(wait time)\t-2\n",
    "+ 服务人员态度(waiter’s attitude)\t-2\n",
    "+ 是否容易停车(parking convenience)\t-2\n",
    "+ 点菜/上菜速度(serving speed)\t-2\n",
    "+ 价格水平(price level)\t-2\n",
    "+ 性价比(cost-effective)\t1\n",
    "+ 折扣力度(discount)\t-2\n",
    "+ 装修情况(decoration)\t1\n",
    "+ 嘈杂情况(noise)\t-2\n",
    "+ 就餐空间(space)\t-2\n",
    "+ 卫生情况(cleaness)\t1\n",
    "+ 分量(portion)\t1\n",
    "+ 口感(taste)\t1\n",
    "+ 外观(look)\t-2\n",
    "+ 推荐程度(recommendation)\t-2\n",
    "+ 次消费感受(overall experience)\t1\n",
    "+ 再次消费的意愿(willing to consume again)\t-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wWR0Rf1IQh9K"
   },
   "source": [
    "## 数据集下载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eizq1kPhQh9M"
   },
   "source": [
    "数据集的下载在 https://challenger.ai/competition/fsauor2018， 大家下载数据集， 以及测试集，注意，训练集我们需要分成 training data, validation data 然后 test data里边的数据绝对不能在训练的时候用。 否则的话就是去了意义。 \n",
    "\n",
    "**注**, 这个数据集之所以被用到了 AI 挑战赛中，是因为其难度很大。 绝大数人在公司中遇到的问题难度**不会**超过这个问题。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "__hKySvOXvun",
    "outputId": "f49d8249-a543-4446-a552-3b45c8af8f43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vh5iKZbqf7Ws"
   },
   "source": [
    "[This Pre-trained WordVec From:](https://www.kaggle.com/guiyihan/fasttext-chinese-word-embedding#cc.zh.300.vec.gz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kq8x_-naGUMs"
   },
   "source": [
    "# [Reference from kaggle competition](https://www.kaggle.com/yekenot/fasttext-crawl-300d-2m/downloads/crawl-300d-2M.vec/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zAgg5yvvGVbw",
    "outputId": "8ff9e3a9-5097-4d56-d371-26b4acbe0c1c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "qAlOXP0mGX8r"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = '/content/drive/My Drive/Colab Notebooks/2comment_classification/cc.zh.300.vec' # consider change this into Chinese Embedding File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ATJIhBNwtMkL"
   },
   "source": [
    "# Clean Chinese Content\n",
    "[Reference Blog](https://www.dlology.com/blog/tutorial-chinese-sentiment-analysis-with-hotel-review-data/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "kZ-7GK8lo4lq"
   },
   "outputs": [],
   "source": [
    "#繁体转简体\n",
    "from langconv import *\n",
    "import jieba\n",
    "\n",
    "def is_CN_char(ch):\n",
    "    return ch >= u'\\u4e00' and ch <= u'\\u9fa5'\n",
    "\n",
    "def cut(string):\n",
    "    return list(jieba.cut(string))\n",
    "\n",
    "def get_stopwords(filename = \"chinese_stopwords.txt\"):\n",
    "    stopwords_dic = open(filename, encoding= 'utf-8')\n",
    "    stopwords = stopwords_dic.readlines()\n",
    "    stopwords = [w.split() for w in stopwords]\n",
    "    stopwords_dic.close()\n",
    "    return stopwords\n",
    "\n",
    "def convert2simple(word):\n",
    "    return Converter('zh-hans').convert(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "uBX0anjWsv4F"
   },
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    stopwords = get_stopwords()\n",
    "    sentence = ''.join(filter(is_CN_char,sentence))\n",
    "    sentence = convert2simple(sentence)\n",
    "    words = [w for w in cut(sentence) if len(w)>1 and w not in stopwords]\n",
    "    words = ' '.join(words)\n",
    "    return words  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "KLZH_SHdswVr"
   },
   "outputs": [],
   "source": [
    "# train[\"content\"] = train[\"content\"].apply(clean_sentence)\n",
    "# test[\"content\"] = test[\"content\"].apply(clean_sentence)\n",
    "# submission[\"content\"] = submission[\"content\"].apply(clean_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "eoWH4qPdvl7V"
   },
   "outputs": [],
   "source": [
    "# train.to_csv(\"/content/drive/My Drive/Colab Notebooks/2comment_classification/train_cut.csv\", index = False)\n",
    "# test.to_csv(\"/content/drive/My Drive/Colab Notebooks/2comment_classification/test_cut.csv\", index = False)\n",
    "# submission.to_csv(\"/content/drive/My Drive/Colab Notebooks/2comment_classification/submission_cut.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Kz1f8wi3yh62"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/2comment_classification/train_cut.csv\")\n",
    "test = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/2comment_classification/test_cut.csv\")\n",
    "submission = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/2comment_classification/submission_cut.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "colab_type": "code",
    "id": "af-SjtDk12Xp",
    "outputId": "bd9fe1b8-4eee-4118-de65-aea3e6a78efa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>location_traffic_convenience</th>\n",
       "      <th>location_distance_from_business_district</th>\n",
       "      <th>location_easy_to_find</th>\n",
       "      <th>service_wait_time</th>\n",
       "      <th>service_waiters_attitude</th>\n",
       "      <th>service_parking_convenience</th>\n",
       "      <th>service_serving_speed</th>\n",
       "      <th>price_level</th>\n",
       "      <th>price_cost_effective</th>\n",
       "      <th>price_discount</th>\n",
       "      <th>environment_decoration</th>\n",
       "      <th>environment_noise</th>\n",
       "      <th>environment_space</th>\n",
       "      <th>environment_cleaness</th>\n",
       "      <th>dish_portion</th>\n",
       "      <th>dish_taste</th>\n",
       "      <th>dish_look</th>\n",
       "      <th>dish_recommendation</th>\n",
       "      <th>others_overall_experience</th>\n",
       "      <th>others_willing_to_consume_again</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>想当年 佘山 时候 没有 三品 香算 镇上 最大 看起来 像样 饭店 菜品 有点 感觉 有杂...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>趁着 国庆节 一家人 白天 山里 玩耍 之后 晚上 决定 李记 搅团 东门外 这家 店门口 ...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  ... others_willing_to_consume_again\n",
       "0   0  ...                               0\n",
       "1   1  ...                               1\n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "donCXxJnrZz_"
   },
   "source": [
    "# 数据预处理及评论分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "2HYZcLKkrZ0D",
    "outputId": "3b1f3135-542f-4cf5-d182-9985cfd39575"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'content', 'location_traffic_convenience',\n",
       "       'location_distance_from_business_district', 'location_easy_to_find',\n",
       "       'service_wait_time', 'service_waiters_attitude',\n",
       "       'service_parking_convenience', 'service_serving_speed', 'price_level',\n",
       "       'price_cost_effective', 'price_discount', 'environment_decoration',\n",
       "       'environment_noise', 'environment_space', 'environment_cleaness',\n",
       "       'dish_portion', 'dish_taste', 'dish_look', 'dish_recommendation',\n",
       "       'others_overall_experience', 'others_willing_to_consume_again'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "3saGdbNbrZ0I"
   },
   "outputs": [],
   "source": [
    "index_row = list(train.columns)\n",
    "index_row.remove(\"id\")\n",
    "index_row.remove(\"content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "tYkjoF-krZ0K"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "row_type_count = defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "7QyCXpGfrZ0O"
   },
   "outputs": [],
   "source": [
    "for i in index_row:\n",
    "    Distribution_types = {i: 0 for i in [1, -1, 0, -2]} #计数初始化\n",
    "    for j in [1, -1, 0, -2]:\n",
    "        Distribution_types[j] = list(train[i]).count(j)\n",
    "    row_type_count[i] = Distribution_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "QHUW-P3vrZ0S"
   },
   "outputs": [],
   "source": [
    "row_type_count = pd.DataFrame(row_type_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "colab_type": "code",
    "id": "ManLGdj_rZ0a",
    "outputId": "a72e950d-1e3e-4f66-8ae6-abd3da8b380d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_traffic_convenience</th>\n",
       "      <th>location_distance_from_business_district</th>\n",
       "      <th>location_easy_to_find</th>\n",
       "      <th>service_wait_time</th>\n",
       "      <th>service_waiters_attitude</th>\n",
       "      <th>service_parking_convenience</th>\n",
       "      <th>service_serving_speed</th>\n",
       "      <th>price_level</th>\n",
       "      <th>price_cost_effective</th>\n",
       "      <th>price_discount</th>\n",
       "      <th>environment_decoration</th>\n",
       "      <th>environment_noise</th>\n",
       "      <th>environment_space</th>\n",
       "      <th>environment_cleaness</th>\n",
       "      <th>dish_portion</th>\n",
       "      <th>dish_taste</th>\n",
       "      <th>dish_look</th>\n",
       "      <th>dish_recommendation</th>\n",
       "      <th>others_overall_experience</th>\n",
       "      <th>others_willing_to_consume_again</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-2</th>\n",
       "      <td>81382</td>\n",
       "      <td>83680</td>\n",
       "      <td>80605</td>\n",
       "      <td>92763</td>\n",
       "      <td>42410</td>\n",
       "      <td>98276</td>\n",
       "      <td>88700</td>\n",
       "      <td>52820</td>\n",
       "      <td>80242</td>\n",
       "      <td>64243</td>\n",
       "      <td>53916</td>\n",
       "      <td>73445</td>\n",
       "      <td>65398</td>\n",
       "      <td>66598</td>\n",
       "      <td>56917</td>\n",
       "      <td>5070</td>\n",
       "      <td>75975</td>\n",
       "      <td>84767</td>\n",
       "      <td>2110</td>\n",
       "      <td>65600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>1318</td>\n",
       "      <td>586</td>\n",
       "      <td>3976</td>\n",
       "      <td>3034</td>\n",
       "      <td>8684</td>\n",
       "      <td>1323</td>\n",
       "      <td>5487</td>\n",
       "      <td>12375</td>\n",
       "      <td>3011</td>\n",
       "      <td>1716</td>\n",
       "      <td>2139</td>\n",
       "      <td>3077</td>\n",
       "      <td>5706</td>\n",
       "      <td>4513</td>\n",
       "      <td>10018</td>\n",
       "      <td>4363</td>\n",
       "      <td>3178</td>\n",
       "      <td>2275</td>\n",
       "      <td>9384</td>\n",
       "      <td>4159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1046</td>\n",
       "      <td>533</td>\n",
       "      <td>2472</td>\n",
       "      <td>4382</td>\n",
       "      <td>12534</td>\n",
       "      <td>1456</td>\n",
       "      <td>2379</td>\n",
       "      <td>24249</td>\n",
       "      <td>3072</td>\n",
       "      <td>18255</td>\n",
       "      <td>9492</td>\n",
       "      <td>4843</td>\n",
       "      <td>9262</td>\n",
       "      <td>4703</td>\n",
       "      <td>9506</td>\n",
       "      <td>40200</td>\n",
       "      <td>4675</td>\n",
       "      <td>1988</td>\n",
       "      <td>23436</td>\n",
       "      <td>2913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21254</td>\n",
       "      <td>20201</td>\n",
       "      <td>17947</td>\n",
       "      <td>4821</td>\n",
       "      <td>41372</td>\n",
       "      <td>3945</td>\n",
       "      <td>8434</td>\n",
       "      <td>15556</td>\n",
       "      <td>18675</td>\n",
       "      <td>20786</td>\n",
       "      <td>39453</td>\n",
       "      <td>23635</td>\n",
       "      <td>24634</td>\n",
       "      <td>29186</td>\n",
       "      <td>28559</td>\n",
       "      <td>55367</td>\n",
       "      <td>21172</td>\n",
       "      <td>15970</td>\n",
       "      <td>70070</td>\n",
       "      <td>32328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    location_traffic_convenience  ...  others_willing_to_consume_again\n",
       "-2                         81382  ...                            65600\n",
       "-1                          1318  ...                             4159\n",
       " 0                          1046  ...                             2913\n",
       " 1                         21254  ...                            32328\n",
       "\n",
       "[4 rows x 20 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_type_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Enz2vJU_rZ0e"
   },
   "outputs": [],
   "source": [
    "row_type_percent = row_type_count/len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "colab_type": "code",
    "id": "HiRnWTULrZ0h",
    "outputId": "e5f3d899-13f9-4ae1-b5f0-80264ac1980c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_traffic_convenience</th>\n",
       "      <th>location_distance_from_business_district</th>\n",
       "      <th>location_easy_to_find</th>\n",
       "      <th>service_wait_time</th>\n",
       "      <th>service_waiters_attitude</th>\n",
       "      <th>service_parking_convenience</th>\n",
       "      <th>service_serving_speed</th>\n",
       "      <th>price_level</th>\n",
       "      <th>price_cost_effective</th>\n",
       "      <th>price_discount</th>\n",
       "      <th>environment_decoration</th>\n",
       "      <th>environment_noise</th>\n",
       "      <th>environment_space</th>\n",
       "      <th>environment_cleaness</th>\n",
       "      <th>dish_portion</th>\n",
       "      <th>dish_taste</th>\n",
       "      <th>dish_look</th>\n",
       "      <th>dish_recommendation</th>\n",
       "      <th>others_overall_experience</th>\n",
       "      <th>others_willing_to_consume_again</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-2</th>\n",
       "      <td>0.775067</td>\n",
       "      <td>0.796952</td>\n",
       "      <td>0.767667</td>\n",
       "      <td>0.883457</td>\n",
       "      <td>0.403905</td>\n",
       "      <td>0.935962</td>\n",
       "      <td>0.844762</td>\n",
       "      <td>0.503048</td>\n",
       "      <td>0.764210</td>\n",
       "      <td>0.611838</td>\n",
       "      <td>0.513486</td>\n",
       "      <td>0.699476</td>\n",
       "      <td>0.622838</td>\n",
       "      <td>0.634267</td>\n",
       "      <td>0.542067</td>\n",
       "      <td>0.048286</td>\n",
       "      <td>0.723571</td>\n",
       "      <td>0.807305</td>\n",
       "      <td>0.020095</td>\n",
       "      <td>0.624762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>0.012552</td>\n",
       "      <td>0.005581</td>\n",
       "      <td>0.037867</td>\n",
       "      <td>0.028895</td>\n",
       "      <td>0.082705</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>0.052257</td>\n",
       "      <td>0.117857</td>\n",
       "      <td>0.028676</td>\n",
       "      <td>0.016343</td>\n",
       "      <td>0.020371</td>\n",
       "      <td>0.029305</td>\n",
       "      <td>0.054343</td>\n",
       "      <td>0.042981</td>\n",
       "      <td>0.095410</td>\n",
       "      <td>0.041552</td>\n",
       "      <td>0.030267</td>\n",
       "      <td>0.021667</td>\n",
       "      <td>0.089371</td>\n",
       "      <td>0.039610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.009962</td>\n",
       "      <td>0.005076</td>\n",
       "      <td>0.023543</td>\n",
       "      <td>0.041733</td>\n",
       "      <td>0.119371</td>\n",
       "      <td>0.013867</td>\n",
       "      <td>0.022657</td>\n",
       "      <td>0.230943</td>\n",
       "      <td>0.029257</td>\n",
       "      <td>0.173857</td>\n",
       "      <td>0.090400</td>\n",
       "      <td>0.046124</td>\n",
       "      <td>0.088210</td>\n",
       "      <td>0.044790</td>\n",
       "      <td>0.090533</td>\n",
       "      <td>0.382857</td>\n",
       "      <td>0.044524</td>\n",
       "      <td>0.018933</td>\n",
       "      <td>0.223200</td>\n",
       "      <td>0.027743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.202419</td>\n",
       "      <td>0.192390</td>\n",
       "      <td>0.170924</td>\n",
       "      <td>0.045914</td>\n",
       "      <td>0.394019</td>\n",
       "      <td>0.037571</td>\n",
       "      <td>0.080324</td>\n",
       "      <td>0.148152</td>\n",
       "      <td>0.177857</td>\n",
       "      <td>0.197962</td>\n",
       "      <td>0.375743</td>\n",
       "      <td>0.225095</td>\n",
       "      <td>0.234610</td>\n",
       "      <td>0.277962</td>\n",
       "      <td>0.271990</td>\n",
       "      <td>0.527305</td>\n",
       "      <td>0.201638</td>\n",
       "      <td>0.152095</td>\n",
       "      <td>0.667333</td>\n",
       "      <td>0.307886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    location_traffic_convenience  ...  others_willing_to_consume_again\n",
       "-2                      0.775067  ...                         0.624762\n",
       "-1                      0.012552  ...                         0.039610\n",
       " 0                      0.009962  ...                         0.027743\n",
       " 1                      0.202419  ...                         0.307886\n",
       "\n",
       "[4 rows x 20 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_type_percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-JBaVOMWrZ0t"
   },
   "source": [
    "## 评论分析\n",
    "- **差评率分析：** 可见类别为-1（差评）的评论数占比最少，但是，该类评论是最重要的以及我们最关心的类别。之后我们对店家提出改进措施也是主要针对该类评论\n",
    "- **好评率分析：** 类别为1（好评）的评论数占比总体不少，在显示生活中也是如此，大部分消费者给出的评论往往是好评。分析店家的好评率也很重要，这对于横向比较以及纵向挖掘店家具体优势点很有帮助。\n",
    "- **提及率分析：** 其次才考虑-2（未提及），提及率体现了消费者对某个特征的关心程度，店家应该针对高提及率的特征进行布局。\n",
    "- **合并变量：** 最后才考虑0（提及但为表达明确感情），这个变量扰动最大，有些消费者提到某个特点，但是1.未表达感情或者2.即说喜欢有说不喜欢的词；比如在dish_taste和price_level该特征中0的比例比较高，因为“吃起来”、“味道”|“价格”等高频词都将评论归为dish_taste，但是消费者的评论出现1和2的情况较常见。考虑到该类别（0）的重要性较小，且占比总体不高，**我们将其合并到-1该类中，统称为not_positive**。\n",
    "- **合并影响：** **合并的弊端：**这样分析的结果往往比真实结果的差评率高一些，尤其是在dish_taste和price_level上。**合并的好处**：1. 分析次数减半，提高了极大的便利；2.一定程度弥补了“评论的客户多为好评客户”以及“模糊评论往往不大满意”的数据缺陷"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S41RCUasrZ0u"
   },
   "source": [
    "# 变量转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "hCZNf8aPrZ0w"
   },
   "outputs": [],
   "source": [
    "# previous columns\n",
    "pre_col = list(train.columns)\n",
    "for x in [\"id\", \"content\"]:\n",
    "    pre_col.remove(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "k2rPIsb9rZ00"
   },
   "outputs": [],
   "source": [
    "suffix_type = [\"mentioned\", \"positive\"]\n",
    "\n",
    "# new columns\n",
    "new_col = []\n",
    "for i in pre_col:\n",
    "    for j in suffix_type:\n",
    "        new_col.append(i + \"_\" + j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZI_6QQqtrZ05",
    "outputId": "79e3d9e2-aa04-46ab-f6ec-5f9ff6886116"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "S9h1v4-prZ0-"
   },
   "outputs": [],
   "source": [
    "for i in pre_col:\n",
    "    for j in suffix_type:\n",
    "        new_col_i_j = i + \"_\" + j\n",
    "        if j == \"mentioned\":\n",
    "            train[new_col_i_j] = np.where(train[i] == -2, 0, 1)\n",
    "        elif j == \"positive\":\n",
    "            train[new_col_i_j] = np.where(train[i] == 1, 1, 0)\n",
    "        else:\n",
    "            print(\"error\")\n",
    "\n",
    "#train.to_csv(\"/content/drive/My Drive/Colab Notebooks/2comment_classification/train_cut_add_new_col.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "colab_type": "code",
    "id": "kwxwRLdBrZ1B",
    "outputId": "a754dfc8-89b7-4b5b-d582-8ae3d21d73ae"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>location_traffic_convenience</th>\n",
       "      <th>location_distance_from_business_district</th>\n",
       "      <th>location_easy_to_find</th>\n",
       "      <th>service_wait_time</th>\n",
       "      <th>service_waiters_attitude</th>\n",
       "      <th>service_parking_convenience</th>\n",
       "      <th>service_serving_speed</th>\n",
       "      <th>price_level</th>\n",
       "      <th>price_cost_effective</th>\n",
       "      <th>price_discount</th>\n",
       "      <th>environment_decoration</th>\n",
       "      <th>environment_noise</th>\n",
       "      <th>environment_space</th>\n",
       "      <th>environment_cleaness</th>\n",
       "      <th>dish_portion</th>\n",
       "      <th>dish_taste</th>\n",
       "      <th>dish_look</th>\n",
       "      <th>dish_recommendation</th>\n",
       "      <th>others_overall_experience</th>\n",
       "      <th>others_willing_to_consume_again</th>\n",
       "      <th>location_traffic_convenience_mentioned</th>\n",
       "      <th>location_traffic_convenience_positive</th>\n",
       "      <th>location_distance_from_business_district_mentioned</th>\n",
       "      <th>location_distance_from_business_district_positive</th>\n",
       "      <th>location_easy_to_find_mentioned</th>\n",
       "      <th>location_easy_to_find_positive</th>\n",
       "      <th>service_wait_time_mentioned</th>\n",
       "      <th>service_wait_time_positive</th>\n",
       "      <th>service_waiters_attitude_mentioned</th>\n",
       "      <th>service_waiters_attitude_positive</th>\n",
       "      <th>service_parking_convenience_mentioned</th>\n",
       "      <th>service_parking_convenience_positive</th>\n",
       "      <th>service_serving_speed_mentioned</th>\n",
       "      <th>service_serving_speed_positive</th>\n",
       "      <th>price_level_mentioned</th>\n",
       "      <th>price_level_positive</th>\n",
       "      <th>price_cost_effective_mentioned</th>\n",
       "      <th>price_cost_effective_positive</th>\n",
       "      <th>price_discount_mentioned</th>\n",
       "      <th>price_discount_positive</th>\n",
       "      <th>environment_decoration_mentioned</th>\n",
       "      <th>environment_decoration_positive</th>\n",
       "      <th>environment_noise_mentioned</th>\n",
       "      <th>environment_noise_positive</th>\n",
       "      <th>environment_space_mentioned</th>\n",
       "      <th>environment_space_positive</th>\n",
       "      <th>environment_cleaness_mentioned</th>\n",
       "      <th>environment_cleaness_positive</th>\n",
       "      <th>dish_portion_mentioned</th>\n",
       "      <th>dish_portion_positive</th>\n",
       "      <th>dish_taste_mentioned</th>\n",
       "      <th>dish_taste_positive</th>\n",
       "      <th>dish_look_mentioned</th>\n",
       "      <th>dish_look_positive</th>\n",
       "      <th>dish_recommendation_mentioned</th>\n",
       "      <th>dish_recommendation_positive</th>\n",
       "      <th>others_overall_experience_mentioned</th>\n",
       "      <th>others_overall_experience_positive</th>\n",
       "      <th>others_willing_to_consume_again_mentioned</th>\n",
       "      <th>others_willing_to_consume_again_positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>吼吼 吼萌 棒棒糖 大众 点评 霸王餐 可爱 一直 好奇 这个 棒棒糖 怎么 东西 大众 点...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>第三次 参加 大众 点评 霸王餐 活动 这家 整体 感觉 一般 首先 环境 只能 中等 其次...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  ... others_willing_to_consume_again_positive\n",
       "0   0  ...                                        0\n",
       "1   1  ...                                        0\n",
       "\n",
       "[2 rows x 62 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "opCjV7UGrZ1E"
   },
   "outputs": [],
   "source": [
    "for i in pre_col:\n",
    "    for j in suffix_type:\n",
    "        new_col_i_j = i + \"_\" + j\n",
    "        if j == \"mentioned\":\n",
    "            test[new_col_i_j] = np.where(test[i] == -2, 0, 1)\n",
    "        elif j == \"positive\":\n",
    "            test[new_col_i_j] = np.where(test[i] == 1, 1, 0)\n",
    "        else:\n",
    "            print(\"error\")\n",
    "\n",
    "#test.to_csv(\"test_cut_add_new_col.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "fHqimsk3rZ1J"
   },
   "outputs": [],
   "source": [
    "for i in pre_col:\n",
    "    for j in suffix_type:\n",
    "        new_col_i_j = i + \"_\" + j\n",
    "        if j == \"mentioned\":\n",
    "            submission[new_col_i_j] = np.where(submission[i] == -2, 0, 1)\n",
    "        elif j == \"positive\":\n",
    "            submission[new_col_i_j] = np.where(submission[i] == 1, 1, 0)\n",
    "        else:\n",
    "            print(\"error\")\n",
    "\n",
    "#submission.to_csv(\"submission_cut_add_new_col.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "colab_type": "code",
    "id": "R0lKn5b32fhN",
    "outputId": "25c9cf2a-5461-4e05-8f5e-b84e4ce92ac4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>location_traffic_convenience</th>\n",
       "      <th>location_distance_from_business_district</th>\n",
       "      <th>location_easy_to_find</th>\n",
       "      <th>service_wait_time</th>\n",
       "      <th>service_waiters_attitude</th>\n",
       "      <th>service_parking_convenience</th>\n",
       "      <th>service_serving_speed</th>\n",
       "      <th>price_level</th>\n",
       "      <th>price_cost_effective</th>\n",
       "      <th>price_discount</th>\n",
       "      <th>environment_decoration</th>\n",
       "      <th>environment_noise</th>\n",
       "      <th>environment_space</th>\n",
       "      <th>environment_cleaness</th>\n",
       "      <th>dish_portion</th>\n",
       "      <th>dish_taste</th>\n",
       "      <th>dish_look</th>\n",
       "      <th>dish_recommendation</th>\n",
       "      <th>others_overall_experience</th>\n",
       "      <th>others_willing_to_consume_again</th>\n",
       "      <th>location_traffic_convenience_mentioned</th>\n",
       "      <th>location_traffic_convenience_positive</th>\n",
       "      <th>location_distance_from_business_district_mentioned</th>\n",
       "      <th>location_distance_from_business_district_positive</th>\n",
       "      <th>location_easy_to_find_mentioned</th>\n",
       "      <th>location_easy_to_find_positive</th>\n",
       "      <th>service_wait_time_mentioned</th>\n",
       "      <th>service_wait_time_positive</th>\n",
       "      <th>service_waiters_attitude_mentioned</th>\n",
       "      <th>service_waiters_attitude_positive</th>\n",
       "      <th>service_parking_convenience_mentioned</th>\n",
       "      <th>service_parking_convenience_positive</th>\n",
       "      <th>service_serving_speed_mentioned</th>\n",
       "      <th>service_serving_speed_positive</th>\n",
       "      <th>price_level_mentioned</th>\n",
       "      <th>price_level_positive</th>\n",
       "      <th>price_cost_effective_mentioned</th>\n",
       "      <th>price_cost_effective_positive</th>\n",
       "      <th>price_discount_mentioned</th>\n",
       "      <th>price_discount_positive</th>\n",
       "      <th>environment_decoration_mentioned</th>\n",
       "      <th>environment_decoration_positive</th>\n",
       "      <th>environment_noise_mentioned</th>\n",
       "      <th>environment_noise_positive</th>\n",
       "      <th>environment_space_mentioned</th>\n",
       "      <th>environment_space_positive</th>\n",
       "      <th>environment_cleaness_mentioned</th>\n",
       "      <th>environment_cleaness_positive</th>\n",
       "      <th>dish_portion_mentioned</th>\n",
       "      <th>dish_portion_positive</th>\n",
       "      <th>dish_taste_mentioned</th>\n",
       "      <th>dish_taste_positive</th>\n",
       "      <th>dish_look_mentioned</th>\n",
       "      <th>dish_look_positive</th>\n",
       "      <th>dish_recommendation_mentioned</th>\n",
       "      <th>dish_recommendation_positive</th>\n",
       "      <th>others_overall_experience_mentioned</th>\n",
       "      <th>others_overall_experience_positive</th>\n",
       "      <th>others_willing_to_consume_again_mentioned</th>\n",
       "      <th>others_willing_to_consume_again_positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>想当年 佘山 时候 没有 三品 香算 镇上 最大 看起来 像样 饭店 菜品 有点 感觉 有杂...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>趁着 国庆节 一家人 白天 山里 玩耍 之后 晚上 决定 李记 搅团 东门外 这家 店门口 ...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  ... others_willing_to_consume_again_positive\n",
       "0   0  ...                                        0\n",
       "1   1  ...                                        1\n",
       "\n",
       "[2 rows x 62 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4EXCaWVKrZ1P"
   },
   "source": [
    "# 变量分组训练与测试-7组*6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "YLTUSS71rZ1U",
    "outputId": "9a0f8304-5f8b-4d71-ba73-08f198c54a6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['location_traffic_convenience_mentioned',\n",
       " 'location_traffic_convenience_positive']"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_col[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "rtTQKQrhrZ1W"
   },
   "outputs": [],
   "source": [
    "group = []\n",
    "for i in [0, 6, 14, 20, 26, 32, 36]:\n",
    "    if i < 36:\n",
    "        group.append(new_col[i:i+6])\n",
    "    else:\n",
    "        group.append(new_col[i:] + new_col[12:14]) # 人为安排，尽可能让同一大类在一组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "colab_type": "code",
    "id": "KY0f86W0rZ1Z",
    "outputId": "f61dedbe-440f-483b-b470-57115bab7502",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['location_traffic_convenience_mentioned',\n",
       "  'location_traffic_convenience_positive',\n",
       "  'location_distance_from_business_district_mentioned',\n",
       "  'location_distance_from_business_district_positive',\n",
       "  'location_easy_to_find_mentioned',\n",
       "  'location_easy_to_find_positive'],\n",
       " ['service_wait_time_mentioned',\n",
       "  'service_wait_time_positive',\n",
       "  'service_waiters_attitude_mentioned',\n",
       "  'service_waiters_attitude_positive',\n",
       "  'service_parking_convenience_mentioned',\n",
       "  'service_parking_convenience_positive'],\n",
       " ['price_level_mentioned',\n",
       "  'price_level_positive',\n",
       "  'price_cost_effective_mentioned',\n",
       "  'price_cost_effective_positive',\n",
       "  'price_discount_mentioned',\n",
       "  'price_discount_positive'],\n",
       " ['environment_decoration_mentioned',\n",
       "  'environment_decoration_positive',\n",
       "  'environment_noise_mentioned',\n",
       "  'environment_noise_positive',\n",
       "  'environment_space_mentioned',\n",
       "  'environment_space_positive'],\n",
       " ['environment_cleaness_mentioned',\n",
       "  'environment_cleaness_positive',\n",
       "  'dish_portion_mentioned',\n",
       "  'dish_portion_positive',\n",
       "  'dish_taste_mentioned',\n",
       "  'dish_taste_positive'],\n",
       " ['dish_look_mentioned',\n",
       "  'dish_look_positive',\n",
       "  'dish_recommendation_mentioned',\n",
       "  'dish_recommendation_positive',\n",
       "  'others_overall_experience_mentioned',\n",
       "  'others_overall_experience_positive'],\n",
       " ['others_overall_experience_mentioned',\n",
       "  'others_overall_experience_positive',\n",
       "  'others_willing_to_consume_again_mentioned',\n",
       "  'others_willing_to_consume_again_positive',\n",
       "  'service_serving_speed_mentioned',\n",
       "  'service_serving_speed_positive']]"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G1nyKboKrZ1c"
   },
   "source": [
    "# 循环7次训练，打印结果并预测submission数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "LnaxM79frZ1c"
   },
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "M6qduZ1IrZ1h"
   },
   "outputs": [],
   "source": [
    "def get_model():    \n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.4)(x)\n",
    "    x = Reshape((maxlen, embed_size, 1))(x)\n",
    "\n",
    "    conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embed_size), kernel_initializer='normal',\n",
    "                                                                                  activation='elu')(x)\n",
    "    conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embed_size), kernel_initializer='normal',\n",
    "                                                                                  activation='elu')(x)\n",
    "    conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embed_size), kernel_initializer='normal',\n",
    "                                                                                  activation='elu')(x)\n",
    "    conv_3 = Conv2D(num_filters, kernel_size=(filter_sizes[3], embed_size), kernel_initializer='normal',\n",
    "                                                                                  activation='elu')(x)\n",
    "\n",
    "    maxpool_0 = MaxPool2D(pool_size=(maxlen - filter_sizes[0] + 1, 1))(conv_0)\n",
    "    maxpool_1 = MaxPool2D(pool_size=(maxlen - filter_sizes[1] + 1, 1))(conv_1)\n",
    "    maxpool_2 = MaxPool2D(pool_size=(maxlen - filter_sizes[2] + 1, 1))(conv_2)\n",
    "    maxpool_3 = MaxPool2D(pool_size=(maxlen - filter_sizes[3] + 1, 1))(conv_3)\n",
    "\n",
    "    z = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2, maxpool_3])   \n",
    "    z = Flatten()(z)\n",
    "    z = Dropout(0.1)(z)\n",
    "\n",
    "    outp = Dense(6, activation=\"sigmoid\")(z) # 输出层维度为6，与预测变量数保持一致\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "def save_model(model, group_i):\n",
    "  # serialize model to JSON\n",
    "  model_json = model.to_json()\n",
    "  with open(\"/content/drive/My Drive/Colab Notebooks/2comment_classification/saved_model/model\" + str(group_i) + \".json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "  # serialize weights to HDF5\n",
    "  model.save_weights(\"/content/drive/My Drive/Colab Notebooks/2comment_classification/saved_model/model\" + str(group_i) + \".h5\")\n",
    "  print(\"Saved model to disk\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "bYM8davrrZ1k",
    "outputId": "15a9b64c-26a9-4eaf-8320-76167acbaae5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 99750 samples, validate on 5250 samples\n",
      "Epoch 1/3\n",
      " - 23s - loss: 0.3166 - acc: 0.8770 - val_loss: 0.2384 - val_acc: 0.9119\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.928242 \n",
      "\n",
      "Epoch 2/3\n",
      " - 22s - loss: 0.2324 - acc: 0.9121 - val_loss: 0.2246 - val_acc: 0.9162\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.940003 \n",
      "\n",
      "Epoch 3/3\n",
      " - 23s - loss: 0.2094 - acc: 0.9187 - val_loss: 0.2237 - val_acc: 0.9165\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.940799 \n",
      "\n",
      "############ finish第 1 组输入 ############\n",
      "Train on 99750 samples, validate on 5250 samples\n",
      "Epoch 1/3\n",
      " - 24s - loss: 0.2413 - acc: 0.9043 - val_loss: 0.1713 - val_acc: 0.9383\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.928728 \n",
      "\n",
      "Epoch 2/3\n",
      " - 23s - loss: 0.1665 - acc: 0.9388 - val_loss: 0.1591 - val_acc: 0.9427\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.937617 \n",
      "\n",
      "Epoch 3/3\n",
      " - 23s - loss: 0.1488 - acc: 0.9455 - val_loss: 0.1549 - val_acc: 0.9438\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.939374 \n",
      "\n",
      "############ finish第 2 组输入 ############\n",
      "Train on 99750 samples, validate on 5250 samples\n",
      "Epoch 1/3\n",
      " - 24s - loss: 0.3749 - acc: 0.8368 - val_loss: 0.2894 - val_acc: 0.8814\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.918742 \n",
      "\n",
      "Epoch 2/3\n",
      " - 24s - loss: 0.2883 - acc: 0.8790 - val_loss: 0.2764 - val_acc: 0.8872\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.926919 \n",
      "\n",
      "Epoch 3/3\n",
      " - 23s - loss: 0.2660 - acc: 0.8892 - val_loss: 0.2748 - val_acc: 0.8879\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.928833 \n",
      "\n",
      "############ finish第 3 组输入 ############\n",
      "Train on 99750 samples, validate on 5250 samples\n",
      "Epoch 1/3\n",
      " - 24s - loss: 0.4203 - acc: 0.8049 - val_loss: 0.3389 - val_acc: 0.8559\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.913760 \n",
      "\n",
      "Epoch 2/3\n",
      " - 24s - loss: 0.3282 - acc: 0.8580 - val_loss: 0.3231 - val_acc: 0.8636\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.922992 \n",
      "\n",
      "Epoch 3/3\n",
      " - 23s - loss: 0.2976 - acc: 0.8731 - val_loss: 0.3207 - val_acc: 0.8640\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.924588 \n",
      "\n",
      "############ finish第 4 组输入 ############\n",
      "Train on 99750 samples, validate on 5250 samples\n",
      "Epoch 1/3\n",
      " - 24s - loss: 0.4532 - acc: 0.7764 - val_loss: 0.3731 - val_acc: 0.8316\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.874294 \n",
      "\n",
      "Epoch 2/3\n",
      " - 24s - loss: 0.3661 - acc: 0.8351 - val_loss: 0.3503 - val_acc: 0.8434\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.890272 \n",
      "\n",
      "Epoch 3/3\n",
      " - 23s - loss: 0.3333 - acc: 0.8537 - val_loss: 0.3457 - val_acc: 0.8457\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.892329 \n",
      "\n",
      "############ finish第 5 组输入 ############\n",
      "Train on 99750 samples, validate on 5250 samples\n",
      "Epoch 1/3\n",
      " - 24s - loss: 0.3768 - acc: 0.8399 - val_loss: 0.3201 - val_acc: 0.8719\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.807823 \n",
      "\n",
      "Epoch 2/3\n",
      " - 24s - loss: 0.3094 - acc: 0.8733 - val_loss: 0.3068 - val_acc: 0.8787\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.814380 \n",
      "\n",
      "Epoch 3/3\n",
      " - 23s - loss: 0.2843 - acc: 0.8854 - val_loss: 0.3098 - val_acc: 0.8769\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.816205 \n",
      "\n",
      "############ finish第 6 组输入 ############\n",
      "Train on 99750 samples, validate on 5250 samples\n",
      "Epoch 1/3\n",
      " - 24s - loss: 0.3644 - acc: 0.8385 - val_loss: 0.2936 - val_acc: 0.8776\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.827505 \n",
      "\n",
      "Epoch 2/3\n",
      " - 24s - loss: 0.2834 - acc: 0.8833 - val_loss: 0.2786 - val_acc: 0.8888\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.841356 \n",
      "\n",
      "Epoch 3/3\n",
      " - 23s - loss: 0.2560 - acc: 0.8972 - val_loss: 0.2714 - val_acc: 0.8910\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.843494 \n",
      "\n",
      "############ finish第 7 组输入 ############\n"
     ]
    }
   ],
   "source": [
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "test2 = test\n",
    "submission2 = submission\n",
    "\n",
    "for group_i in range(7):\n",
    "    X_train = train[\"content\"].fillna(\"fillna\").values\n",
    "    X_test = test[\"content\"].fillna(\"fillna\").values\n",
    "    X_submission = submission[\"content\"].fillna(\"fillna\").values\n",
    "    \n",
    "    y_train = train[group[group_i]] # 此处用到i，作为7组变量依次放入模型\n",
    "    y_test = test[group[group_i]]\n",
    "    y_submission = submission[group[group_i]]\n",
    "    \n",
    "    y_train_values = y_train.values\n",
    "    y_test_values = y_test.values\n",
    "    y_submission_values = y_submission.values\n",
    "    \n",
    "    # 对X中的分词进行text_to_sequences和pad_sequences处理，转换成数值\n",
    "    max_features = 100000 # 汉语词汇量是英语词汇量5倍左右，考虑用500,000\n",
    "    maxlen = 200\n",
    "    embed_size = 300\n",
    "    \n",
    "    filter_sizes = [1,2,3,5] # 和输入和输出的维度有关\n",
    "    num_filters = 32\n",
    "    \n",
    "    batch_size = 256\n",
    "    epochs = 3 # 迭代3批次，每批次输入256堆数据\n",
    "\n",
    "    tokenizer = text.Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(X_train) + list(X_test) + list(X_submission))\n",
    "    X_train = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test = tokenizer.texts_to_sequences(X_test)\n",
    "    X_submission = tokenizer.texts_to_sequences(X_submission)\n",
    "    \n",
    "    x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "    x_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "    x_submission = sequence.pad_sequences(X_submission, maxlen=maxlen)\n",
    "\n",
    "    # 结合预训练词向量embeddings_index优化embedding_vector\n",
    "    word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    model = get_model() # 自定义函数作为对象赋值给model\n",
    "    \n",
    "    X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train_values, train_size=0.95, random_state=233)\n",
    "    RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "    \n",
    "    hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                 callbacks=[RocAuc], verbose=2)\n",
    "    print(\"############ finish第 \" + str(group_i+1) + \" 组输入 ############\")\n",
    "    \n",
    "    # 存储模型\n",
    "    save_model(model, group_i)\n",
    "    \n",
    "    y_test_pred = model.predict(x_test, batch_size=1024)\n",
    "    test2[group[group_i]] = y_test_pred #存储进新的DF，之后与原DF做比较，计算accuracy\n",
    "    if group_i == 6:\n",
    "        test2.to_csv('/content/drive/My Drive/Colab Notebooks/2comment_classification/test_cut_add_prediction'+str(group_i+1)+'.csv', index=False)\n",
    "    \n",
    "    y_submission_pred = model.predict(x_submission, batch_size=1024)\n",
    "    submission2[group[group_i]] = y_submission_pred #存储进新的DF，之后与原DF做比较，计算accuracy\n",
    "    if group_i == 6:\n",
    "        submission2.to_csv('/content/drive/My Drive/Colab Notebooks/2comment_classification/submission_cut_add_prediction'+str(group_i+1)+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c-PTqaUurZ1p"
   },
   "source": [
    "# 计算总体预测正确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "kjDoxJc7rZ1p"
   },
   "outputs": [],
   "source": [
    "sum_test = [0]*len(new_col)\n",
    "for index, i in enumerate(new_col):\n",
    "    for j in range(len(test)):\n",
    "        if test2[i].iloc[j] == test[i].iloc[j]:\n",
    "            sum_test[index] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "AeAfmlUvrZ1t",
    "outputId": "125dcf09-b6d4-4b5f-c2e9-bd4ae9497a43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print([float(i / len(test)) for i in sum_test])\n",
    "print(float(sum(sum_test) / (len(test) * len(new_col))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "iGSVfht0rZ1v"
   },
   "outputs": [],
   "source": [
    "sum_submission = [0]*len(new_col)\n",
    "for index, i in enumerate(new_col):\n",
    "    for j in range(len(submission)):\n",
    "        if abs(submission2[i].iloc[j] - submission[i].iloc[j]) < 0.5:\n",
    "            sum_submission[index] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "L9d6bUzzrZ1y",
    "outputId": "ef7a7088-f07c-4517-8e50-1e281c6a9004"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9423333333333334, 0.9474666666666667, 0.8803333333333333, 0.8862, 0.907, 0.9318666666666666, 0.9238, 0.96, 0.9172666666666667, 0.8975333333333333, 0.9882, 0.9788666666666667, 0.9265333333333333, 0.9569333333333333, 0.8400666666666666, 0.8782666666666666, 0.9018666666666667, 0.9054, 0.898, 0.8999333333333334, 0.8890666666666667, 0.8709333333333333, 0.8619333333333333, 0.8718666666666667, 0.8328, 0.8647333333333334, 0.8711333333333333, 0.8783333333333333, 0.7842, 0.8195333333333333, 0.961, 0.7751333333333333, 0.8088666666666666, 0.8476, 0.8934, 0.8950666666666667, 0.981, 0.8228666666666666, 0.828, 0.8491333333333333]\n",
      "0.8893616666666667\n"
     ]
    }
   ],
   "source": [
    "print([i / len(submission) for i in sum_submission])\n",
    "print(sum(sum_submission) / (len(submission) * len(new_col)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V6-ljIEeQh9N"
   },
   "source": [
    "## 评价\n",
    "\n",
    "参照 https://challenger.ai/competition/fsauor2018, 这个问题的评价其实就是多个分类的f1 score 的平均值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 查看模型对新评论的预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "validation = pd.read_csv(\"ai_challenger_sentiment_analysis_validationset_20180816/sentiment_analysis_validationset.csv\")\n",
    "content = validation[\"content\"]\n",
    "\n",
    "validation_predict = pd.read_csv(\"submission_cut_add_prediction7.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 抽取含mentioned的变量名，用来筛选所有被提及的指标\n",
    "predict_col = list(validation_predict.columns)\n",
    "all_mentioned = [i for i in predict_col if \"mentioned\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = validation_predict.iloc[2,:] # 抽取第三条查看测试结果\n",
    "\n",
    "# 筛选出mentioned为1的变量名\n",
    "mentioned_1 = []\n",
    "for i in all_mentioned:\n",
    "    if sample[i] >= 0.5:\n",
    "        mentioned_1.append(i)\n",
    "\n",
    "positive = [i[:-10] + \"_positive\" for i in mentioned_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"这家店是我目前吃到的最干净的串串店，目前看到最有意思的串串店。\\n菜单是张试题卷，很有新意，和80后餐厅有点像，比较怀旧，装饮料的杯子也是80年代的搪瓷杯！\\n\\n整体味道还不错，至少锅底看起来挺干净的。不过锅底味道一般，点的鸳鸯锅，基本没吃过白锅，太咸，也不鲜。下次直接红锅就好了。\\n\\n2个人干掉将近60串串串，感觉还是挺能吃的。\\n老板服务什么都还不错，小工就差很多，看来还需要多培训培训。\\n当场点评还能送一杯冰粉，冰粉好大一杯，感觉不需要再点饮料了。\\n\\n唯一让我很想吐槽的就是油碟——花生酱之类的酱料竟然不能续加！！！不能续加！！！上来给我的一碟本来就不多，吃完了还不让加！！！还要再付钱！！！下次去吃，油碟必须加满！\"'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "service_waiters_attitude_positive           0.848832\n",
       "environment_cleaness_positive               0.662375\n",
       "dish_taste_positive                         0.176822\n",
       "others_overall_experience_positive          0.168516\n",
       "others_willing_to_consume_again_positive    0.558983\n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[positive]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 可以看到，“目前吃到的最干净的串串店”体现了环境干净度为正向评价（相符）\n",
    "- “老板服务什么都还不错，小工就差很多，看来还需要多培训培训”体现服务态度还不错（估计是根据关键词“服务”判断的，“小工”表现此处未体现）\n",
    "- “不过锅底味道一般，点的鸳鸯锅，基本没吃过白锅，太咸，也不鲜”体现了口味为负向评价（相符）\n",
    "- 整体消费感受具有负向评价（相符）\n",
    "- 回头率难说，既包含“下次去吃”，又具有一些负面评价。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = validation_predict.iloc[-100,:] # 抽取第三条查看测试结果\n",
    "\n",
    "# 筛选出mentioned为1的变量名\n",
    "mentioned_1 = []\n",
    "for i in all_mentioned:\n",
    "    if sample[i] >= 0.5:\n",
    "        mentioned_1.append(i)\n",
    "\n",
    "positive = [i[:-10] + \"_positive\" for i in mentioned_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"路过德基，不想吃午饭，就来吃他家的下午茶，团购很赞！团了双人下午茶套餐！一壶有机乌龙茶，一杯卡布奇洛咖啡、还有一个三层下午茶甜点！乌龙茶很香，服务员会主动加水，给你倒茶，服务很赞，卡布奇洛咖啡好喝不腻。他家马卡龙甜而不腻，是用蔬菜做的，喜欢可丽路，有焦香的味道，很Q，小汉堡是咸的，口感很清新，瑞士卷颜色很亮丽，奶油不腻，有坚果的小饼很脆很香，最喜欢。总而言之，他家的三层下午茶价格不贵，还好吃，性价比超高。之前吃的素食很棒，摆盘、食物口感都很好，连我这个肉食动物都爱上。\"'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.iloc[-100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "service_waiters_attitude_positive     0.910944\n",
       "price_level_positive                  0.766989\n",
       "price_cost_effective_positive         0.992662\n",
       "price_discount_positive               0.236092\n",
       "dish_taste_positive                   0.911531\n",
       "dish_look_positive                    0.426262\n",
       "others_overall_experience_positive    0.922351\n",
       "Name: 14900, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[positive]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 可以看到，“服务员会主动加水，给你倒茶，服务很赞”，服务员态度正向（相符）\n",
    "- 口味赞了很多，dish_taste_positive相符\n",
    "- “三层下午茶价格不贵，还好吃，性价比超高”，price_cost_effective_positive（相符）\n",
    "- price_level_positive正向评价（相符）\n",
    "- price_discount_positive此处未提及或者正向评价（团购很赞！团了双人下午茶套餐）（不符）\n",
    "- dish_look_positive此处为正向评价（瑞士卷颜色很亮丽）（不符）\n",
    "- 总体消费体验为正向（相符）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = validation_predict.iloc[193,:] # 抽取第三条查看测试结果\n",
    "\n",
    "# 筛选出mentioned为1的变量名\n",
    "mentioned_1 = []\n",
    "for i in all_mentioned:\n",
    "    if sample[i] >= 0.5:\n",
    "        mentioned_1.append(i)\n",
    "\n",
    "positive = [i[:-10] + \"_positive\" for i in mentioned_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"第二次来。这次来吃酸菜鱼火锅，服务员好努力的销58一位的自助火锅，好卖力，我团好了券就不想改计划了婉拒之后服务员又hard sale各种饮料和飞饼。想说经理开会了吧执行不错可惜全程销的很生硬感觉不buy她的客人就做错事了一样，服务员带着不开心的脸走开了。酸菜鱼这次好酸呀额滴神，比冬阴公汤还酸，吃完鱼噜菜和面劲觉得好酸好酸。上饭那个男员工指甲有1cm那么长吧，瞬间觉得好恶心。最后隔壁桌有个素质极低的女的（见图绿裙biao），一声不吭跑来我的台上倒她的洗碗水！真是一声不吭！旁若无人当我这桌的人是空气，尼玛你桌上没缸你特马不会叫服务员拿给你吗？别人桌上的缸她理所当然老倒邋遢洗碗水！这特马什么素质！没下次。\"'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.iloc[193]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "service_waiters_attitude_positive           0.0204602\n",
       "dish_taste_positive                         0.0428773\n",
       "others_overall_experience_positive          0.0456303\n",
       "others_willing_to_consume_again_positive     0.102588\n",
       "Name: 193, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[positive]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 这条评论里面一顿批评，其中涉及到服务员态度负向（相符）\n",
    "- 味道负向（相符）\n",
    "- 整体体验负向（相符）\n",
    "- 回头率负向（相符）\n",
    "- 环境负向未被预测出来（不相符）  可以考虑调低阈值\n",
    "- 比如将0.5调低一点，获得更多mentioned，此处environment_cleaness_mentioned被漏检"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "location_traffic_convenience_mentioned                0.0363344\n",
       "location_distance_from_business_district_mentioned    0.0235446\n",
       "location_easy_to_find_mentioned                       0.0218006\n",
       "service_wait_time_mentioned                           0.0589759\n",
       "service_waiters_attitude_mentioned                     0.989928\n",
       "service_parking_convenience_mentioned                 0.0068225\n",
       "service_serving_speed_mentioned                       0.0375848\n",
       "price_level_mentioned                                  0.311168\n",
       "price_cost_effective_mentioned                        0.0288143\n",
       "price_discount_mentioned                               0.452502\n",
       "environment_decoration_mentioned                       0.072329\n",
       "environment_noise_mentioned                           0.0672673\n",
       "environment_space_mentioned                           0.0850905\n",
       "environment_cleaness_mentioned                         0.459837\n",
       "dish_portion_mentioned                                 0.160806\n",
       "dish_taste_mentioned                                   0.577765\n",
       "dish_look_mentioned                                     0.24382\n",
       "dish_recommendation_mentioned                         0.0415919\n",
       "others_overall_experience_mentioned                    0.987002\n",
       "others_willing_to_consume_again_mentioned              0.712736\n",
       "Name: 193, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[all_mentioned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.038001805543899536"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"environment_cleaness_positive\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zuHKqHL-Qh9O"
   },
   "source": [
    "# 第二部分 基础理论部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OYkDuNtFQh9P"
   },
   "source": [
    "#### Q1: 机器学习中的Loss函数的作用为何？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OLudXdKcQh9Q"
   },
   "source": [
    "回答：评判预测值和真实值之间的误差，Loss函数因模型而变，线性模型可用均方差表示，Logistics模型可用y*log(h) - (1-y)*log(1-h)表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "k3TqsA4qQh9Q"
   },
   "outputs": [],
   "source": [
    "# remove the # before hint, what you find? \n",
    "#hint('8fd9.4e2a.7b54.6848.53ef.4ee5.5199.5f88.591a.ff0c.4f46.662f.4e3b.8981.662f.8981.6d89.53ca.5230.ff1a.4c.6f.73.73.20.51fd.6570.7528.6765.8861.91cf.673a.5668.5b66.4e60.8fc7.7a0b.4e2d.6a21.578b.8868.73b0.7684.ff0c.6211.4eec.901a.8fc7.20.4c.6f.73.73.20.51fd.6570.6765.8fdb.884c.4f18.5316.6a21.578b.6216.8005.9009.62e9.6a21.578b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RECSe_S_Qh9T"
   },
   "source": [
    "####  Q2: 为什么 SVM 适合核函数的方法？（考虑基于拉格朗日距离的 SVM 的 Loss 函数）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-VunB5QDQh9U"
   },
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "jD5y6RdKQh9V"
   },
   "outputs": [],
   "source": [
    "#hint('8be6.60c5.8bf7.53c2.8003.6211.4eec.7684.8bfe.7a0b.89c6.9891.ff0c.4e00.4e2a.4e3b.8981.7684.70b9.662f.ff0c.6700.540e.8bc1.660e.20.53.56.4d.20.6a21.578b.7684.6027.80fd.53ea.4e0e.20.78.5f.69.78.5f.6a.20.7684.4e58.673a.76f8.5173.ff0c.6240.4ee5.6211.4eec.53ef.4ee5.65b9.4fbf.7684.628a.78.5f.69.20.78.5f.6a.20.6620.5c04.5230.67d0.4e2a.65b0.51fd.6570.4e0a.ff0c.4e0d.6539.53d8.5176.20.4c.6f.73.73.20.7684.5355.8c03.6027.5373.53ef')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DkGsX0PkQh9Z"
   },
   "source": [
    "#### Q3: 决策树的 Loss 函数是什么？随机森林是什么？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DqtNjhOUQh9b"
   },
   "source": [
    "回答:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "lrXAA3wqQh9c"
   },
   "outputs": [],
   "source": [
    "#hint(\"\"\"4e00.3001.71b5.7684.548c.6700.5c0f.ff0c.8981.9009.62e9.4e00.4e2a.6761.4ef6.8ba9.8fd9.6b21.5206.7c7b.7684.4e24.8fb9.7ed3.679c.7adf.53ef.80fd.7684.2018.7eaf.2019.2c.20.4f8b.5982.ff0c.6211.4eec.6709.5b.32.2c.20.31.2c.20.30.2c.20.30.2c.20.30.2c.20.30.2c.20.31.2c.20.31.2c.20.31.2c.20.31.2c.20.31.5d.a.5982.679c.6211.4eec.9009.62e9.4e00.4e2a.6761.4ef6.662f.27.662f.4e0d.662f.31.27.ff0c.6211.4eec.53ef.4ee5.5206.6210.5b.30.2c.20.30.2c.20.30.2c.20.30.2c.20.32.5d.2c.20.5b.31.2c.20.31.2c.20.31.2c.20.31.2c.20.31.5d.ff0c.20.4e5f.53ef.4ee5.6761.4ef6.662f.27.662f.4e0d.662f.32.27.2c.20.a.6211.4eec.5c31.53ef.4ee5.5206.6210.5b.32.5d.2c.20.5b.31.2c.20.30.2c.20.30.2c.20.30.2c.20.30.2c.20.31.2c.20.31.2c.20.31.2c.20.31.2c.20.31.5d.ff0c.20.663e.7136.524d.8005.66f4.7eaf.3002.20.5982.679c.540c.5b66.4eec.5fd8.4e86.71b5.7684.6982.5ff5.ff0c.5927.5bb6.8d76.7d27.518d.67e5.4e00.4e0b.ff0c.7136.540e.8ba1.7b97.4e00.4e0b.8fd9.4e24.4e2a.5206.7c7b.7ed3.679c.7684.71b5.662f.591a.5927.ff1b.a.4e8c.3001.968f.673a.68ee.6797.662f.7528.5f88.591a.5c0f.7684.51b3.7b56.6811.7528.6765.6295.7968.7684.96c6.6210.6a21.578b.ff08.45.6e.73.65.6d.62.6c.65.ff09.ff0c.6bcf.4e2a.5c0f.51b3.7b56.6811.4f7f.7528.4e00.90e8.5206.7684.20.66.65.61.74.75.72.65.20.8fdb.884c.8bad.7ec3\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vQZFqRrcQh9g"
   },
   "source": [
    "**插入**：我们在这里快速的过一下如何在 Jupyter 中写数学公式。 这个其实很简单，如果我们要输入一个公式，例如A1, 那么，我们在Jupyter中输入`$A_i$`, 然后 Enter， 是不是就变成了$A_i$? 其实两个`$$`之间的东西就是 Latex 的符号，`$..$`这个我们叫做inline模式，意思就是说你写出来的公式是和你的文字在一行里，如果你`$$..$$``，这个公式就会单独是一行。\n",
    "\n",
    "我们现在再试一个, 输入`$$\\frac{P_i}{\\sum_{j \\in \\mathbf{V}}^NP_j}$$`, 输完之后 Enter， 你看到了什么？ \n",
    "\n",
    "$$\\frac{P_i}{\\sum_{j \\in \\mathbf{V}}^NP_j}$$\n",
    "\n",
    "这个时候会有同学说，可是这些符号，我怎么记得住呢？ 我给大家提供了一个参考手册，大家有空就看看 https://github.com/Artificial-Intelligence-for-NLP/comment-setimental-classification/blob/master/Latex-Symbols.pdf，熟能生巧。 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R15pcMaaQh9h"
   },
   "source": [
    "#### Q4: 使用Latex 写出来决策树希望找到一个 feature，这个 feature 使得熵的和最少的公式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5ZlYTSVOQh9i"
   },
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FvyBJBBJQh9i"
   },
   "source": [
    "##### Q5: 贝叶斯公式的原理是什么？ 我们现在用的贝叶斯分类器为什么是“朴素贝叶斯”， 它为什么朴素？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KdXYMerqQh9k"
   },
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HvCRW9fFQh9l"
   },
   "source": [
    "#### Q6: 神经网络的Loss函数的作用为何？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GqmqolZuQh9m"
   },
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "3IKSXoQQQh9n"
   },
   "outputs": [],
   "source": [
    "#hint(\"\"\"795e.7ecf.7f51.7edc.91cc.9762.7684.4c.6f.73.73.51fd.6570.662f.7528.6765.8861.91cf.6a21.578b.7684.597d.574f.ff0c.4c.6f.73.73.51fd.6570.8d8a.5927.ff0c.9884.6d4b.4e0e.5b9e.9645.7684.8bef.5dee.8d8a.5927.ff0c.9884.6d4b.8d8a.4e0d.51c6.786e.3002.4e3a.4e86.8ba9.9884.6d4b.7ed3.679c.66f4.52a0.7cbe.51c6.ff0c.6211.4eec.8981.51cf.5c11.4c.6f.73.73.51fd.6570.ff0c.901a.8fc7.68af.5ea6.4e0b.964d.6cd5.ff0c.5229.7528.53cd.5411.4f20.64ad.4e0d.505c.8fed.4ee3.8c03.6574.795e.7ecf.7f51.7edc.4e2d.7684.53c2.6570.ff0c.627e.5230.4f7f.4c.6f.73.73.51fd.6570.6700.5c0f.7684.53c2.6570.ff0c.786e.5b9a.6a21.578b.3002\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iaHHFJ4LQh9r"
   },
   "source": [
    "#### Q7: 神经网络的激活函数(activation function)起什么作用？ 如果没有激活函数会怎么样？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KGIy3PSpQh9s"
   },
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "M8jhmcWWQh9t"
   },
   "outputs": [],
   "source": [
    "#hint('6fc0.6d3b.51fd.6570.7528.6765.8fdb.884c.975e.7ebf.6027.53d8.5316.ff0c.4e0d.65ad.5f97.975e.7ebf.6027.53d8.5316.4f7f.5f97.6211.4eec.28.7406.8bba.4e0a.29.53ef.4ee5.62df.5408.4efb.610f.51fd.6570.ff0c.8fd9.4e5f.662f.4e3a.4ec0.4e48.795e.7ecf.7f51.7edc.80fd.60.5b66.4e60.60.7684.539f.56e0.3002.795e.7ecf.7f51.7edc.91cc.8fb9.7684.60.5b66.4e60.60.5176.5b9e.5c31.662f.51fd.6570.62df.5408.7684.610f.601d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sMyq6CLJQh9v"
   },
   "source": [
    "#### Q8: 神经网络的softmax如何理解， 其作用是什么？ 在`答案`中写出softmax的python表达；\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xu8Py5NcQh9w"
   },
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QC7ZKzPHQh9x"
   },
   "source": [
    "#### Q9: 简述 normalized_1 和softmax函数的相同点和不同点， 说明softmax相比normalized_1该函数的优势所在"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3KTT7LbdQh9y"
   },
   "source": [
    "```\n",
    "output = np.array([y1, y2, y3])\n",
    "\n",
    "normalized_1 = output / np.sum(output)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jdmQo9M1Qh9y"
   },
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NKhu8YteQh9z"
   },
   "source": [
    "#### Q10: 写出crossentropy的函数表达式，说明该函数的作用和意义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ley0ArARQh90"
   },
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f6KVZWMmQh91"
   },
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p_piYj_5Qh92"
   },
   "source": [
    ".."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "idJkD7EKQh94"
   },
   "source": [
    ".."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ncC9IEMVQh96"
   },
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5kzdRUMTQh97"
   },
   "source": [
    "----------------- 休息一下，接下来是关于 Word2Vec的 ------------- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XkFRJI_gQh98"
   },
   "source": [
    "#### Q11: 说明word2vec要解决的问题背景， 以及word2vec的基本思路， 说明word2vec比起之前方法的优势；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P7OygHgxQh99"
   },
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xtlgQaR4Qh9-"
   },
   "source": [
    "#### Q12: 说明word2vec的预测目标， predication target, 在答案中写出skip-gram和cbow的预测概率；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eRNnIlcnQh9_"
   },
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zu5_fdzLQh-A"
   },
   "source": [
    "#### Q13: 请说明word2vec的两种常见优化方法，分别阐述其原理；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VnM1Jn0oQh-B"
   },
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YLte0lzGQh-C"
   },
   "source": [
    "#### Q14: 请说明word2vec中哈夫曼树的作用；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O4pXMB6TQh-D"
   },
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mHaYniGqQh-E"
   },
   "source": [
    "#### Q15: 哈夫曼树如何构建？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "KuAInRa5Qh-F"
   },
   "outputs": [],
   "source": [
    "#hint('a.31.2e.20.68.74.74.70.73.3a.2f.2f.67.69.74.68.75.62.2e.63.6f.6d.2f.68.65.69.6e.65.6d.61.6e.2f.70.79.74.68.6f.6e.2d.64.61.74.61.2d.73.74.72.75.63.74.75.72.65.73.2f.62.6c.6f.62.2f.6d.61.73.74.65.72.2f.35.2e.25.32.30.48.65.61.70.2d.62.61.73.65.64.25.32.30.53.74.72.75.63.74.75.72.65.73.2f.68.75.66.66.6d.61.6e.2e.70.79.a.32.2e.20.68.74.74.70.73.3a.2f.2f.67.69.74.68.75.62.2e.63.6f.6d.2f.52.61.52.65.2d.54.65.63.68.6e.6f.6c.6f.67.69.65.73.2f.67.65.6e.73.69.6d.2f.62.6c.6f.62.2f.33.64.35.61.32.31.63.31.63.38.31.32.38.63.62.38.64.64.34.66.36.65.35.31.65.39.65.66.33.64.63.35.61.66.30.30.30.38.37.31.2f.67.65.6e.73.69.6d.2f.6d.6f.64.65.6c.73.2f.64.65.70.72.65.63.61.74.65.64.2f.77.6f.72.64.32.76.65.63.2e.70.79.23.4c.36.37.30.22.a.33.2e.20.68.74.74.70.73.3a.2f.2f.77.77.77.2e.77.69.6b.69.77.61.6e.64.2e.63.6f.6d.2f.65.6e.2f.48.75.66.66.6d.61.6e.5f.63.6f.64.69.6e.67.a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ohuRE6ydQh-J"
   },
   "source": [
    "#### Q16: 在gensim中如何实现词向量？ 请将gensim中实现词向量的代码置于答案中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qiyju6U4Qh-K"
   },
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1MCwBDf1Qh-L"
   },
   "source": [
    "#### Q17: 请说出除了 skip-gram和cbow的其他4中词向量方法的名字， 并且选取其中两个叙述其基本原理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gSMjmW7aQh-L"
   },
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "EpuJDpu9Qh-N"
   },
   "outputs": [],
   "source": [
    "#hint('a.4f.6e.65.68.6f.74.2c.20.47.6c.6f.76.65.2c.43.6f.76.65.2c.45.4d.4c.6f.a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dbYERmRyQh-P"
   },
   "source": [
    "-------------------- 休息一下，接下来是关于 Keras 和 Tensorflow 使用的 -------------- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VDzzITqSQh-Q"
   },
   "source": [
    "大家先熟悉一下什么是MNIST数据集： \n",
    "\n",
    "> http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    ">https://en.wikipedia.org/wiki/MNIST_database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YCKFY-HEQh-R"
   },
   "source": [
    "#### Q18: 参考keras参考手册，构建一个机器学习模型，该模型能够完成使用DNN(deep neural networks) 实现MNIST数据集的分类；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EBy8eGP7Qh-R"
   },
   "source": [
    "关键代码: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fWEHLs-gQh-S"
   },
   "source": [
    "#### Q19:参考tensorflow的参考手册，构建一个机器学习模型，该模型能够完成使用DNN(deep neural networks)实现MNIST数据集的分类；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ufUe7W20Qh-T"
   },
   "source": [
    "关键代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ZDLziYpVQh-U"
   },
   "outputs": [],
   "source": [
    "#hint('68.69.6e.74.73.3a.74.65.6e.73.6f.72.66.6c.6f.77.5b9e.73b0.4d.4e.49.53.54.20.68.74.74.70.73.3a.2f.2f.67.69.74.68.75.62.2e.63.6f.6d.2f.74.65.6e.73.6f.72.66.6c.6f.77.2f.74.65.6e.73.6f.72.66.6c.6f.77.2f.62.6c.6f.62.2f.6d.61.73.74.65.72.2f.74.65.6e.73.6f.72.66.6c.6f.77.2f.65.78.61.6d.70.6c.65.73.2f.75.64.61.63.69.74.79.2f.32.5f.66.75.6c.6c.79.63.6f.6e.6e.65.63.74.65.64.2e.69.70.79.6e.62')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d1sYb_FLQh-X"
   },
   "source": [
    "#### Q20: 参考keras和tensorflow对同一问题的实现，说明keras和tensorflow的异同；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9d8EJbq2Qh-X"
   },
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wtNRq4XJQh-Y"
   },
   "source": [
    "#### Q21: tensorflow 使用 Graph 计算机制的优缺点是什么？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b5gP_CavQh-Z"
   },
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A92dwZHWQh-a"
   },
   "source": [
    "#### Q22: Q18， Q19 的tensorflow 或 keras 模型的训练时准确率和测试集准确率分别是多少？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OcJtqYKVQh-d"
   },
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Mo4KIXyoQh-d"
   },
   "outputs": [],
   "source": [
    "#### Q23: 训练时准确率大于测试集准确率的现象叫什么名字，在神经网络中如何解决该问题？(至少提出5个解决方法)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P5O60V3iQh-g"
   },
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ykKrU_2kQh-h"
   },
   "outputs": [],
   "source": [
    "#### Q24: 请使用自己的语言简述通过正则化 (regularization)减小过拟合的原理；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E0af3kLTQh-k"
   },
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "4-oeNVv5Qh-l"
   },
   "outputs": [],
   "source": [
    "#### Q25: 在tensorflow官方实例中给出的fully connected 神经网络的分类模型中，数据进行了哪些预处理，这些预处理的原因是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XUrNUe30Qh-p"
   },
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "syO0Vn7nQh-p"
   },
   "source": [
    "--------------------- 休息一下，接下来是关于 RNN 和 CNN 的 ---------- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FVMXDbvOQh-q"
   },
   "source": [
    "#### Q26: 简述CNN的原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ft3MGMDQh-r"
   },
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fnoiDHyhQh-r"
   },
   "source": [
    "#### Q27: CNN的 Spatial Invariant是什么意思？ 是如何做到的？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lcT2HE4jQh-s"
   },
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dCHkL_5cQh-t"
   },
   "source": [
    "#### Q28: CNN增加了很多层数，这些层数使用 filter 进行计算。 按说需要拟合的参数变得很多，请问 CNN 是如何解决这个问题的，如何加快速度的？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C522oNW9Qh-t"
   },
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "biYu4E4AQh-u"
   },
   "outputs": [],
   "source": [
    "#hint('a.6d.61.69.6e.20.70.6f.69.6e.74.73.3a.20.50.6f.6f.6c.69.6e.67.2c.20.50.61.72.61.6d.65.74.65.72.20.53.68.61.72.69.6e.67.a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1kZEDrmLQh-x"
   },
   "source": [
    "#### Q29: CNN中的 Batch Normalization有什么意义？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k8uMIizkQh-y"
   },
   "source": [
    "回答:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pip4zNT3Qh-y"
   },
   "source": [
    "#### Q30: CNN中的 Pooling 起到什么作用？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g6VVpZ2TQh-z"
   },
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qz5Rq4SUQh-0"
   },
   "source": [
    "#### Q31: CNN中的 Fully Connect起到什么作业？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TumQXAFjQh-0"
   },
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LlPhS9uiQh-1"
   },
   "source": [
    "#### Q32: 深度网络中的权值初始化有什么讲究？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PufotZp6Qh-2"
   },
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KwFIZTXYQh-2"
   },
   "source": [
    "#### Reading: 参照 Keras 和 Tensorflow 的示例，手敲使用 keras, tensorflow + CNN 实现MNIST分类的问题：\n",
    "\n",
    "+ https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
    "+ https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/4_convolutions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c9BbQZWIQh-4"
   },
   "source": [
    "把代码手敲一遍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "k3g8r291Qh-7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3GPn9cHwQh-_"
   },
   "source": [
    "#### Q33: 简述RNN解决的问题所具有的特点；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5XoC-CEgQh_A"
   },
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "el7tFKP0Qh_A"
   },
   "source": [
    "#### Q34: 写出RNN实现时间或者序列相关的数学实现(见课程slides)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kDc92wS9Qh_C"
   },
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MfjYS4eDQh_D"
   },
   "source": [
    "#### Q35: 简述RNN的两种重要变体的提出原因和基本原理？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LzVkNT2EQh_E"
   },
   "source": [
    "回答: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ye3ahV8QQh_F"
   },
   "source": [
    "#### Q36:  Attentional RNN 以及 Stacked RNN 和 Bi-RNN 分别是什么，其做了什么改动？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2qg-XGcoQh_G"
   },
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "87_Jp3SPQh_G"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s-fl0cFsQh_J"
   },
   "source": [
    "#### Reading 和 CNN 类似，请在 Keras,Tensorflow中查找如何实现 RNN 模型\n",
    "\n",
    "+ https://github.com/Artificial-Intelligence-for-NLP/References/blob/master/AI%20%26%20Machine%20Learning/Hands.On.TensorFlow.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-9QZDYmDQh_J"
   },
   "source": [
    "## 第二部分： 项目解决过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BPQic2K5Qh_K"
   },
   "source": [
    "代码主要在 Pycharm 里边写，jupyter 里边写一个关键步骤就行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TZQIKD2VQh_M"
   },
   "source": [
    "#### Q37: 要实现文本分类或情感分类，文本信息需要进行哪些初始化操作？自己手工实现，keras提供的API，tenorflow提供的API，分别是哪些？请提供关键代码置于下边`回答`中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FoNj1nTjQh_N"
   },
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "4vKe0J2yQh_N"
   },
   "outputs": [],
   "source": [
    "#hint('id_to_word, word_to_id, padding, batched')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lJpeCLLyQh_P"
   },
   "source": [
    "#### Q38 在没有预训练的词向量时候， keras 如何实现embedding操作，即如何依据一个单词的序列获得其向量表示？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QbLxSM4rQh_Q"
   },
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h8k2AhMHQh_R"
   },
   "source": [
    "#### Q 39: 在**有**预先训练的词向量时候，keras和tensorflow又如何实现embeding操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OOibUVZZQh_R"
   },
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a__bGiYaQh_S"
   },
   "source": [
    "#### Q40：基于上文进行的数据预处理，使用keras和tensorflow如何构建神经网络模型？请提供关键代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Moy_CQRNQh_S"
   },
   "source": [
    "### 好的 现在开始切入正题 --\n",
    "\n",
    "其实，我们解决实际问题的时候，很少自己从头到尾写一个神经网络模型，我们往往是找一个效果比较好的类似问题的模型，然后在这个问题上改造。 或者我们在去一个公司的时候，接手的工作也往往是改动以前的模型，所以我们解决这个语义分类问题我们也首先是找一个类似的问题，然后参考一个模型进行修改，变成能够解决我们这个问题的模型。\n",
    "\n",
    "我们以上所以的理论知识，都是用来支持我们做修改，能够看懂别人为何要这样写，然后自己要改哪里。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5g3MzCLeQh_T"
   },
   "source": [
    "kaggle上的“恶意评价识别”这个项目和我们的这个项目是类似的, 大家请首先在这个的 Kernel 里边找到一个公开代码的示例，然后选择一个自己能够看懂且效果较好的模型进行改造。\n",
    "\n",
    "+ https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\n",
    "+ https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/kernels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bBCewoCJQh_T"
   },
   "source": [
    "Kaggle这个问题和我们的问题类似，但是并不是完全一样， 其中最不一样的其实是我们的期望的结果这里, Kaggle 这里的输出是5个类别，然后类别的是0~1直接的数字来预测是否是这个类别，然后我们的客户评价问题中，打标对20个分类的-2, -1, 0, 1四个标记. 一种最简单的方法，是把这个20分类问题变成80分类问题，然后每个分类的输出是0或者1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cu-yRoOMQh_U"
   },
   "source": [
    "#### Q41. 依据Kernel 中选择方法，对数据和代码进行改造，使其符合选择该问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W1SiICVpQh_W"
   },
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oVFzwBNGQh_W"
   },
   "source": [
    "#### Q42. 你现在的模型的准确率是多少？ 如何知道你的模型是不是真的学习了 而不是随机的进行猜测？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ll1QYUM2Qh_X"
   },
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LVIHuxCOQh_Y"
   },
   "source": [
    "#### Q43. 你的模型现在准确度不高的原因，你猜测主要是什么？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gZyvITEDQh_Z"
   },
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WtfRMqqjQh_a"
   },
   "source": [
    "#### Q43. 如前文所述，这个问题很难，其实现在也没有什么万灵药方法。 所以需要同学们多想想如何有效， 可以给大家参考的优化方式有， 修改vocabulary size, embedding size,去掉停用词，重新组合词组等。 并且结合使用LSTM， GRU， Bi-RNN， Stacked， Attentional, regularization, 等各种方法组合进行模型的优化， 至少进行10次优化，每次优化请按照以下步骤填写："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j_F-P0BxQh_b"
   },
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "opSiBAe-Qh_b"
   },
   "source": [
    "回答：\n",
    "\n",
    "---这是一个实例----\n",
    "\n",
    "第1次优化：\n",
    "\n",
    "1. 存在的问题： loss下降太慢；\n",
    "2. 准备进行的优化：减小模型的神经单元数量；\n",
    "3. 期待的结果：loss下降加快；\n",
    "4. 实际结果：loss下降的确加快(或者并没有加快)\n",
    "5. 原因分析：模型神经元数量减小，收敛需要的次数减少，loss下降加快\n",
    "\n",
    "\n",
    "---你的实验优化结构记录在此---\n",
    "\n",
    "**第1次优化**：\n",
    "\n",
    "1. 存在的问题： \n",
    "2. 准备进行的优化：\n",
    "3. 期待的结果：\n",
    "4. 实际结果：\n",
    "5. 原因分析：\n",
    "\n",
    "**第2次优化**：\n",
    "\n",
    "1. 存在的问题： \n",
    "2. 准备进行的优化：\n",
    "3. 期待的结果：\n",
    "4. 实际结果：\n",
    "5. 原因分析：\n",
    "\n",
    "**第3次优化**：\n",
    "\n",
    "1. 存在的问题： \n",
    "2. 准备进行的优化：\n",
    "3. 期待的结果：\n",
    "4. 实际结果：\n",
    "5. 原因分析：\n",
    "\n",
    "**第4次优化**：\n",
    "\n",
    "1. 存在的问题： \n",
    "2. 准备进行的优化：\n",
    "3. 期待的结果：\n",
    "4. 实际结果：\n",
    "5. 原因分析：\n",
    "\n",
    "**第5次优化**：\n",
    "\n",
    "1. 存在的问题： \n",
    "2. 准备进行的优化：\n",
    "3. 期待的结果：\n",
    "4. 实际结果：\n",
    "5. 原因分析：\n",
    "\n",
    "**第6次优化**：\n",
    "\n",
    "1. 存在的问题： \n",
    "2. 准备进行的优化：\n",
    "3. 期待的结果：\n",
    "4. 实际结果：\n",
    "5. 原因分析：\n",
    "\n",
    "**第7次优化**：\n",
    "\n",
    "1. 存在的问题： \n",
    "2. 准备进行的优化：\n",
    "3. 期待的结果：\n",
    "4. 实际结果：\n",
    "5. 原因分析：\n",
    "\n",
    "**第9次优化**：\n",
    "\n",
    "1. 存在的问题： \n",
    "2. 准备进行的优化：\n",
    "3. 期待的结果：\n",
    "4. 实际结果：\n",
    "5. 原因分析：\n",
    "\n",
    "**第10次优化**：\n",
    "\n",
    "1. 存在的问题： \n",
    "2. 准备进行的优化：\n",
    "3. 期待的结果：\n",
    "4. 实际结果：\n",
    "5. 原因分析：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mB0SYJr8Qh_c"
   },
   "source": [
    "## 最后一步： 使用Flask、Bottle、Bootstrap变成一个网络应用并且部署在服务器上，这样别人就可以直接通过网址访问你的应用啦。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kTX2EvKDQh_d"
   },
   "source": [
    "最后一步，我们使用Bottle，Bootstrap,Flask等工具进行可视化现实，做出网页能够访问的形式，就像我们的第一个项目一样 😁."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-tKaEzMvQh_d"
   },
   "source": [
    "## 本次项目的总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9JD17iEtQh_e"
   },
   "source": [
    "请写项目的总结报告，描述此次项目的主要过程，其中遇到的问题，以及如何解决这些问题的，以及有什么经验和收获。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IDx56AmUQh_e"
   },
   "source": [
    "恭喜你，你完成了一个**十分**复杂的问题， 能完成这个问题，求是求是，你的能力其实已经达到了国内绝大多数公司的要求，你缺的只是熟练程度。 多多在 Kaggle， 阿里天池里边找一些自己感兴趣的问题，多练习练习。 熟能生巧。"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "OYkDuNtFQh9P",
    "RECSe_S_Qh9T",
    "DkGsX0PkQh9Z",
    "R15pcMaaQh9h",
    "HvCRW9fFQh9l",
    "iaHHFJ4LQh9r",
    "sMyq6CLJQh9v",
    "QC7ZKzPHQh9x",
    "NKhu8YteQh9z",
    "XkFRJI_gQh98",
    "xtlgQaR4Qh9-",
    "Zu5_fdzLQh-A",
    "YLte0lzGQh-C",
    "mHaYniGqQh-E",
    "ohuRE6ydQh-J",
    "1MCwBDf1Qh-L",
    "YCKFY-HEQh-R",
    "fWEHLs-gQh-S",
    "d1sYb_FLQh-X",
    "wtNRq4XJQh-Y",
    "A92dwZHWQh-a",
    "FVMXDbvOQh-q",
    "fnoiDHyhQh-r",
    "dCHkL_5cQh-t",
    "1kZEDrmLQh-x",
    "pip4zNT3Qh-y",
    "qz5Rq4SUQh-0",
    "LlPhS9uiQh-1",
    "KwFIZTXYQh-2",
    "3GPn9cHwQh-_",
    "el7tFKP0Qh_A",
    "MfjYS4eDQh_D",
    "Ye3ahV8QQh_F",
    "s-fl0cFsQh_J",
    "-9QZDYmDQh_J",
    "TZQIKD2VQh_M",
    "lJpeCLLyQh_P",
    "h8k2AhMHQh_R",
    "a__bGiYaQh_S",
    "Moy_CQRNQh_S",
    "cu-yRoOMQh_U",
    "oVFzwBNGQh_W",
    "LVIHuxCOQh_Y",
    "WtfRMqqjQh_a",
    "mB0SYJr8Qh_c",
    "-tKaEzMvQh_d"
   ],
   "name": "Project_2_细粒度情感分类二级指标F1得分92%.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "325px",
    "left": "1080px",
    "top": "407px",
    "width": "224px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
