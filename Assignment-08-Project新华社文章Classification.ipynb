{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Answers-for-part2---判断Classify是否为“新华社”文章\" data-toc-modified-id=\"Answers-for-part2---判断Classify是否为“新华社”文章-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Answers for part2 - 判断Classify是否为“新华社”文章</a></span><ul class=\"toc-item\"><li><span><a href=\"#data-pre-process\" data-toc-modified-id=\"data-pre-process-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>data pre-process</a></span></li><li><span><a href=\"#Data-exploration-analysis-of-y:-content-source\" data-toc-modified-id=\"Data-exploration-analysis-of-y:-content-source-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Data exploration analysis of y: content source</a></span></li><li><span><a href=\"#Word2Vec\" data-toc-modified-id=\"Word2Vec-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Word2Vec</a></span><ul class=\"toc-item\"><li><span><a href=\"#生成word2vec-model\" data-toc-modified-id=\"生成word2vec-model-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>生成word2vec model</a></span></li><li><span><a href=\"#文章转换成向量\" data-toc-modified-id=\"文章转换成向量-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>文章转换成向量</a></span></li><li><span><a href=\"#Try-models\" data-toc-modified-id=\"Try-models-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>Try models</a></span></li></ul></li><li><span><a href=\"#TFIDF\" data-toc-modified-id=\"TFIDF-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>TFIDF</a></span><ul class=\"toc-item\"><li><span><a href=\"#train-model\" data-toc-modified-id=\"train-model-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>train model</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers for part2 - 判断Classify是否为“新华社”文章"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_path = 'mydata/sqlResult_1558435.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "content = pd.read_csv(csv_path, encoding='gb18030')\n",
    "content = content.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "def cut(string):\n",
    "    return(\" \".join(jieba.cut(string)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\CHRIST~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.965 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'这是 一个 测试'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut(\"这是一个测试\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def token(string):\n",
    "    return re.findall(r'[\\w|\\d]+', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['这是1个测试']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token(\"这是1个测试。\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news_content = content['content'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89611"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news_content = [token(n) for n in news_content]\n",
    "news_content = [\" \".join(n) for n in news_content]\n",
    "news_content = [cut(n) for n in news_content]\n",
    "\n",
    "with open(\"news-sentences-cut.txt\", \"w\") as f:\n",
    "    for n in news_content:\n",
    "        f.write(n + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"news-sentences-cut.txt\", \"r\") as f:\n",
    "    news_content = []\n",
    "    for n in f:\n",
    "        news_content.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89611"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'此外   自 本周   6 月 12 日   起   除 小米 手机 6 等 15 款 机型 外   其余 机型 已 暂停 更新 发布   含 开发 版   体验版 内测   稳定版 暂不受 影响   以 确保 工程师 可以 集中 全部 精力 进行 系统优化 工作   有人 猜测 这 也 是 将 精力 主要 用到 MIUI   9 的 研发 之中   MIUI   8 去年 5 月 发布   距今已有 一年 有余   也 是 时候 更新换代 了   当然   关于 MIUI   9 的 确切 信息   我们 还是 等待 官方消息\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_content[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration analysis of y: content source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "975"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content[\"source\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#content[\"source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78661"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[\"source\"].tolist().count(\"新华社\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_count_dic = Counter(content[\"source\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78661"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_count_dic[\"新华社\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we can also define function to count it\n",
    "def dis_map(content_source = content[\"source\"], terms = 10):\n",
    "    source_list = content_source.unique().tolist()\n",
    "    source_count_dic = {source: content_source.tolist().count(source) for source in source_list}\n",
    "    return source_count_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_count_dic = dis_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_source_list = sorted(source_count_dic, key=lambda i: source_count_dic[i], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('新华社', 78661),\n",
       " ('微博', 2477),\n",
       " ('中国证券报?中证网', 538),\n",
       " ('中国新闻网', 525),\n",
       " ('参考消息网', 385),\n",
       " ('环球网', 308),\n",
       " ('南方日报第01版', 278),\n",
       " ('中国台湾网', 233),\n",
       " ('央广网', 197),\n",
       " ('新华网', 172),\n",
       " ('新浪体育', 132),\n",
       " ('人民网', 122),\n",
       " ('环球时报', 119),\n",
       " ('中国经济网', 96),\n",
       " ('参考军事', 95),\n",
       " ('央视网', 94),\n",
       " ('证券时报网', 84),\n",
       " ('cnBeta.COM', 82),\n",
       " ('中国证券网', 80),\n",
       " ('环球时报-环球网', 73),\n",
       " ('广州日报第01版', 64),\n",
       " ('中国网财经', 61),\n",
       " ('每日经济新闻', 58),\n",
       " ('澎湃新闻网', 54),\n",
       " ('海南日报第001版', 54),\n",
       " ('证券日报', 51),\n",
       " ('证券时报', 51),\n",
       " ('21世纪经济报道', 51)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(i, source_count_dic[i]) for i in sorted_source_list if source_count_dic[i] >=50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87780518016761333"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_count_dic[\"新华社\"]/content[\"source\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_target = []\n",
    "for i in content[\"source\"]:\n",
    "    if i == \"新华社\":\n",
    "        content_target.append(1)\n",
    "    else:\n",
    "        content_target.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89611"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content[\"target\"] = content_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christopher_luole\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成word2vec耗时507.99秒\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "startTime = time.time()\n",
    "model = Word2Vec(news_content, size = 200, iter = 10, min_count = 2, workers = 4)\n",
    "\n",
    "usedTime = time.time() - startTime\n",
    "print('生成word2vec耗时%.2f秒' %usedTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "pickleFilePath = 'word2vec_model_1558435.pickle'\n",
    "with open(pickleFilePath, 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christopher_luole\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "# load the model\n",
    "pickleFilePath = 'word2vec_model_1558435.pickle'\n",
    "with open(pickleFilePath, 'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文章转换成向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_contentVector(cutWords, model):\n",
    "    vector_list = [model.wv[k] for k in cutWords if k in model]\n",
    "    contentVector = np.array(vector_list).mean(axis=0)\n",
    "    return contentVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christopher_luole\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Christopher_luole\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: Mean of empty slice.\n",
      "  \"\"\"\n",
      "C:\\Users\\Christopher_luole\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "startTime = time.time()\n",
    "contentVector_list = []\n",
    "for i in range(len(news_content)):\n",
    "    \n",
    "    cutWords = news_content[i]\n",
    "    if (i+1) % 2000 == 0:\n",
    "        usedTime = time.time() - startTime\n",
    "        print('2000文章耗时%.2f秒' %(usedTime))\n",
    "        startTime = time.time()\n",
    "    contentVector_list.append(get_contentVector(cutWords, model))\n",
    "    \n",
    "X = np.array(contentVector_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txtFilePath = 'mydata/X_1558435.txt'\n",
    "X.dump(txtFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the X.txt into array\n",
    "import numpy as np\n",
    "txtFilePath = 'mydata/X_1558435.txt'\n",
    "X = np.load(txtFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89611"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89611,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(i.tolist()) for i in X if isinstance(i.tolist(), list) ]) == 200*len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([[1,2]],[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '1', '2'],\n",
       "      dtype='<U11')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack((\"1\",[1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define function to turn all array into shape (n, 200)\n",
    "import numpy as np\n",
    "def turn_array_shape(input_array):\n",
    "    if max([len(i.tolist()) for i in input_array if isinstance(i.tolist(), list)]) >200:\n",
    "        return(\"Error, at least one array's length over 200\")\n",
    "    \n",
    "    output_array = np.zeros((len(input_array), 200))\n",
    "    for i, value in enumerate(input_array):\n",
    "        output_array[i] = np.hstack((value, [0]*200))[:200]\n",
    "\n",
    "    #output_array = [np.hstack((value, [0]*(200-len(sum([value], []))))) for value in input_array]\n",
    "    return(output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1 = turn_array_shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(i.tolist()) for i in X1 if isinstance(i.tolist(), list) ]) == 200*len(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89611, 200)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89611,)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89611, 1)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.matrix(X)[:,0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([            nan, -12514.08562642,  34778.47629471, -10745.89171196,\n",
       "       -32382.47180979, -18667.1909707 ,  -8952.31246953, -48071.58216711,\n",
       "         9485.81108605, -55678.94524151,  38658.50199698,  17867.36417167,\n",
       "       -40282.59942581,  44782.34188972,  32672.68749676,   7025.43234065,\n",
       "       -12367.52821205,  23275.4370972 ,  31352.42737935, -47610.57710001,\n",
       "         3735.91323437,  21755.8155619 ,   2556.77186848, -21727.89916845,\n",
       "        12466.72628351,   5676.30350566,  17487.24368699,   9509.84083266,\n",
       "        22259.00523343, -40484.0340591 ,  -6021.68037831,  35306.01863047,\n",
       "        -9043.95846522,  62134.07116724,  30485.85828222, -24274.32777995,\n",
       "        58868.00010483, -44858.59758202,  -5861.21258157, -25711.83703891,\n",
       "       -26986.49857255,   7001.7327836 ,  14086.97399657,  27515.42345755,\n",
       "        33435.38418601,  22619.40872307, -29856.44996941,  12591.36943839,\n",
       "        -9550.6541189 ,  41346.77709252,   4140.14712115,   2159.47532549,\n",
       "        17694.363806  , -11542.4814067 ,    683.06820546,  20112.1045946 ,\n",
       "       -30714.47623439,  22442.45913506,  11135.80678689,  -8113.80557886,\n",
       "        84167.0813537 , -31990.9849315 ,  -7198.16467937, -46394.40887714,\n",
       "         -222.2589992 ,  19391.95854899,  -2625.31930749,  -5405.67700442,\n",
       "       -43806.90878583,   7475.70275099, -12849.64351545, -12367.0338419 ,\n",
       "       -34946.10862922,  14195.76979227,  25053.34177483, -19110.51776323,\n",
       "       -24359.82763389, -13123.2673075 , -16704.25475683,  -6632.05550374,\n",
       "        32165.51616676,  15436.86950832,  34719.01093388, -43426.47329125,\n",
       "        27295.99940229,    764.88277057,  40263.71046653, -49192.87559523,\n",
       "        24155.63457886,   -489.15975747,   6437.89368263,  -8358.85092695,\n",
       "        14171.48690499,   4892.73022024, -15789.26169753,  59080.40894641,\n",
       "        -4244.57010208, -66547.63575087,  44986.34484151, -32513.99868562,\n",
       "       -20683.87355909,  11368.53579138,   6871.31977025, -14374.92665543,\n",
       "        46297.80591102,  37203.7306277 ,  55252.83419774,  49225.26444161,\n",
       "        -1980.36884376,  35168.18354904, -14601.62442174,  23243.17099157,\n",
       "        36267.95927246, -18461.36620288, -31681.21128309,  25863.70358965,\n",
       "        28564.27185517, -10809.05395105,   -827.47826247,  -1803.12541553,\n",
       "       -28194.72112853,   7131.35538384, -27435.42844706, -26359.04264027,\n",
       "       -34840.42208533,  15278.54495991,   -915.59584316,   6395.32900818,\n",
       "        45348.54210265,  -7404.85488518,  56680.43816012,  12325.34143634,\n",
       "       -37797.04321213, -56407.38063346,  19338.63566127,  11015.47584137,\n",
       "       -34272.45539221,   6617.78660264,   1792.87490061,  17409.28660414,\n",
       "       -17913.2515935 ,  16476.09439521, -40561.82718949, -55125.51609136,\n",
       "        -5071.72294088,   8290.17469464,  -4007.08229202,  22403.93947447,\n",
       "        10202.80307338,  45351.9213425 ,  28126.80934275,   3068.54388747,\n",
       "          230.91277275, -52545.39601249, -60376.79597343,  27158.90964802,\n",
       "        78667.1931565 , -28543.11452806,  26252.92878501,  35785.17715292,\n",
       "       -51702.01998142,  12273.20533724,  53985.61093801, -73101.05762444,\n",
       "       -50674.98517975, -39066.15830968,  22996.44735642, -24281.66371361,\n",
       "       -31548.59633204, -33729.56035092, -20959.56636697,  25893.62702334,\n",
       "        23075.88168067,  12776.24177945,  17740.18592759,  24042.30580812,\n",
       "         1104.82371819,   6656.82355551, -33078.91467565,    654.73981859,\n",
       "       -42555.35797904, -11281.34667514,  -2554.27886734, -17670.74388304,\n",
       "        38161.44911317, -28969.85861954,   2606.42118703,  22445.94890701,\n",
       "       -15842.79845937,  11458.76812718,  22055.36199672, -22390.90499413,\n",
       "        38286.5204793 , -90463.56089325,   6577.24471955,  31923.69226478,\n",
       "       -24139.94207548,  15777.72371358,  18264.71306206,  33660.2743012 ])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fillna for X\n",
    "# first turn X into PandasDataframe format\n",
    "col_name = [\"X\"+str(i) for i in range(200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_X = pd.DataFrame(columns = col_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, value in enumerate(col_name):\n",
    "    df_X[value] = X[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_X.to_csv(\"mydata/df_X.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"mydata/df_X.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12514.085626424918"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(X[\"X1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = content[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89611,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christopher_luole\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.reshape((1, len(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89611"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78661"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_test_X, valid_X, train_test_y, valid_y = train_test_split(X, y, test_size=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(train_test_X, train_test_y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68104, 200)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 LogisticRegression\n",
      "1 DecisionTreeClassifier\n",
      "2 RandomForestClassifier\n",
      "3 GradientBoostingClassifier\n",
      "4 XGBClassifier\n",
      "5 GaussianNB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Train Accuracy Score</th>\n",
       "      <th>Test Accuracy Score</th>\n",
       "      <th>Valid Accuracy Score</th>\n",
       "      <th>Comsumed Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{'C': 1.0, 'class_weight': None, 'dual': False...</td>\n",
       "      <td>0.971176</td>\n",
       "      <td>0.970751</td>\n",
       "      <td>0.972104</td>\n",
       "      <td>9.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>{'base_score': 0.5, 'booster': 'gbtree', 'cols...</td>\n",
       "      <td>0.956464</td>\n",
       "      <td>0.950781</td>\n",
       "      <td>0.94711</td>\n",
       "      <td>185.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>{'criterion': 'friedman_mse', 'init': None, 'l...</td>\n",
       "      <td>0.956141</td>\n",
       "      <td>0.9499</td>\n",
       "      <td>0.951127</td>\n",
       "      <td>162.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>{'bootstrap': True, 'class_weight': None, 'cri...</td>\n",
       "      <td>0.998869</td>\n",
       "      <td>0.948197</td>\n",
       "      <td>0.94778</td>\n",
       "      <td>14.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>{'class_weight': None, 'criterion': 'gini', 'm...</td>\n",
       "      <td>0.999956</td>\n",
       "      <td>0.927405</td>\n",
       "      <td>0.921669</td>\n",
       "      <td>43.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>{'priors': None}</td>\n",
       "      <td>0.72517</td>\n",
       "      <td>0.730765</td>\n",
       "      <td>0.728409</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Name  \\\n",
       "0          LogisticRegression   \n",
       "4               XGBClassifier   \n",
       "3  GradientBoostingClassifier   \n",
       "2      RandomForestClassifier   \n",
       "1      DecisionTreeClassifier   \n",
       "5                  GaussianNB   \n",
       "\n",
       "                                          Parameters Train Accuracy Score  \\\n",
       "0  {'C': 1.0, 'class_weight': None, 'dual': False...             0.971176   \n",
       "4  {'base_score': 0.5, 'booster': 'gbtree', 'cols...             0.956464   \n",
       "3  {'criterion': 'friedman_mse', 'init': None, 'l...             0.956141   \n",
       "2  {'bootstrap': True, 'class_weight': None, 'cri...             0.998869   \n",
       "1  {'class_weight': None, 'criterion': 'gini', 'm...             0.999956   \n",
       "5                                   {'priors': None}              0.72517   \n",
       "\n",
       "  Test Accuracy Score Valid Accuracy Score Comsumed Time  \n",
       "0            0.970751             0.972104          9.01  \n",
       "4            0.950781              0.94711        185.37  \n",
       "3              0.9499             0.951127        162.42  \n",
       "2            0.948197              0.94778         14.38  \n",
       "1            0.927405             0.921669         43.62  \n",
       "5            0.730765             0.728409          0.33  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB##GNB-- Gaussian Naive Bayes\n",
    "#import lightgbm as lgb\n",
    "\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "# from sklearn.ensemble import BaggingClassifier\n",
    "# from sklearn.ensemble import ExtraTreesClassifier\n",
    "# from sklearn.svm import LinearSVR\n",
    "# from sklearn.svm import NuSVR\n",
    "# from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "estimator_list = [\n",
    "    LogisticRegression(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    XGBClassifier(),\n",
    "    #KNeighborsClassifier(),\n",
    "    GaussianNB(),\n",
    "]\n",
    "\n",
    "#cv_split = ShuffleSplit(n_splits=6, train_size=0.7, test_size=0.2, random_state=2019)\n",
    "df_columns = ['Name', 'Parameters', 'Train Accuracy Score', 'Test Accuracy Score', 'Valid Accuracy Score', 'Comsumed Time']\n",
    "df = pd.DataFrame(columns=df_columns)\n",
    "\n",
    "row_index = 0\n",
    "for estimator in estimator_list:\n",
    "    T1 = time.time()\n",
    "    df.loc[row_index, 'Name'] = estimator.__class__.__name__\n",
    "    df.loc[row_index, 'Parameters'] = str(estimator.get_params())\n",
    "    estimator.fit(train_X, train_y)\n",
    "    T2 = time.time()\n",
    "    #cv_results = cross_validate(estimator, train_test_X, train_test_y, cv=cv_split)\n",
    "    df.loc[row_index, 'Train Accuracy Score'] = estimator.score(train_X, train_y)\n",
    "    #df.loc[row_index, 'Test Accuracy Mean'] = cv_results['test_score'].mean()\n",
    "    df.loc[row_index, 'Test Accuracy Score'] = estimator.score(test_X, test_y)\n",
    "    df.loc[row_index, 'Valid Accuracy Score'] = estimator.score(valid_X, valid_y)\n",
    "    df.loc[row_index, 'Comsumed Time'] = round(T2-T1, 2)\n",
    "    print(row_index, estimator.__class__.__name__)\n",
    "    #print(cv_results['test_score'])\n",
    "    row_index += 1\n",
    "df = df.sort_values(by='Test Accuracy Score', ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf(word, doc):\n",
    "    words = doc.split()\n",
    "    return sum(1 for w in words if word == w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def idf(word, all_doc):\n",
    "    df = sum(1 for doc in all_doc if word in doc)\n",
    "    return math.log10(len(all_doc) / df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf(\"的\", news_content[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf(\"的\", news_content) < idf(\"小米\", news_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1051466115514474"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf(\"的\", news_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We have successfully transformed the target variable and now let's turn our focus to extract features from the cleaned version of the movie plots. I have decided to go ahead with TF-IDF features. You are free to use any other feature extraction method such as Bag-of-Words, word2vec, GloVe, or ELMo. \n",
    "\n",
    "\n",
    "I recommend you check out these articles to learn more about different ways of creating features from text:\n",
    "\n",
    "\n",
    "\n",
    "*   [An Intuitive Understanding of Word Embeddings: From Count Vectors to Word2Vec](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/)\n",
    "*   [A Step-by-Step NLP Guide to Learn ELMo for Extracting Features from Text](https://www.analyticsvidhya.com/blog/2019/03/learn-to-use-elmo-to-extract-features-from-text/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.88, max_features=100)\n",
    "#max document frequency can be tuned by various senarios, here we use 0.8 because 新华社 and not 新华社 have a ratio about 0.8:0.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that I have used the 100 most frequent words in the data as my features. You can try any other number as well for the parameter max_features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_test_X, valid_X, train_test_y, valid_y = train_test_split(news_content, y, test_size=0.05)\n",
    "train_X, test_X, train_y, test_y = train_test_split(train_test_X, train_test_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "耗时22.45秒\n"
     ]
    }
   ],
   "source": [
    "# create TF-IDF features\n",
    "import time\n",
    "T1 = time.time()\n",
    "xtrain_tfidf = tfidf_vectorizer.fit_transform(train_X)\n",
    "xtest_tfidf = tfidf_vectorizer.transform(test_X)\n",
    "xvalid_tfidf = tfidf_vectorizer.fit_transform(valid_X)\n",
    "T2 = time.time()\n",
    "print(\"耗时{}秒\".format(round(T2-T1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68104, 100)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 LogisticRegression\n",
      "1 DecisionTreeClassifier\n",
      "2 RandomForestClassifier\n",
      "3 GradientBoostingClassifier\n",
      "4 XGBClassifier\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Train Accuracy Score</th>\n",
       "      <th>Test Accuracy Score</th>\n",
       "      <th>Valid Accuracy Score</th>\n",
       "      <th>Comsumed Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>{'bootstrap': True, 'class_weight': None, 'cri...</td>\n",
       "      <td>0.995448</td>\n",
       "      <td>0.984729</td>\n",
       "      <td>0.768578</td>\n",
       "      <td>5.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>{'base_score': 0.5, 'booster': 'gbtree', 'cols...</td>\n",
       "      <td>0.981191</td>\n",
       "      <td>0.982204</td>\n",
       "      <td>0.774158</td>\n",
       "      <td>31.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>{'criterion': 'friedman_mse', 'init': None, 'l...</td>\n",
       "      <td>0.981308</td>\n",
       "      <td>0.980912</td>\n",
       "      <td>0.719036</td>\n",
       "      <td>64.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>{'class_weight': None, 'criterion': 'gini', 'm...</td>\n",
       "      <td>0.9963</td>\n",
       "      <td>0.977564</td>\n",
       "      <td>0.700736</td>\n",
       "      <td>5.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{'C': 1.0, 'class_weight': None, 'dual': False...</td>\n",
       "      <td>0.966948</td>\n",
       "      <td>0.967931</td>\n",
       "      <td>0.784869</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Name  \\\n",
       "2      RandomForestClassifier   \n",
       "4               XGBClassifier   \n",
       "3  GradientBoostingClassifier   \n",
       "1      DecisionTreeClassifier   \n",
       "0          LogisticRegression   \n",
       "\n",
       "                                          Parameters Train Accuracy Score  \\\n",
       "2  {'bootstrap': True, 'class_weight': None, 'cri...             0.995448   \n",
       "4  {'base_score': 0.5, 'booster': 'gbtree', 'cols...             0.981191   \n",
       "3  {'criterion': 'friedman_mse', 'init': None, 'l...             0.981308   \n",
       "1  {'class_weight': None, 'criterion': 'gini', 'm...               0.9963   \n",
       "0  {'C': 1.0, 'class_weight': None, 'dual': False...             0.966948   \n",
       "\n",
       "  Test Accuracy Score Valid Accuracy Score Comsumed Time  \n",
       "2            0.984729             0.768578          5.47  \n",
       "4            0.982204             0.774158         31.07  \n",
       "3            0.980912             0.719036         64.56  \n",
       "1            0.977564             0.700736          5.19  \n",
       "0            0.967931             0.784869          0.61  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB##GNB-- Gaussian Naive Bayes\n",
    "#import lightgbm as lgb\n",
    "\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "# from sklearn.ensemble import BaggingClassifier\n",
    "# from sklearn.ensemble import ExtraTreesClassifier\n",
    "# from sklearn.svm import LinearSVR\n",
    "# from sklearn.svm import NuSVR\n",
    "# from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "estimator_list = [\n",
    "    LogisticRegression(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    XGBClassifier(),\n",
    "    #KNeighborsClassifier(),\n",
    "    #GaussianNB(),\n",
    "]\n",
    "\n",
    "#cv_split = ShuffleSplit(n_splits=6, train_size=0.7, test_size=0.2, random_state=2019)\n",
    "df_columns = ['Name', 'Parameters', 'Train Accuracy Score', 'Test Accuracy Score', 'Valid Accuracy Score', 'Comsumed Time']\n",
    "df = pd.DataFrame(columns=df_columns)\n",
    "\n",
    "row_index = 0\n",
    "for estimator in estimator_list:\n",
    "    T1 = time.time()\n",
    "    df.loc[row_index, 'Name'] = estimator.__class__.__name__\n",
    "    df.loc[row_index, 'Parameters'] = str(estimator.get_params())\n",
    "    estimator.fit(xtrain_tfidf, train_y)\n",
    "    T2 = time.time()\n",
    "    #cv_results = cross_validate(estimator, train_test_X, train_test_y, cv=cv_split)\n",
    "    df.loc[row_index, 'Train Accuracy Score'] = estimator.score(xtrain_tfidf, train_y)\n",
    "    #df.loc[row_index, 'Test Accuracy Mean'] = cv_results['test_score'].mean()\n",
    "    df.loc[row_index, 'Test Accuracy Score'] = estimator.score(xtest_tfidf, test_y)\n",
    "    df.loc[row_index, 'Valid Accuracy Score'] = estimator.score(xvalid_tfidf, valid_y)\n",
    "    df.loc[row_index, 'Comsumed Time'] = round(T2-T1, 2)\n",
    "    print(row_index, estimator.__class__.__name__)\n",
    "    #print(cv_results['test_score'])\n",
    "    row_index += 1\n",
    "df = df.sort_values(by='Test Accuracy Score', ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
