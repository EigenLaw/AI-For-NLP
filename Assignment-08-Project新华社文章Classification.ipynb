{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#问题实战\" data-toc-modified-id=\"问题实战-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>问题实战</a></span><ul class=\"toc-item\"><li><span><a href=\"#data-pre-process\" data-toc-modified-id=\"data-pre-process-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>data pre-process</a></span></li><li><span><a href=\"#Data-exploration-analysis-of-y:-content-source\" data-toc-modified-id=\"Data-exploration-analysis-of-y:-content-source-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Data exploration analysis of y: content source</a></span></li><li><span><a href=\"#Word2Vec\" data-toc-modified-id=\"Word2Vec-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Word2Vec</a></span><ul class=\"toc-item\"><li><span><a href=\"#生成word2vec-model\" data-toc-modified-id=\"生成word2vec-model-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>生成word2vec model</a></span></li><li><span><a href=\"#文章转换成向量\" data-toc-modified-id=\"文章转换成向量-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>文章转换成向量</a></span></li><li><span><a href=\"#Try-models\" data-toc-modified-id=\"Try-models-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>Try models</a></span></li></ul></li><li><span><a href=\"#TFIDF\" data-toc-modified-id=\"TFIDF-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>TFIDF</a></span><ul class=\"toc-item\"><li><span><a href=\"#train-model\" data-toc-modified-id=\"train-model-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>train model</a></span></li><li><span><a href=\"#抄袭文章查找\" data-toc-modified-id=\"抄袭文章查找-1.4.2\"><span class=\"toc-item-num\">1.4.2&nbsp;&nbsp;</span>抄袭文章查找</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 问题实战\n",
    "\n",
    "问题描述： 在新闻出版业中一个常常的问题就是新闻版权抄袭，所以我们现在为了避免这个事情，需要建立一个模型，判断这个文章是不是由某个新闻出版单位出版的。 在我们这个问题里，我们需要建立一个模型，该模型接受一个作为文本的输入，然后判断该文本是不是由“新华社”发布的。 \n",
    "\n",
    "Enviroment: \n",
    "\n",
    "+ Python 3.6\n",
    "+ numpy \n",
    "+ scikit learning\n",
    "\n",
    "**请在 pycharm 中运行程序，该处只作为关键信息的记录。 **\n",
    "\n",
    "1. 问什么此问题应该用机器学习方法？ \n",
    "\n",
    "Ans：\n",
    "\n",
    "2. 问什么要对文本进行向量化？ 如何进行文本向量化表示？ （请使用tfidf 或者词向量）\n",
    "\n",
    "Ans:\n",
    "\n",
    "hint: 如果你使用 tfidf，则需要 scikit learning 如果你需要词向量，则需要 gensim\n",
    "\n",
    "3. 请对数据进行Preprocessing, Normalization 操作\n",
    "（你需要在 Preprocssing 的时候，把文章开头的“新华社”3 个字去掉。如果不去掉，会出现什么问题？）\n",
    "\n",
    "4. 请确定模型的 Baseline 以及确定评测指标（Evaluation）.\n",
    "\n",
    "5. 尝试不同的模型、不同的参数，观察结果变化。 \n",
    "\n",
    "6. 依据模型的表现，进行参数调节。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_path = 'mydata/sqlResult_1558435.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "content = pd.read_csv(csv_path, encoding='gb18030')\n",
    "content = content.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "def cut(string):\n",
    "    return(\" \".join(jieba.cut(string)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\CHRIST~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.075 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'这是 一个 测试'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut(\"这是一个测试\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def token(string):\n",
    "    return re.findall(r'[\\w|\\d]+', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['这是1个测试']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token(\"这是1个测试。\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news_content = content['content'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89611"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news_content = [token(n) for n in news_content]\n",
    "news_content = [\" \".join(n) for n in news_content]\n",
    "news_content = [cut(n) for n in news_content]\n",
    "\n",
    "with open(\"news-sentences-cut.txt\", \"w\") as f:\n",
    "    for n in news_content:\n",
    "        f.write(n + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"news-sentences-cut.txt\", \"r\") as f:\n",
    "    news_content = []\n",
    "    for n in f:\n",
    "        news_content.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89611"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' abc 123'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"新华社 abc 123\".replace(\"新华社\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news_content = [i.replace(\"新华社\",\"\").replace(\"新华网\",\"\") for i in news_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'此外   自 本周   6 月 12 日   起   除 小米 手机 6 等 15 款 机型 外   其余 机型 已 暂停 更新 发布   含 开发 版   体验版 内测   稳定版 暂不受 影响   以 确保 工程师 可以 集中 全部 精力 进行 系统优化 工作   有人 猜测 这 也 是 将 精力 主要 用到 MIUI   9 的 研发 之中   MIUI   8 去年 5 月 发布   距今已有 一年 有余   也 是 时候 更新换代 了   当然   关于 MIUI   9 的 确切 信息   我们 还是 等待 官方消息\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_content[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration analysis of y: content source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "975"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content[\"source\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#content[\"source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78661"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[\"source\"].tolist().count(\"新华社\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_count_dic = Counter(content[\"source\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78833"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_count_dic[\"新华社\"]+source_count_dic[\"新华网\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we can also define function to count it\n",
    "def dis_map(content_source = content[\"source\"], terms = 10):\n",
    "    source_list = content_source.unique().tolist()\n",
    "    source_count_dic = {source: content_source.tolist().count(source) for source in source_list}\n",
    "    return source_count_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_count_dic = dis_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_source_list = sorted(source_count_dic, key=lambda i: source_count_dic[i], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('新华社', 78661),\n",
       " ('微博', 2477),\n",
       " ('中国证券报?中证网', 538),\n",
       " ('中国新闻网', 525),\n",
       " ('参考消息网', 385),\n",
       " ('环球网', 308),\n",
       " ('南方日报第01版', 278),\n",
       " ('中国台湾网', 233),\n",
       " ('央广网', 197),\n",
       " ('新华网', 172),\n",
       " ('新浪体育', 132),\n",
       " ('人民网', 122),\n",
       " ('环球时报', 119),\n",
       " ('中国经济网', 96),\n",
       " ('参考军事', 95),\n",
       " ('央视网', 94),\n",
       " ('证券时报网', 84),\n",
       " ('cnBeta.COM', 82),\n",
       " ('中国证券网', 80),\n",
       " ('环球时报-环球网', 73),\n",
       " ('广州日报第01版', 64),\n",
       " ('中国网财经', 61),\n",
       " ('每日经济新闻', 58),\n",
       " ('澎湃新闻网', 54),\n",
       " ('海南日报第001版', 54),\n",
       " ('证券日报', 51),\n",
       " ('证券时报', 51),\n",
       " ('21世纪经济报道', 51)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(i, source_count_dic[i]) for i in sorted_source_list if source_count_dic[i] >=50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87780518016761333"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_count_dic[\"新华社\"]/content[\"source\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_target = []\n",
    "for i in content[\"source\"]:\n",
    "    if i == \"新华社\" or i ==\"新华网\":\n",
    "        content_target.append(1)\n",
    "    else:\n",
    "        content_target.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89611"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content[\"target\"] = content_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christopher_luole\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成word2vec耗时507.99秒\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "startTime = time.time()\n",
    "model = Word2Vec(news_content, size = 200, iter = 10, min_count = 2, workers = 4)\n",
    "\n",
    "usedTime = time.time() - startTime\n",
    "print('生成word2vec耗时%.2f秒' %usedTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "pickleFilePath = 'word2vec_model_1558435.pickle'\n",
    "with open(pickleFilePath, 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "# load the model\n",
    "pickleFilePath = 'word2vec_model_1558435.pickle'\n",
    "with open(pickleFilePath, 'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文章转换成向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_contentVector(cutWords, model):\n",
    "    vector_list = [model.wv[k] for k in cutWords if k in model]\n",
    "    contentVector = np.array(vector_list).mean(axis=0)\n",
    "    return contentVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christopher_luole\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Christopher_luole\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: Mean of empty slice.\n",
      "  \"\"\"\n",
      "C:\\Users\\Christopher_luole\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n",
      "2000文章耗时0.00秒\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "startTime = time.time()\n",
    "contentVector_list = []\n",
    "for i in range(len(news_content)):\n",
    "    \n",
    "    cutWords = news_content[i]\n",
    "    if (i+1) % 2000 == 0:\n",
    "        usedTime = time.time() - startTime\n",
    "        print('2000文章耗时%.2f秒' %(usedTime))\n",
    "        startTime = time.time()\n",
    "    contentVector_list.append(get_contentVector(cutWords, model))\n",
    "    \n",
    "X = np.array(contentVector_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txtFilePath = 'mydata/X_1558435.txt'\n",
    "X.dump(txtFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the X.txt into array\n",
    "import numpy as np\n",
    "txtFilePath = 'mydata/X_1558435.txt'\n",
    "X = np.load(txtFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89611"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89611,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(i.tolist()) for i in X if isinstance(i.tolist(), list) ]) == 200*len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([[1,2]],[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '1', '2'],\n",
       "      dtype='<U11')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack((\"1\",[1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define function to turn all array into shape (n, 200)\n",
    "import numpy as np\n",
    "def turn_array_shape(input_array):\n",
    "    if max([len(i.tolist()) for i in input_array if isinstance(i.tolist(), list)]) >200:\n",
    "        return(\"Error, at least one array's length over 200\")\n",
    "    \n",
    "    output_array = np.zeros((len(input_array), 200))\n",
    "    for i, value in enumerate(input_array):\n",
    "        output_array[i] = np.hstack((value, [0]*200))[:200]\n",
    "\n",
    "    #output_array = [np.hstack((value, [0]*(200-len(sum([value], []))))) for value in input_array]\n",
    "    return(output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1 = turn_array_shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(i.tolist()) for i in X1 if isinstance(i.tolist(), list) ]) == 200*len(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89611, 200)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89611,)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89611, 1)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.matrix(X)[:,0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fillna for X\n",
    "# first turn X into PandasDataframe format\n",
    "col_name = [\"X\"+str(i) for i in range(200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_X = pd.DataFrame(columns = col_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, value in enumerate(col_name):\n",
    "    df_X[value] = X[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_X.to_csv(\"mydata/df_X.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"mydata/df_X.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12514.085626424918"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(X[\"X1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = content[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89611,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christopher_luole\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.reshape((1, len(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89611"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78833"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_test_X, valid_X, train_test_y, valid_y = train_test_split(X, y, test_size=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(train_test_X, train_test_y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68104, 200)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 LogisticRegression\n",
      "1 DecisionTreeClassifier\n",
      "2 RandomForestClassifier\n",
      "3 GradientBoostingClassifier\n",
      "4 XGBClassifier\n",
      "5 GaussianNB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Train Accuracy Score</th>\n",
       "      <th>Test Accuracy Score</th>\n",
       "      <th>Valid Accuracy Score</th>\n",
       "      <th>Comsumed Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{'C': 1.0, 'class_weight': None, 'dual': False...</td>\n",
       "      <td>0.971176</td>\n",
       "      <td>0.970751</td>\n",
       "      <td>0.972104</td>\n",
       "      <td>9.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>{'base_score': 0.5, 'booster': 'gbtree', 'cols...</td>\n",
       "      <td>0.956464</td>\n",
       "      <td>0.950781</td>\n",
       "      <td>0.94711</td>\n",
       "      <td>185.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>{'criterion': 'friedman_mse', 'init': None, 'l...</td>\n",
       "      <td>0.956141</td>\n",
       "      <td>0.9499</td>\n",
       "      <td>0.951127</td>\n",
       "      <td>162.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>{'bootstrap': True, 'class_weight': None, 'cri...</td>\n",
       "      <td>0.998869</td>\n",
       "      <td>0.948197</td>\n",
       "      <td>0.94778</td>\n",
       "      <td>14.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>{'class_weight': None, 'criterion': 'gini', 'm...</td>\n",
       "      <td>0.999956</td>\n",
       "      <td>0.927405</td>\n",
       "      <td>0.921669</td>\n",
       "      <td>43.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>{'priors': None}</td>\n",
       "      <td>0.72517</td>\n",
       "      <td>0.730765</td>\n",
       "      <td>0.728409</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Name  \\\n",
       "0          LogisticRegression   \n",
       "4               XGBClassifier   \n",
       "3  GradientBoostingClassifier   \n",
       "2      RandomForestClassifier   \n",
       "1      DecisionTreeClassifier   \n",
       "5                  GaussianNB   \n",
       "\n",
       "                                          Parameters Train Accuracy Score  \\\n",
       "0  {'C': 1.0, 'class_weight': None, 'dual': False...             0.971176   \n",
       "4  {'base_score': 0.5, 'booster': 'gbtree', 'cols...             0.956464   \n",
       "3  {'criterion': 'friedman_mse', 'init': None, 'l...             0.956141   \n",
       "2  {'bootstrap': True, 'class_weight': None, 'cri...             0.998869   \n",
       "1  {'class_weight': None, 'criterion': 'gini', 'm...             0.999956   \n",
       "5                                   {'priors': None}              0.72517   \n",
       "\n",
       "  Test Accuracy Score Valid Accuracy Score Comsumed Time  \n",
       "0            0.970751             0.972104          9.01  \n",
       "4            0.950781              0.94711        185.37  \n",
       "3              0.9499             0.951127        162.42  \n",
       "2            0.948197              0.94778         14.38  \n",
       "1            0.927405             0.921669         43.62  \n",
       "5            0.730765             0.728409          0.33  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB##GNB-- Gaussian Naive Bayes\n",
    "#import lightgbm as lgb\n",
    "\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "# from sklearn.ensemble import BaggingClassifier\n",
    "# from sklearn.ensemble import ExtraTreesClassifier\n",
    "# from sklearn.svm import LinearSVR\n",
    "# from sklearn.svm import NuSVR\n",
    "# from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "estimator_list = [\n",
    "    LogisticRegression(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    XGBClassifier(),\n",
    "    #KNeighborsClassifier(),\n",
    "    GaussianNB(),\n",
    "]\n",
    "\n",
    "#cv_split = ShuffleSplit(n_splits=6, train_size=0.7, test_size=0.2, random_state=2019)\n",
    "df_columns = ['Name', 'Parameters', 'Train Accuracy Score', 'Test Accuracy Score', 'Valid Accuracy Score', 'Comsumed Time']\n",
    "df = pd.DataFrame(columns=df_columns)\n",
    "\n",
    "row_index = 0\n",
    "for estimator in estimator_list:\n",
    "    T1 = time.time()\n",
    "    df.loc[row_index, 'Name'] = estimator.__class__.__name__\n",
    "    df.loc[row_index, 'Parameters'] = str(estimator.get_params())\n",
    "    estimator.fit(train_X, train_y)\n",
    "    T2 = time.time()\n",
    "    #cv_results = cross_validate(estimator, train_test_X, train_test_y, cv=cv_split)\n",
    "    df.loc[row_index, 'Train Accuracy Score'] = estimator.score(train_X, train_y)\n",
    "    #df.loc[row_index, 'Test Accuracy Mean'] = cv_results['test_score'].mean()\n",
    "    df.loc[row_index, 'Test Accuracy Score'] = estimator.score(test_X, test_y)\n",
    "    df.loc[row_index, 'Valid Accuracy Score'] = estimator.score(valid_X, valid_y)\n",
    "    df.loc[row_index, 'Comsumed Time'] = round(T2-T1, 2)\n",
    "    print(row_index, estimator.__class__.__name__)\n",
    "    #print(cv_results['test_score'])\n",
    "    row_index += 1\n",
    "df = df.sort_values(by='Test Accuracy Score', ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf(word, doc):\n",
    "    words = doc.split()\n",
    "    return sum(1 for w in words if word == w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def idf(word, all_doc):\n",
    "    df = sum(1 for doc in all_doc if word in doc)\n",
    "    return math.log10(len(all_doc) / df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf(\"的\", news_content[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf(\"的\", news_content) < idf(\"小米\", news_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1051466115514474"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf(\"的\", news_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We have successfully transformed the target variable and now let's turn our focus to extract features from the cleaned version of the movie plots. I have decided to go ahead with TF-IDF features. You are free to use any other feature extraction method such as Bag-of-Words, word2vec, GloVe, or ELMo. \n",
    "\n",
    "\n",
    "I recommend you check out these articles to learn more about different ways of creating features from text:\n",
    "\n",
    "\n",
    "\n",
    "*   [An Intuitive Understanding of Word Embeddings: From Count Vectors to Word2Vec](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/)\n",
    "*   [A Step-by-Step NLP Guide to Learn ELMo for Extracting Features from Text](https://www.analyticsvidhya.com/blog/2019/03/learn-to-use-elmo-to-extract-features-from-text/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.88, max_features=100)\n",
    "#max document frequency can be tuned by various senarios, here we use 0.8 because 新华社 and not 新华社 have a ratio about 0.8:0.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that I have used the 100 most frequent words in the data as my features. You can try any other number as well for the parameter max_features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_test_X, valid_X, train_test_y, valid_y = train_test_split(news_content, y, test_size=0.05)\n",
    "train_X, test_X, train_y, test_y = train_test_split(train_test_X, train_test_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_indices = test_y.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "耗时14.62秒\n"
     ]
    }
   ],
   "source": [
    "# create TF-IDF features\n",
    "import time\n",
    "T1 = time.time()\n",
    "xtrain_tfidf = tfidf_vectorizer.fit_transform(train_X)\n",
    "xtest_tfidf = tfidf_vectorizer.transform(test_X)\n",
    "xvalid_tfidf = tfidf_vectorizer.fit_transform(valid_X)\n",
    "T2 = time.time()\n",
    "print(\"耗时{}秒\".format(round(T2-T1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68104, 100)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def get_performance(clf, X_, y_):\n",
    "    y_hat = clf.predict(X_.toarray())\n",
    "    print('percision is: {}'.format(precision_score(y_, y_hat)))\n",
    "    print('recall is: {}'.format(recall_score(y_, y_hat)))\n",
    "    print('roc_auc is: {}'.format(roc_auc_score(y_, y_hat)))\n",
    "    print('confusion matrix: \\n{}'.format(confusion_matrix(y_, y_hat, labels=[0, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percision is: 0.9601624950858341\n",
      "recall is: 0.9756974498967974\n",
      "roc_auc is: 0.8363788694426687\n",
      "confusion matrix: \n",
      "[[ 1399   608]\n",
      " [  365 14654]]\n",
      "0 LogisticRegression\n",
      "percision is: 0.973835268989454\n",
      "recall is: 0.9714361808376057\n",
      "roc_auc is: 0.888059894105898\n",
      "confusion matrix: \n",
      "[[ 1615   392]\n",
      " [  429 14590]]\n",
      "1 DecisionTreeClassifier\n",
      "percision is: 0.976286679472743\n",
      "recall is: 0.9813569478660363\n",
      "roc_auc is: 0.9014906313819468\n",
      "confusion matrix: \n",
      "[[ 1649   358]\n",
      " [  280 14739]]\n",
      "2 RandomForestClassifier\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Train Accuracy Score</th>\n",
       "      <th>Test Accuracy Score</th>\n",
       "      <th>Valid Accuracy Score</th>\n",
       "      <th>Comsumed Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>{'bootstrap': True, 'class_weight': None, 'cri...</td>\n",
       "      <td>0.993686</td>\n",
       "      <td>0.962528</td>\n",
       "      <td>0.805847</td>\n",
       "      <td>5.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>{'class_weight': None, 'criterion': 'gini', 'm...</td>\n",
       "      <td>0.995008</td>\n",
       "      <td>0.95178</td>\n",
       "      <td>0.720152</td>\n",
       "      <td>4.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{'C': 1.0, 'class_weight': None, 'dual': False...</td>\n",
       "      <td>0.942147</td>\n",
       "      <td>0.942852</td>\n",
       "      <td>0.829279</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Name                                         Parameters  \\\n",
       "2  RandomForestClassifier  {'bootstrap': True, 'class_weight': None, 'cri...   \n",
       "1  DecisionTreeClassifier  {'class_weight': None, 'criterion': 'gini', 'm...   \n",
       "0      LogisticRegression  {'C': 1.0, 'class_weight': None, 'dual': False...   \n",
       "\n",
       "  Train Accuracy Score Test Accuracy Score Valid Accuracy Score Comsumed Time  \n",
       "2             0.993686            0.962528             0.805847          5.24  \n",
       "1             0.995008             0.95178             0.720152          4.42  \n",
       "0             0.942147            0.942852             0.829279          0.51  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB##GNB-- Gaussian Naive Bayes\n",
    "#import lightgbm as lgb\n",
    "\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "# from sklearn.ensemble import BaggingClassifier\n",
    "# from sklearn.ensemble import ExtraTreesClassifier\n",
    "# from sklearn.svm import LinearSVR\n",
    "# from sklearn.svm import NuSVR\n",
    "# from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "estimator_list = [\n",
    "    LogisticRegression(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    #GradientBoostingClassifier(),\n",
    "    #XGBClassifier(),\n",
    "    #KNeighborsClassifier(),\n",
    "    #GaussianNB(),\n",
    "]\n",
    "\n",
    "#cv_split = ShuffleSplit(n_splits=6, train_size=0.7, test_size=0.2, random_state=2019)\n",
    "df_columns = ['Name', 'Parameters', 'Train Accuracy Score', 'Test Accuracy Score', 'Valid Accuracy Score', 'Comsumed Time']\n",
    "df = pd.DataFrame(columns=df_columns)\n",
    "\n",
    "row_index = 0\n",
    "for estimator in estimator_list:\n",
    "    T1 = time.time()\n",
    "    df.loc[row_index, 'Name'] = estimator.__class__.__name__\n",
    "    df.loc[row_index, 'Parameters'] = str(estimator.get_params())\n",
    "    estimator.fit(xtrain_tfidf, train_y)\n",
    "    T2 = time.time()\n",
    "    get_performance(estimator, xtest_tfidf, test_y)\n",
    "    #cv_results = cross_validate(estimator, train_test_X, train_test_y, cv=cv_split)\n",
    "    df.loc[row_index, 'Train Accuracy Score'] = estimator.score(xtrain_tfidf, train_y)\n",
    "    #df.loc[row_index, 'Test Accuracy Mean'] = cv_results['test_score'].mean()\n",
    "    df.loc[row_index, 'Test Accuracy Score'] = estimator.score(xtest_tfidf, test_y)\n",
    "    df.loc[row_index, 'Valid Accuracy Score'] = estimator.score(xvalid_tfidf, valid_y)\n",
    "    df.loc[row_index, 'Comsumed Time'] = round(T2-T1, 2)\n",
    "    print(row_index, estimator.__class__.__name__)\n",
    "    #print(cv_results['test_score'])\n",
    "    row_index += 1\n",
    "df = df.sort_values(by='Test Accuracy Score', ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 LogisticRegression\n",
      "1 DecisionTreeClassifier\n",
      "2 RandomForestClassifier\n",
      "3 GradientBoostingClassifier\n",
      "4 XGBClassifier\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Train Accuracy Score</th>\n",
       "      <th>Test Accuracy Score</th>\n",
       "      <th>Valid Accuracy Score</th>\n",
       "      <th>Comsumed Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>{'bootstrap': True, 'class_weight': None, 'cri...</td>\n",
       "      <td>0.993671</td>\n",
       "      <td>0.961353</td>\n",
       "      <td>0.751172</td>\n",
       "      <td>5.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>{'base_score': 0.5, 'booster': 'gbtree', 'cols...</td>\n",
       "      <td>0.954966</td>\n",
       "      <td>0.953131</td>\n",
       "      <td>0.778398</td>\n",
       "      <td>14.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>{'criterion': 'friedman_mse', 'init': None, 'l...</td>\n",
       "      <td>0.953365</td>\n",
       "      <td>0.952778</td>\n",
       "      <td>0.546976</td>\n",
       "      <td>29.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>{'class_weight': None, 'criterion': 'gini', 'm...</td>\n",
       "      <td>0.995008</td>\n",
       "      <td>0.951545</td>\n",
       "      <td>0.657666</td>\n",
       "      <td>4.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{'C': 1.0, 'class_weight': None, 'dual': False...</td>\n",
       "      <td>0.942147</td>\n",
       "      <td>0.942852</td>\n",
       "      <td>0.829279</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Name  \\\n",
       "2      RandomForestClassifier   \n",
       "4               XGBClassifier   \n",
       "3  GradientBoostingClassifier   \n",
       "1      DecisionTreeClassifier   \n",
       "0          LogisticRegression   \n",
       "\n",
       "                                          Parameters Train Accuracy Score  \\\n",
       "2  {'bootstrap': True, 'class_weight': None, 'cri...             0.993671   \n",
       "4  {'base_score': 0.5, 'booster': 'gbtree', 'cols...             0.954966   \n",
       "3  {'criterion': 'friedman_mse', 'init': None, 'l...             0.953365   \n",
       "1  {'class_weight': None, 'criterion': 'gini', 'm...             0.995008   \n",
       "0  {'C': 1.0, 'class_weight': None, 'dual': False...             0.942147   \n",
       "\n",
       "  Test Accuracy Score Valid Accuracy Score Comsumed Time  \n",
       "2            0.961353             0.751172          5.49  \n",
       "4            0.953131             0.778398         14.86  \n",
       "3            0.952778             0.546976         29.41  \n",
       "1            0.951545             0.657666          4.36  \n",
       "0            0.942852             0.829279          0.37  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB##GNB-- Gaussian Naive Bayes\n",
    "#import lightgbm as lgb\n",
    "\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "# from sklearn.ensemble import BaggingClassifier\n",
    "# from sklearn.ensemble import ExtraTreesClassifier\n",
    "# from sklearn.svm import LinearSVR\n",
    "# from sklearn.svm import NuSVR\n",
    "# from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "estimator_list = [\n",
    "    LogisticRegression(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    XGBClassifier(),\n",
    "    #KNeighborsClassifier(),\n",
    "    #GaussianNB(),\n",
    "]\n",
    "\n",
    "#cv_split = ShuffleSplit(n_splits=6, train_size=0.7, test_size=0.2, random_state=2019)\n",
    "df_columns = ['Name', 'Parameters', 'Train Accuracy Score', 'Test Accuracy Score', 'Valid Accuracy Score', 'Comsumed Time']\n",
    "df = pd.DataFrame(columns=df_columns)\n",
    "\n",
    "row_index = 0\n",
    "for estimator in estimator_list:\n",
    "    T1 = time.time()\n",
    "    df.loc[row_index, 'Name'] = estimator.__class__.__name__\n",
    "    df.loc[row_index, 'Parameters'] = str(estimator.get_params())\n",
    "    estimator.fit(xtrain_tfidf, train_y)\n",
    "    T2 = time.time()\n",
    "    #get_performance(estimator, xtest_tfidf, test_y)\n",
    "    #cv_results = cross_validate(estimator, train_test_X, train_test_y, cv=cv_split)\n",
    "    df.loc[row_index, 'Train Accuracy Score'] = estimator.score(xtrain_tfidf, train_y)\n",
    "    #df.loc[row_index, 'Test Accuracy Mean'] = cv_results['test_score'].mean()\n",
    "    df.loc[row_index, 'Test Accuracy Score'] = estimator.score(xtest_tfidf, test_y)\n",
    "    df.loc[row_index, 'Valid Accuracy Score'] = estimator.score(xvalid_tfidf, valid_y)\n",
    "    df.loc[row_index, 'Comsumed Time'] = round(T2-T1, 2)\n",
    "    print(row_index, estimator.__class__.__name__)\n",
    "    #print(cv_results['test_score'])\n",
    "    row_index += 1\n",
    "df = df.sort_values(by='Test Accuracy Score', ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 抄袭文章查找"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we choose the model with highest Valid Accuracy Score - Logistic Regression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(xtrain_tfidf, train_y)\n",
    "y_hat_test = clf.predict(xtest_tfidf.toarray())\n",
    "#test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "candidate_indices = []\n",
    "for index, (y,yhat) in enumerate(zip(test_y,y_hat_test)):\n",
    "    if y == 0 and yhat == 1:\n",
    "        candidate_indices.append(test_indices[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5897, 7705, 6956, 685, 8031]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_indices[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'新浪体育'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.iloc[5897]['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IP_violator = [content.iloc[i]['source'] for i in candidate_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_source_list = sorted(Counter(IP_violator), key=lambda i: Counter(IP_violator)[i], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('中国新闻网', 90),\n",
       " ('参考消息网', 41),\n",
       " ('南方日报第01版', 23),\n",
       " ('央广网', 17),\n",
       " ('环球时报', 16),\n",
       " ('新浪体育', 15),\n",
       " ('环球网', 14),\n",
       " ('人民网', 13),\n",
       " ('环球时报-环球网', 9),\n",
       " ('湖北日报第1版', 9),\n",
       " ('中国台湾网', 8),\n",
       " ('中国经济网', 8),\n",
       " ('中国证券报?中证网', 8),\n",
       " ('郑州日报第03版', 7),\n",
       " ('参考军事', 7),\n",
       " ('经济参考报', 6),\n",
       " ('河南日报第03版', 5),\n",
       " ('证券时报', 5),\n",
       " ('成都商报第01版', 5),\n",
       " ('央视网', 5),\n",
       " ('中国网', 5),\n",
       " ('长江日报第1版', 5),\n",
       " ('郑州日报第08版', 5),\n",
       " ('凤凰体育', 5),\n",
       " ('郑州日报第02版', 5),\n",
       " ('每日经济新闻', 5)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(i, Counter(IP_violator)[i]) for i in sorted_source_list if Counter(IP_violator)[i] >=5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
