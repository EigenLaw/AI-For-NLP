{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#神经网络就是一个个函数运算\" data-toc-modified-id=\"神经网络就是一个个函数运算-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>神经网络就是一个个函数运算</a></span><ul class=\"toc-item\"><li><span><a href=\"#1个epoch\" data-toc-modified-id=\"1个epoch-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>1个epoch</a></span><ul class=\"toc-item\"><li><span><a href=\"#正向传递-forward\" data-toc-modified-id=\"正向传递-forward-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>正向传递 forward</a></span></li><li><span><a href=\"#反向传递-backward\" data-toc-modified-id=\"反向传递-backward-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>反向传递 backward</a></span></li></ul></li><li><span><a href=\"#传递图示\" data-toc-modified-id=\"传递图示-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>传递图示</a></span></li><li><span><a href=\"#Python从0到1实现神经网络\" data-toc-modified-id=\"Python从0到1实现神经网络-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Python从0到1实现神经网络</a></span></li><li><span><a href=\"#使用波士顿房价数据测试\" data-toc-modified-id=\"使用波士顿房价数据测试-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>使用波士顿房价数据测试</a></span></li><li><span><a href=\"#使用Keras---TensorFlow进阶版进行分析\" data-toc-modified-id=\"使用Keras---TensorFlow进阶版进行分析-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>使用Keras - TensorFlow进阶版进行分析</a></span></li><li><span><a href=\"#激活函数表达式\" data-toc-modified-id=\"激活函数表达式-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>激活函数表达式</a></span></li><li><span><a href=\"#优化方法比较\" data-toc-modified-id=\"优化方法比较-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>优化方法比较</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 先学知识：类，初始化，函数赋值等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/jackfrued/Python-100-Days/tree/master/Day01-15\n",
    "\n",
    "-[面向对象](https://github.com/jackfrued/Python-100-Days/blob/master/Day01-15/08.%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络就是一个个函数运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1个epoch\n",
    "### 正向传递 forward\n",
    "1. Score线性: $y_j = W_n^T*X$ ;或者用非线性计算\n",
    "- Softmax计算每类预测的概率: $ \\frac{e^{y_{i}}}{\\Sigma_{j=1}^{k} e^{y}} $ ；或者用Maxout来代表概率\n",
    "- 计算交叉熵Cross entropy（信息不确定性）： $ -\\Sigma_{j=1}^{k} y_{i}log(h_{j})$；或者用MSE计算信息代表度\n",
    "- 概率最大者取到1，其他为0，得到output\n",
    "\n",
    "### 反向传递 backward\n",
    "- 逆过来求偏导\n",
    "- 更新参数和x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 传递图示\n",
    "[Reference: softmax和交叉熵图解](https://zhuanlan.zhihu.com/p/52018321)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://pic4.zhimg.com/80/v2-3ebf4f31631c4ff12c313df2a6ad4903_hd.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python从0到1实现神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, inputs=[]):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = []\n",
    "\n",
    "        for n in self.inputs:\n",
    "            n.outputs.append(self)\n",
    "            # set 'self' node as inbound_nodes's outbound_nodes\n",
    "\n",
    "        self.value = None\n",
    "\n",
    "        self.gradients = {}\n",
    "        # keys are the inputs to this node, and their\n",
    "        # values are the partials of this node with \n",
    "        # respect to that input.\n",
    "        # \\partial{node}{input_i}\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        '''\n",
    "        Forward propagation. \n",
    "        Compute the output value based on 'inbound_nodes' and store the \n",
    "        result in self.value\n",
    "        '''\n",
    "\n",
    "        raise NotImplemented\n",
    "\n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "        raise NotImplemented\n",
    "\n",
    "class Input(Node):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        An Input node has no inbound nodes.\n",
    "        So no need to pass anything to the Node instantiator.\n",
    "        '''\n",
    "        Node.__init__(self)\n",
    "\n",
    "    def forward(self, value=None):\n",
    "        '''\n",
    "        Only input node is the node where the value may be passed\n",
    "        as an argument to forward().\n",
    "        All other node implementations should get the value of the \n",
    "        previous node from self.inbound_nodes\n",
    "\n",
    "        Example: \n",
    "        val0: self.inbound_nodes[0].value\n",
    "        '''\n",
    "        if value is not None:\n",
    "            self.value = value\n",
    "            ## It's is input node, when need to forward, this node initiate self's value.\n",
    "\n",
    "        # Input subclass just holds a value, such as a data feature or a model parameter(weight/bias)\n",
    "\n",
    "    def backward(self):\n",
    "        self.gradients = {self:0}\n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self] = grad_cost * 1\n",
    "\n",
    "\n",
    "        # input N --> N1, N2\n",
    "        # \\partial L / \\partial N \n",
    "        # ==> \\partial L / \\partial N1 * \\ partial N1 / \\partial N\n",
    "\n",
    "\n",
    "class Add(Node):\n",
    "    def __init__(self, *nodes):\n",
    "        Node.__init__(self, nodes)\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        self.value = sum(map(lambda n: n.value, self.inputs))\n",
    "        ## when execute forward, this node caculate value as defined.\n",
    "\n",
    "class Linear(Node):\n",
    "    def __init__(self, nodes, weights, bias):\n",
    "        Node.__init__(self, [nodes, weights, bias])\n",
    "\n",
    "    def forward(self):\n",
    "        inputs = self.inputs[0].value\n",
    "        weights = self.inputs[1].value\n",
    "        bias = self.inputs[2].value\n",
    "\n",
    "        self.value = np.dot(inputs, weights) + bias\n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "        # initial a partial for each of the inbound_nodes.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inputs}\n",
    "\n",
    "        for n in self.outputs:\n",
    "            # Get the partial of the cost w.r.t this node.\n",
    "            grad_cost = n.gradients[self]\n",
    "\n",
    "            self.gradients[self.inputs[0]] = np.dot(grad_cost, self.inputs[1].value.T)\n",
    "            self.gradients[self.inputs[1]] = np.dot(self.inputs[0].value.T, grad_cost)\n",
    "            self.gradients[self.inputs[2]] = np.sum(grad_cost, axis=0, keepdims=False)\n",
    "\n",
    "        # WX + B / W ==> X\n",
    "        # WX + B / X ==> W\n",
    "\n",
    "class Sigmoid(Node):\n",
    "    def __init__(self, node):\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1./(1 + np.exp(-1 * x))\n",
    "\n",
    "    def forward(self):\n",
    "        self.x = self.inputs[0].value\n",
    "        self.value = self._sigmoid(self.x)\n",
    "\n",
    "    def backward(self):\n",
    "        self.partial = self._sigmoid(self.x) * (1 - self._sigmoid(self.x))\n",
    "\n",
    "        # y = 1 / (1 + e^-x)\n",
    "        # y' = 1 / (1 + e^-x) (1 - 1 / (1 + e^-x))\n",
    "\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inputs}\n",
    "\n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]  # Get the partial of the cost with respect to this node.\n",
    "\n",
    "            self.gradients[self.inputs[0]] = grad_cost * self.partial\n",
    "            # use * to keep all the dimension same!.\n",
    "\n",
    "\n",
    "\n",
    "class MSE(Node):\n",
    "    def __init__(self, y, a):\n",
    "        Node.__init__(self, [y, a])\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        y = self.inputs[0].value.reshape(-1, 1)\n",
    "        a = self.inputs[1].value.reshape(-1, 1)\n",
    "        assert(y.shape == a.shape)\n",
    "\n",
    "        self.m = self.inputs[0].value.shape[0]\n",
    "        self.diff = y - a\n",
    "\n",
    "        self.value = np.mean(self.diff**2)\n",
    "\n",
    "\n",
    "    def backward(self):\n",
    "        self.gradients[self.inputs[0]] = (2 / self.m) * self.diff\n",
    "        self.gradients[self.inputs[1]] = (-2 / self.m) * self.diff\n",
    "\n",
    "\n",
    "def forward_and_backward(outputnode, graph):\n",
    "    # execute all the forward method of sorted_nodes.\n",
    "\n",
    "    ## In practice, it's common to feed in mutiple data example in each forward pass rather than just 1. Because the examples can be processed in parallel. The number of examples is called batch size.\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "        ## each node execute forward, get self.value based on the topological sort result.\n",
    "\n",
    "    for n in  graph[::-1]:\n",
    "        n.backward()\n",
    "\n",
    "    #return outputnode.value\n",
    "\n",
    "###   v -->  a -->  C\n",
    "##    b --> C\n",
    "##    b --> v -- a --> C\n",
    "##    v --> v ---> a -- > C\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort generic nodes in topological order using Kahn's Algorithm.\n",
    "    `feed_dict`: A dictionary where the key is a `Input` node and the value is the respective value feed to that node.\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outputs:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "            ## if n is Input Node, set n'value as \n",
    "            ## feed_dict[n]\n",
    "            ## else, n's value is caculate as its\n",
    "            ## inbounds\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outputs:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def sgd_update(trainables, learning_rate=1e-2):\n",
    "    # there are so many other update / optimization methods\n",
    "    # such as Adam, Mom, \n",
    "    for t in trainables:\n",
    "        t.value += -1 * learning_rate * t.gradients[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用波士顿房价数据测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "data = load_boston()\n",
    "\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples = 506\n",
      "Epoch: 1, Loss: 173.138\n",
      "Epoch: 101, Loss: 7.503\n",
      "Epoch: 201, Loss: 6.715\n",
      "Epoch: 301, Loss: 6.043\n",
      "Epoch: 401, Loss: 4.703\n",
      "Epoch: 501, Loss: 4.727\n",
      "Epoch: 601, Loss: 4.069\n",
      "Epoch: 701, Loss: 4.805\n",
      "Epoch: 801, Loss: 4.407\n",
      "Epoch: 901, Loss: 3.935\n",
      "Epoch: 1001, Loss: 3.637\n",
      "Epoch: 1101, Loss: 4.177\n",
      "Epoch: 1201, Loss: 4.195\n",
      "Epoch: 1301, Loss: 4.467\n",
      "Epoch: 1401, Loss: 3.925\n",
      "Epoch: 1501, Loss: 3.938\n",
      "Epoch: 1601, Loss: 4.145\n",
      "Epoch: 1701, Loss: 4.050\n",
      "Epoch: 1801, Loss: 3.451\n",
      "Epoch: 1901, Loss: 3.914\n",
      "Epoch: 2001, Loss: 4.131\n",
      "Epoch: 2101, Loss: 3.767\n",
      "Epoch: 2201, Loss: 3.694\n",
      "Epoch: 2301, Loss: 4.466\n",
      "Epoch: 2401, Loss: 3.623\n",
      "Epoch: 2501, Loss: 3.494\n",
      "Epoch: 2601, Loss: 4.010\n",
      "Epoch: 2701, Loss: 3.822\n",
      "Epoch: 2801, Loss: 3.883\n",
      "Epoch: 2901, Loss: 4.060\n",
      "Epoch: 3001, Loss: 3.801\n",
      "Epoch: 3101, Loss: 3.569\n",
      "Epoch: 3201, Loss: 3.788\n",
      "Epoch: 3301, Loss: 3.589\n",
      "Epoch: 3401, Loss: 4.078\n",
      "Epoch: 3501, Loss: 4.121\n",
      "Epoch: 3601, Loss: 3.573\n",
      "Epoch: 3701, Loss: 3.406\n",
      "Epoch: 3801, Loss: 4.304\n",
      "Epoch: 3901, Loss: 3.958\n",
      "Epoch: 4001, Loss: 3.725\n",
      "Epoch: 4101, Loss: 4.103\n",
      "Epoch: 4201, Loss: 4.930\n",
      "Epoch: 4301, Loss: 4.086\n",
      "Epoch: 4401, Loss: 3.504\n",
      "Epoch: 4501, Loss: 3.827\n",
      "Epoch: 4601, Loss: 3.448\n",
      "Epoch: 4701, Loss: 3.698\n",
      "Epoch: 4801, Loss: 3.663\n",
      "Epoch: 4901, Loss: 3.690\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Check out the new network architecture and dataset!\n",
    "Notice that the weights and biases are\n",
    "generated randomly.\n",
    "No need to change anything, but feel free to tweak\n",
    "to test your network, play around with the epochs, batch size, etc!\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.utils import shuffle, resample\n",
    "#from miniflow import *\n",
    "\n",
    "# Load data\n",
    "data = load_boston()\n",
    "X_ = data['data']\n",
    "y_ = data['target']\n",
    "\n",
    "# Normalize data\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
    "\n",
    "n_features = X_.shape[1]\n",
    "n_hidden = 10\n",
    "W1_ = np.random.randn(n_features, n_hidden)\n",
    "b1_ = np.zeros(n_hidden)\n",
    "W2_ = np.random.randn(n_hidden, 1)\n",
    "b2_ = np.zeros(1)\n",
    "\n",
    "# Neural network\n",
    "X, y = Input(), Input()\n",
    "W1, b1 = Input(), Input()\n",
    "W2, b2 = Input(), Input()\n",
    "\n",
    "l1 = Linear(X, W1, b1)\n",
    "s1 = Sigmoid(l1)\n",
    "l2 = Linear(s1, W2, b2)\n",
    "cost = MSE(y, l2)\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W1: W1_,\n",
    "    b1: b1_,\n",
    "    W2: W2_,\n",
    "    b2: b2_\n",
    "}\n",
    "\n",
    "epochs = 5000\n",
    "# Total number of examples\n",
    "m = X_.shape[0]\n",
    "batch_size = 16\n",
    "steps_per_epoch = m // batch_size\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "trainables = [W1, b1, W2, b2]\n",
    "\n",
    "print(\"Total number of examples = {}\".format(m))\n",
    "\n",
    "# Step 4\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    for j in range(steps_per_epoch):\n",
    "        # Step 1\n",
    "        # Randomly sample a batch of examples\n",
    "        X_batch, y_batch = resample(X_, y_, n_samples=batch_size)\n",
    "\n",
    "        # Reset value of X and y Inputs\n",
    "        X.value = X_batch\n",
    "        y.value = y_batch\n",
    "\n",
    "        # Step 2\n",
    "        _ = None\n",
    "        forward_and_backward(_, graph) # set output node not important.\n",
    "\n",
    "        # Step 3\n",
    "        rate = 1e-2\n",
    "\n",
    "        sgd_update(trainables, rate)\n",
    "\n",
    "        loss += graph[-1].value\n",
    "\n",
    "    if i % 100 == 0: \n",
    "        print(\"Epoch: {}, Loss: {:.3f}\".format(i+1, loss/steps_per_epoch))\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x9249470>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJdDlvpAUHAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9ed00f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(losses)), losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.21995722],\n",
       "       [ 8.17686475],\n",
       "       [ 5.77763835],\n",
       "       [ 6.80566872],\n",
       "       [14.53510194],\n",
       "       [ 8.19204634],\n",
       "       [ 6.64423872],\n",
       "       [ 9.63037265],\n",
       "       [10.59204368],\n",
       "       [ 5.13821794]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_ = data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.320e-03, 1.800e+01, 2.310e+00, 0.000e+00, 5.380e-01, 6.575e+00,\n",
       "       6.520e+01, 4.090e+00, 1.000e+00, 2.960e+02, 1.530e+01, 3.969e+02,\n",
       "       4.980e+00])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Keras - TensorFlow进阶版进行分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TensorFlow练习\n",
    "- [TensorFLow Official Github](https://github.com/tensorflow/examples/tree/master/courses/udacity_deep_learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Keras先学知识\n",
    "- [Keras Official Documentation](https://keras.io/getting-started/sequential-model-guide/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.python.framework'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-c74e2bd4ca71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Globally-importable utils.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tensorflow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;31m# Try and load external backend.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor_array_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.python.framework'"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.320e-03, 1.800e+01, 2.310e+00, 0.000e+00, 5.380e-01, 6.575e+00,\n",
       "       6.520e+01, 4.090e+00, 1.000e+00, 2.960e+02, 1.530e+01, 3.969e+02,\n",
       "       4.980e+00])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=64, activation='sigmoid', input_dim=13))\n",
    "model.add(Dense(units=30, activation='sigmoid', input_dim=64))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "model.compile(loss='mse',\n",
    "              optimizer='sgd',\n",
    "              metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "506/506 [==============================] - 0s 482us/step - loss: 143.4432 - mean_squared_error: 143.4432\n",
      "Epoch 2/5000\n",
      "506/506 [==============================] - 0s 42us/step - loss: 82.3686 - mean_squared_error: 82.3686\n",
      "Epoch 3/5000\n",
      "506/506 [==============================] - 0s 42us/step - loss: 77.3528 - mean_squared_error: 77.3528\n",
      "Epoch 4/5000\n",
      "506/506 [==============================] - 0s 41us/step - loss: 76.8472 - mean_squared_error: 76.8472\n",
      "Epoch 5/5000\n",
      "506/506 [==============================] - 0s 50us/step - loss: 75.8336 - mean_squared_error: 75.8336\n",
      "Epoch 6/5000\n",
      "506/506 [==============================] - 0s 50us/step - loss: 78.7476 - mean_squared_error: 78.7476\n",
      "Epoch 7/5000\n",
      "506/506 [==============================] - 0s 40us/step - loss: 79.6914 - mean_squared_error: 79.6914\n",
      "Epoch 8/5000\n",
      "506/506 [==============================] - 0s 45us/step - loss: 78.6502 - mean_squared_error: 78.6502\n",
      "Epoch 9/5000\n",
      "506/506 [==============================] - 0s 52us/step - loss: 74.4281 - mean_squared_error: 74.4281\n",
      "Epoch 10/5000\n",
      "506/506 [==============================] - 0s 55us/step - loss: 74.3106 - mean_squared_error: 74.3106\n",
      "Epoch 11/5000\n",
      "506/506 [==============================] - 0s 64us/step - loss: 74.9850 - mean_squared_error: 74.9850\n",
      "Epoch 12/5000\n",
      "506/506 [==============================] - 0s 55us/step - loss: 74.8295 - mean_squared_error: 74.8295\n",
      "Epoch 13/5000\n",
      "506/506 [==============================] - 0s 34us/step - loss: 76.8475 - mean_squared_error: 76.8475\n",
      "Epoch 14/5000\n",
      "506/506 [==============================] - 0s 44us/step - loss: 85.1263 - mean_squared_error: 85.1263\n",
      "Epoch 15/5000\n",
      "506/506 [==============================] - 0s 47us/step - loss: 88.4637 - mean_squared_error: 88.4637\n",
      "Epoch 16/5000\n",
      "506/506 [==============================] - 0s 44us/step - loss: 89.6845 - mean_squared_error: 89.6845\n",
      "Epoch 17/5000\n",
      "506/506 [==============================] - 0s 57us/step - loss: 84.8725 - mean_squared_error: 84.8725\n",
      "Epoch 18/5000\n",
      "506/506 [==============================] - 0s 41us/step - loss: 85.7164 - mean_squared_error: 85.7164\n",
      "Epoch 19/5000\n",
      "506/506 [==============================] - 0s 35us/step - loss: 84.9748 - mean_squared_error: 84.9748\n",
      "Epoch 20/5000\n",
      "506/506 [==============================] - 0s 48us/step - loss: 86.1252 - mean_squared_error: 86.1252\n",
      "Epoch 21/5000\n",
      "506/506 [==============================] - 0s 41us/step - loss: 87.4500 - mean_squared_error: 87.4500\n",
      "Epoch 22/5000\n",
      "506/506 [==============================] - 0s 63us/step - loss: 85.7354 - mean_squared_error: 85.7354\n",
      "Epoch 23/5000\n",
      "506/506 [==============================] - 0s 39us/step - loss: 84.9278 - mean_squared_error: 84.9278\n",
      "Epoch 24/5000\n",
      "506/506 [==============================] - 0s 41us/step - loss: 85.7913 - mean_squared_error: 85.7913\n",
      "Epoch 25/5000\n",
      "506/506 [==============================] - 0s 47us/step - loss: 85.0535 - mean_squared_error: 85.0535\n",
      "Epoch 26/5000\n",
      "506/506 [==============================] - 0s 44us/step - loss: 85.0658 - mean_squared_error: 85.0658\n",
      "Epoch 27/5000\n",
      "506/506 [==============================] - 0s 50us/step - loss: 85.7904 - mean_squared_error: 85.7904\n",
      "Epoch 28/5000\n",
      "506/506 [==============================] - 0s 69us/step - loss: 85.2221 - mean_squared_error: 85.2221\n",
      "Epoch 29/5000\n",
      "506/506 [==============================] - 0s 45us/step - loss: 85.4434 - mean_squared_error: 85.4434\n",
      "Epoch 30/5000\n",
      "506/506 [==============================] - 0s 37us/step - loss: 85.6103 - mean_squared_error: 85.6103\n",
      "Epoch 31/5000\n",
      "506/506 [==============================] - 0s 36us/step - loss: 85.4166 - mean_squared_error: 85.4166\n",
      "Epoch 32/5000\n",
      "506/506 [==============================] - 0s 41us/step - loss: 84.5406 - mean_squared_error: 84.5406\n",
      "Epoch 33/5000\n",
      "506/506 [==============================] - 0s 37us/step - loss: 85.1172 - mean_squared_error: 85.1172\n",
      "Epoch 34/5000\n",
      "506/506 [==============================] - 0s 40us/step - loss: 85.6599 - mean_squared_error: 85.6599\n",
      "Epoch 35/5000\n",
      "506/506 [==============================] - 0s 56us/step - loss: 85.1513 - mean_squared_error: 85.1513\n",
      "Epoch 36/5000\n",
      "506/506 [==============================] - 0s 40us/step - loss: 84.8684 - mean_squared_error: 84.8684\n",
      "Epoch 37/5000\n",
      "506/506 [==============================] - 0s 33us/step - loss: 85.7398 - mean_squared_error: 85.7398\n",
      "Epoch 38/5000\n",
      "506/506 [==============================] - 0s 60us/step - loss: 84.5626 - mean_squared_error: 84.5626\n",
      "Epoch 39/5000\n",
      "506/506 [==============================] - 0s 54us/step - loss: 84.6975 - mean_squared_error: 84.6975\n",
      "Epoch 40/5000\n",
      "506/506 [==============================] - 0s 45us/step - loss: 84.4825 - mean_squared_error: 84.4825\n",
      "Epoch 41/5000\n",
      "506/506 [==============================] - 0s 39us/step - loss: 84.5680 - mean_squared_error: 84.5680\n",
      "Epoch 42/5000\n",
      "506/506 [==============================] - 0s 67us/step - loss: 84.9623 - mean_squared_error: 84.9623\n",
      "Epoch 43/5000\n",
      "506/506 [==============================] - 0s 37us/step - loss: 85.3657 - mean_squared_error: 85.3657\n",
      "Epoch 44/5000\n",
      "506/506 [==============================] - 0s 42us/step - loss: 85.3019 - mean_squared_error: 85.3019\n",
      "Epoch 45/5000\n",
      "506/506 [==============================] - 0s 40us/step - loss: 85.2700 - mean_squared_error: 85.2700\n",
      "Epoch 46/5000\n",
      "506/506 [==============================] - 0s 45us/step - loss: 85.1239 - mean_squared_error: 85.1239\n",
      "Epoch 47/5000\n",
      "506/506 [==============================] - 0s 37us/step - loss: 84.9346 - mean_squared_error: 84.9346\n",
      "Epoch 48/5000\n",
      "506/506 [==============================] - 0s 64us/step - loss: 84.8322 - mean_squared_error: 84.8322\n",
      "Epoch 49/5000\n",
      "506/506 [==============================] - 0s 44us/step - loss: 85.3293 - mean_squared_error: 85.3293\n",
      "Epoch 50/5000\n",
      "506/506 [==============================] - 0s 37us/step - loss: 85.4013 - mean_squared_error: 85.4013\n",
      "Epoch 51/5000\n",
      "506/506 [==============================] - 0s 43us/step - loss: 85.6436 - mean_squared_error: 85.6436\n",
      "Epoch 52/5000\n",
      "506/506 [==============================] - 0s 43us/step - loss: 85.6256 - mean_squared_error: 85.6256\n",
      "Epoch 53/5000\n",
      "506/506 [==============================] - 0s 50us/step - loss: 84.7833 - mean_squared_error: 84.7833\n",
      "Epoch 54/5000\n",
      "506/506 [==============================] - 0s 46us/step - loss: 84.8088 - mean_squared_error: 84.8088\n",
      "Epoch 55/5000\n",
      "506/506 [==============================] - 0s 75us/step - loss: 84.6359 - mean_squared_error: 84.6359\n",
      "Epoch 56/5000\n",
      "506/506 [==============================] - 0s 41us/step - loss: 84.4420 - mean_squared_error: 84.4420\n",
      "Epoch 57/5000\n",
      "506/506 [==============================] - 0s 51us/step - loss: 85.5745 - mean_squared_error: 85.5745\n",
      "Epoch 58/5000\n",
      "506/506 [==============================] - 0s 48us/step - loss: 85.0695 - mean_squared_error: 85.0695\n",
      "Epoch 59/5000\n",
      "506/506 [==============================] - 0s 58us/step - loss: 84.7676 - mean_squared_error: 84.7676\n",
      "Epoch 60/5000\n",
      "506/506 [==============================] - 0s 55us/step - loss: 84.5295 - mean_squared_error: 84.5295\n",
      "Epoch 61/5000\n",
      "506/506 [==============================] - 0s 46us/step - loss: 84.7503 - mean_squared_error: 84.7503\n",
      "Epoch 62/5000\n",
      "506/506 [==============================] - 0s 73us/step - loss: 85.2943 - mean_squared_error: 85.2943\n",
      "Epoch 63/5000\n",
      "506/506 [==============================] - 0s 46us/step - loss: 85.2788 - mean_squared_error: 85.2788\n",
      "Epoch 64/5000\n",
      "506/506 [==============================] - 0s 55us/step - loss: 85.3548 - mean_squared_error: 85.3548\n",
      "Epoch 65/5000\n",
      "506/506 [==============================] - 0s 54us/step - loss: 84.6432 - mean_squared_error: 84.6432\n",
      "Epoch 66/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 84.5094 - mean_squared_error: 84.5094\n",
      "Epoch 67/5000\n",
      "506/506 [==============================] - 0s 79us/step - loss: 84.8343 - mean_squared_error: 84.8343\n",
      "Epoch 68/5000\n",
      "506/506 [==============================] - 0s 48us/step - loss: 85.3985 - mean_squared_error: 85.3985\n",
      "Epoch 69/5000\n",
      "506/506 [==============================] - 0s 55us/step - loss: 84.9762 - mean_squared_error: 84.9762\n",
      "Epoch 70/5000\n",
      "506/506 [==============================] - 0s 35us/step - loss: 85.1308 - mean_squared_error: 85.1308\n",
      "Epoch 71/5000\n",
      "506/506 [==============================] - 0s 36us/step - loss: 85.1061 - mean_squared_error: 85.1061\n",
      "Epoch 72/5000\n",
      "506/506 [==============================] - 0s 42us/step - loss: 85.9251 - mean_squared_error: 85.9251\n",
      "Epoch 73/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 85.6596 - mean_squared_error: 85.6596\n",
      "Epoch 74/5000\n",
      "506/506 [==============================] - 0s 69us/step - loss: 85.0436 - mean_squared_error: 85.0436\n",
      "Epoch 75/5000\n",
      "506/506 [==============================] - 0s 63us/step - loss: 84.7368 - mean_squared_error: 84.7368\n",
      "Epoch 76/5000\n",
      "506/506 [==============================] - 0s 59us/step - loss: 84.8656 - mean_squared_error: 84.8656\n",
      "Epoch 77/5000\n",
      "506/506 [==============================] - 0s 46us/step - loss: 84.8080 - mean_squared_error: 84.8080\n",
      "Epoch 78/5000\n",
      "506/506 [==============================] - 0s 57us/step - loss: 85.2302 - mean_squared_error: 85.2302\n",
      "Epoch 79/5000\n",
      "506/506 [==============================] - 0s 63us/step - loss: 85.5889 - mean_squared_error: 85.5889\n",
      "Epoch 80/5000\n",
      "506/506 [==============================] - 0s 64us/step - loss: 85.1431 - mean_squared_error: 85.1431\n",
      "Epoch 81/5000\n",
      "506/506 [==============================] - 0s 60us/step - loss: 85.8358 - mean_squared_error: 85.8358\n",
      "Epoch 82/5000\n",
      "506/506 [==============================] - 0s 59us/step - loss: 85.7320 - mean_squared_error: 85.7320\n",
      "Epoch 83/5000\n",
      "506/506 [==============================] - 0s 51us/step - loss: 85.0127 - mean_squared_error: 85.0127\n",
      "Epoch 84/5000\n",
      "506/506 [==============================] - 0s 62us/step - loss: 85.7259 - mean_squared_error: 85.7259\n",
      "Epoch 85/5000\n",
      "506/506 [==============================] - 0s 56us/step - loss: 86.2586 - mean_squared_error: 86.2586\n",
      "Epoch 86/5000\n",
      "506/506 [==============================] - 0s 54us/step - loss: 85.3898 - mean_squared_error: 85.3898\n",
      "Epoch 87/5000\n",
      "506/506 [==============================] - 0s 69us/step - loss: 84.9969 - mean_squared_error: 84.9969\n",
      "Epoch 88/5000\n",
      "506/506 [==============================] - 0s 54us/step - loss: 85.2161 - mean_squared_error: 85.2161\n",
      "Epoch 89/5000\n",
      "506/506 [==============================] - 0s 46us/step - loss: 84.8121 - mean_squared_error: 84.8121\n",
      "Epoch 90/5000\n",
      "506/506 [==============================] - 0s 58us/step - loss: 85.3470 - mean_squared_error: 85.3470\n",
      "Epoch 91/5000\n",
      "506/506 [==============================] - 0s 75us/step - loss: 85.2808 - mean_squared_error: 85.2808\n",
      "Epoch 92/5000\n",
      "506/506 [==============================] - 0s 62us/step - loss: 85.5244 - mean_squared_error: 85.5244\n",
      "Epoch 93/5000\n",
      "506/506 [==============================] - 0s 54us/step - loss: 85.7538 - mean_squared_error: 85.7538\n",
      "Epoch 94/5000\n",
      "506/506 [==============================] - 0s 60us/step - loss: 85.2039 - mean_squared_error: 85.2039\n",
      "Epoch 95/5000\n",
      "506/506 [==============================] - 0s 64us/step - loss: 85.0566 - mean_squared_error: 85.0566\n",
      "Epoch 96/5000\n",
      "506/506 [==============================] - 0s 63us/step - loss: 85.2410 - mean_squared_error: 85.2410\n",
      "Epoch 97/5000\n",
      "506/506 [==============================] - 0s 56us/step - loss: 85.0608 - mean_squared_error: 85.0608\n",
      "Epoch 98/5000\n",
      "506/506 [==============================] - 0s 59us/step - loss: 85.9028 - mean_squared_error: 85.9028\n",
      "Epoch 99/5000\n",
      "506/506 [==============================] - 0s 55us/step - loss: 85.3375 - mean_squared_error: 85.3375\n",
      "Epoch 100/5000\n",
      "506/506 [==============================] - 0s 69us/step - loss: 84.9887 - mean_squared_error: 84.9887\n",
      "Epoch 101/5000\n",
      "506/506 [==============================] - 0s 49us/step - loss: 84.8838 - mean_squared_error: 84.8838\n",
      "Epoch 102/5000\n",
      "506/506 [==============================] - 0s 59us/step - loss: 85.1226 - mean_squared_error: 85.1226\n",
      "Epoch 103/5000\n",
      "506/506 [==============================] - 0s 58us/step - loss: 84.5668 - mean_squared_error: 84.5668\n",
      "Epoch 104/5000\n",
      "506/506 [==============================] - 0s 81us/step - loss: 84.4731 - mean_squared_error: 84.4731\n",
      "Epoch 105/5000\n",
      "506/506 [==============================] - 0s 64us/step - loss: 84.9594 - mean_squared_error: 84.9594\n",
      "Epoch 106/5000\n",
      "506/506 [==============================] - 0s 59us/step - loss: 85.6458 - mean_squared_error: 85.6458\n",
      "Epoch 107/5000\n",
      "506/506 [==============================] - 0s 71us/step - loss: 85.5375 - mean_squared_error: 85.5375\n",
      "Epoch 108/5000\n",
      "506/506 [==============================] - 0s 75us/step - loss: 84.2950 - mean_squared_error: 84.2950\n",
      "Epoch 109/5000\n",
      "506/506 [==============================] - 0s 56us/step - loss: 85.5529 - mean_squared_error: 85.5529\n",
      "Epoch 110/5000\n",
      "506/506 [==============================] - 0s 63us/step - loss: 84.9323 - mean_squared_error: 84.9323\n",
      "Epoch 111/5000\n",
      "506/506 [==============================] - 0s 65us/step - loss: 84.8903 - mean_squared_error: 84.8903\n",
      "Epoch 112/5000\n",
      "506/506 [==============================] - 0s 66us/step - loss: 85.3986 - mean_squared_error: 85.3986\n",
      "Epoch 113/5000\n",
      "506/506 [==============================] - 0s 71us/step - loss: 85.1139 - mean_squared_error: 85.1139\n",
      "Epoch 114/5000\n",
      "506/506 [==============================] - 0s 56us/step - loss: 84.9884 - mean_squared_error: 84.9884\n",
      "Epoch 115/5000\n",
      "506/506 [==============================] - 0s 70us/step - loss: 85.2846 - mean_squared_error: 85.2846\n",
      "Epoch 116/5000\n",
      "506/506 [==============================] - 0s 74us/step - loss: 84.8478 - mean_squared_error: 84.8478\n",
      "Epoch 117/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 84.7682 - mean_squared_error: 84.7682\n",
      "Epoch 118/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 84.6082 - mean_squared_error: 84.6082\n",
      "Epoch 119/5000\n",
      "506/506 [==============================] - 0s 60us/step - loss: 85.3597 - mean_squared_error: 85.3597\n",
      "Epoch 120/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 84.8531 - mean_squared_error: 84.8531\n",
      "Epoch 121/5000\n",
      "506/506 [==============================] - 0s 57us/step - loss: 85.7689 - mean_squared_error: 85.7689\n",
      "Epoch 122/5000\n",
      "506/506 [==============================] - 0s 69us/step - loss: 84.9442 - mean_squared_error: 84.9442\n",
      "Epoch 123/5000\n",
      "506/506 [==============================] - 0s 68us/step - loss: 85.6404 - mean_squared_error: 85.6404\n",
      "Epoch 124/5000\n",
      "506/506 [==============================] - 0s 56us/step - loss: 85.0244 - mean_squared_error: 85.0244\n",
      "Epoch 125/5000\n",
      "506/506 [==============================] - 0s 64us/step - loss: 85.1091 - mean_squared_error: 85.1091\n",
      "Epoch 126/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 85.4748 - mean_squared_error: 85.4748\n",
      "Epoch 127/5000\n",
      "506/506 [==============================] - 0s 57us/step - loss: 85.8782 - mean_squared_error: 85.8782\n",
      "Epoch 128/5000\n",
      "506/506 [==============================] - 0s 40us/step - loss: 85.0747 - mean_squared_error: 85.0747\n",
      "Epoch 129/5000\n",
      "506/506 [==============================] - 0s 37us/step - loss: 85.2666 - mean_squared_error: 85.2666\n",
      "Epoch 130/5000\n",
      "506/506 [==============================] - 0s 50us/step - loss: 85.1888 - mean_squared_error: 85.1888\n",
      "Epoch 131/5000\n",
      "506/506 [==============================] - 0s 66us/step - loss: 85.2130 - mean_squared_error: 85.2130\n",
      "Epoch 132/5000\n",
      "506/506 [==============================] - 0s 44us/step - loss: 84.6154 - mean_squared_error: 84.6154\n",
      "Epoch 133/5000\n",
      "506/506 [==============================] - 0s 56us/step - loss: 84.9958 - mean_squared_error: 84.9958\n",
      "Epoch 134/5000\n",
      "506/506 [==============================] - 0s 58us/step - loss: 85.0678 - mean_squared_error: 85.0678\n",
      "Epoch 135/5000\n",
      "506/506 [==============================] - 0s 62us/step - loss: 84.7094 - mean_squared_error: 84.7094\n",
      "Epoch 136/5000\n",
      "506/506 [==============================] - 0s 51us/step - loss: 84.9481 - mean_squared_error: 84.9481\n",
      "Epoch 137/5000\n",
      "506/506 [==============================] - 0s 57us/step - loss: 85.1520 - mean_squared_error: 85.1520\n",
      "Epoch 138/5000\n",
      "506/506 [==============================] - 0s 62us/step - loss: 85.3930 - mean_squared_error: 85.3930\n",
      "Epoch 139/5000\n",
      "506/506 [==============================] - 0s 69us/step - loss: 84.8964 - mean_squared_error: 84.8964\n",
      "Epoch 140/5000\n",
      "506/506 [==============================] - 0s 68us/step - loss: 84.9530 - mean_squared_error: 84.9530\n",
      "Epoch 141/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 62us/step - loss: 85.3288 - mean_squared_error: 85.3288\n",
      "Epoch 142/5000\n",
      "506/506 [==============================] - 0s 49us/step - loss: 85.0092 - mean_squared_error: 85.0092\n",
      "Epoch 143/5000\n",
      "506/506 [==============================] - 0s 64us/step - loss: 85.2259 - mean_squared_error: 85.2259\n",
      "Epoch 144/5000\n",
      "506/506 [==============================] - 0s 80us/step - loss: 85.1957 - mean_squared_error: 85.1957\n",
      "Epoch 145/5000\n",
      "506/506 [==============================] - 0s 66us/step - loss: 85.4370 - mean_squared_error: 85.4370\n",
      "Epoch 146/5000\n",
      "506/506 [==============================] - 0s 56us/step - loss: 85.3190 - mean_squared_error: 85.3190\n",
      "Epoch 147/5000\n",
      "506/506 [==============================] - 0s 56us/step - loss: 85.7632 - mean_squared_error: 85.7632\n",
      "Epoch 148/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 84.8774 - mean_squared_error: 84.8774\n",
      "Epoch 149/5000\n",
      "506/506 [==============================] - 0s 55us/step - loss: 85.3631 - mean_squared_error: 85.3631\n",
      "Epoch 150/5000\n",
      "506/506 [==============================] - 0s 62us/step - loss: 84.9701 - mean_squared_error: 84.9701\n",
      "Epoch 151/5000\n",
      "506/506 [==============================] - 0s 48us/step - loss: 85.5819 - mean_squared_error: 85.5819\n",
      "Epoch 152/5000\n",
      "506/506 [==============================] - 0s 56us/step - loss: 84.9883 - mean_squared_error: 84.9883\n",
      "Epoch 153/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 84.5349 - mean_squared_error: 84.5349\n",
      "Epoch 154/5000\n",
      "506/506 [==============================] - 0s 74us/step - loss: 84.5096 - mean_squared_error: 84.5096\n",
      "Epoch 155/5000\n",
      "506/506 [==============================] - 0s 66us/step - loss: 85.0800 - mean_squared_error: 85.0800\n",
      "Epoch 156/5000\n",
      "506/506 [==============================] - 0s 52us/step - loss: 84.8302 - mean_squared_error: 84.8302\n",
      "Epoch 157/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 85.3333 - mean_squared_error: 85.3333\n",
      "Epoch 158/5000\n",
      "506/506 [==============================] - 0s 75us/step - loss: 85.0880 - mean_squared_error: 85.0880\n",
      "Epoch 159/5000\n",
      "506/506 [==============================] - 0s 67us/step - loss: 86.1177 - mean_squared_error: 86.1177\n",
      "Epoch 160/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 85.0748 - mean_squared_error: 85.0748\n",
      "Epoch 161/5000\n",
      "506/506 [==============================] - 0s 54us/step - loss: 85.0227 - mean_squared_error: 85.0227\n",
      "Epoch 162/5000\n",
      "506/506 [==============================] - 0s 60us/step - loss: 85.3481 - mean_squared_error: 85.3481\n",
      "Epoch 163/5000\n",
      "506/506 [==============================] - 0s 54us/step - loss: 85.1629 - mean_squared_error: 85.1629\n",
      "Epoch 164/5000\n",
      "506/506 [==============================] - 0s 66us/step - loss: 84.9194 - mean_squared_error: 84.9194\n",
      "Epoch 165/5000\n",
      "506/506 [==============================] - 0s 62us/step - loss: 85.2581 - mean_squared_error: 85.2581\n",
      "Epoch 166/5000\n",
      "506/506 [==============================] - 0s 64us/step - loss: 85.2600 - mean_squared_error: 85.2600\n",
      "Epoch 167/5000\n",
      "506/506 [==============================] - 0s 72us/step - loss: 84.9868 - mean_squared_error: 84.9868\n",
      "Epoch 168/5000\n",
      "506/506 [==============================] - 0s 71us/step - loss: 85.0238 - mean_squared_error: 85.0238\n",
      "Epoch 169/5000\n",
      "506/506 [==============================] - 0s 58us/step - loss: 85.5384 - mean_squared_error: 85.5384\n",
      "Epoch 170/5000\n",
      "506/506 [==============================] - 0s 56us/step - loss: 84.2228 - mean_squared_error: 84.2228\n",
      "Epoch 171/5000\n",
      "506/506 [==============================] - 0s 55us/step - loss: 85.3065 - mean_squared_error: 85.3065\n",
      "Epoch 172/5000\n",
      "506/506 [==============================] - 0s 56us/step - loss: 84.3533 - mean_squared_error: 84.3533\n",
      "Epoch 173/5000\n",
      "506/506 [==============================] - 0s 56us/step - loss: 85.2767 - mean_squared_error: 85.2767\n",
      "Epoch 174/5000\n",
      "506/506 [==============================] - 0s 72us/step - loss: 85.0150 - mean_squared_error: 85.0150\n",
      "Epoch 175/5000\n",
      "506/506 [==============================] - 0s 64us/step - loss: 84.8051 - mean_squared_error: 84.8051\n",
      "Epoch 176/5000\n",
      "506/506 [==============================] - 0s 50us/step - loss: 84.7357 - mean_squared_error: 84.7357\n",
      "Epoch 177/5000\n",
      "506/506 [==============================] - 0s 57us/step - loss: 85.0936 - mean_squared_error: 85.0936\n",
      "Epoch 178/5000\n",
      "506/506 [==============================] - 0s 52us/step - loss: 85.3993 - mean_squared_error: 85.3993\n",
      "Epoch 179/5000\n",
      "506/506 [==============================] - 0s 49us/step - loss: 85.0618 - mean_squared_error: 85.0618\n",
      "Epoch 180/5000\n",
      "506/506 [==============================] - 0s 46us/step - loss: 85.1448 - mean_squared_error: 85.1448\n",
      "Epoch 181/5000\n",
      "506/506 [==============================] - 0s 78us/step - loss: 85.4502 - mean_squared_error: 85.4502\n",
      "Epoch 182/5000\n",
      "506/506 [==============================] - 0s 57us/step - loss: 84.6504 - mean_squared_error: 84.6504\n",
      "Epoch 183/5000\n",
      "506/506 [==============================] - 0s 67us/step - loss: 84.7891 - mean_squared_error: 84.7891\n",
      "Epoch 184/5000\n",
      "506/506 [==============================] - 0s 51us/step - loss: 84.9234 - mean_squared_error: 84.9234\n",
      "Epoch 185/5000\n",
      "506/506 [==============================] - 0s 66us/step - loss: 84.8754 - mean_squared_error: 84.8754\n",
      "Epoch 186/5000\n",
      "506/506 [==============================] - 0s 62us/step - loss: 84.6737 - mean_squared_error: 84.6737\n",
      "Epoch 187/5000\n",
      "506/506 [==============================] - 0s 67us/step - loss: 85.1154 - mean_squared_error: 85.1154\n",
      "Epoch 188/5000\n",
      "506/506 [==============================] - 0s 49us/step - loss: 85.9534 - mean_squared_error: 85.9534\n",
      "Epoch 189/5000\n",
      "506/506 [==============================] - 0s 49us/step - loss: 85.1888 - mean_squared_error: 85.1888\n",
      "Epoch 190/5000\n",
      "506/506 [==============================] - 0s 37us/step - loss: 85.6461 - mean_squared_error: 85.6461\n",
      "Epoch 191/5000\n",
      "506/506 [==============================] - 0s 54us/step - loss: 85.4531 - mean_squared_error: 85.4531\n",
      "Epoch 192/5000\n",
      "506/506 [==============================] - 0s 47us/step - loss: 84.9078 - mean_squared_error: 84.9078\n",
      "Epoch 193/5000\n",
      "506/506 [==============================] - 0s 43us/step - loss: 85.5552 - mean_squared_error: 85.5552\n",
      "Epoch 194/5000\n",
      "506/506 [==============================] - 0s 41us/step - loss: 84.2000 - mean_squared_error: 84.2000\n",
      "Epoch 195/5000\n",
      "506/506 [==============================] - 0s 50us/step - loss: 85.3122 - mean_squared_error: 85.3122\n",
      "Epoch 196/5000\n",
      "506/506 [==============================] - 0s 37us/step - loss: 85.1241 - mean_squared_error: 85.1241\n",
      "Epoch 197/5000\n",
      "506/506 [==============================] - 0s 38us/step - loss: 84.6246 - mean_squared_error: 84.6246\n",
      "Epoch 198/5000\n",
      "506/506 [==============================] - 0s 45us/step - loss: 84.9514 - mean_squared_error: 84.9514\n",
      "Epoch 199/5000\n",
      "506/506 [==============================] - 0s 31us/step - loss: 85.6158 - mean_squared_error: 85.6158\n",
      "Epoch 200/5000\n",
      "506/506 [==============================] - 0s 69us/step - loss: 84.9214 - mean_squared_error: 84.9214\n",
      "Epoch 201/5000\n",
      "506/506 [==============================] - 0s 51us/step - loss: 84.9986 - mean_squared_error: 84.9986\n",
      "Epoch 202/5000\n",
      "506/506 [==============================] - 0s 57us/step - loss: 84.7934 - mean_squared_error: 84.7934\n",
      "Epoch 203/5000\n",
      "506/506 [==============================] - 0s 62us/step - loss: 85.2759 - mean_squared_error: 85.2759\n",
      "Epoch 204/5000\n",
      "506/506 [==============================] - 0s 71us/step - loss: 84.9476 - mean_squared_error: 84.9476\n",
      "Epoch 205/5000\n",
      "506/506 [==============================] - 0s 66us/step - loss: 84.7529 - mean_squared_error: 84.7529\n",
      "Epoch 206/5000\n",
      "506/506 [==============================] - 0s 54us/step - loss: 84.6415 - mean_squared_error: 84.6415\n",
      "Epoch 207/5000\n",
      "506/506 [==============================] - 0s 59us/step - loss: 85.2891 - mean_squared_error: 85.2891\n",
      "Epoch 208/5000\n",
      "506/506 [==============================] - 0s 51us/step - loss: 85.0432 - mean_squared_error: 85.0432\n",
      "Epoch 209/5000\n",
      "506/506 [==============================] - 0s 59us/step - loss: 84.8955 - mean_squared_error: 84.8955\n",
      "Epoch 210/5000\n",
      "506/506 [==============================] - 0s 53us/step - loss: 85.2339 - mean_squared_error: 85.2339\n",
      "Epoch 211/5000\n",
      "506/506 [==============================] - 0s 54us/step - loss: 84.9909 - mean_squared_error: 84.9909\n",
      "Epoch 212/5000\n",
      "506/506 [==============================] - 0s 52us/step - loss: 84.8783 - mean_squared_error: 84.8783\n",
      "Epoch 213/5000\n",
      "506/506 [==============================] - 0s 65us/step - loss: 85.1552 - mean_squared_error: 85.1552\n",
      "Epoch 214/5000\n",
      "506/506 [==============================] - 0s 55us/step - loss: 85.5238 - mean_squared_error: 85.5238\n",
      "Epoch 215/5000\n",
      "506/506 [==============================] - 0s 59us/step - loss: 85.6824 - mean_squared_error: 85.6824\n",
      "Epoch 216/5000\n",
      "506/506 [==============================] - 0s 50us/step - loss: 85.4589 - mean_squared_error: 85.4589\n",
      "Epoch 217/5000\n",
      "506/506 [==============================] - 0s 47us/step - loss: 84.9196 - mean_squared_error: 84.9196\n",
      "Epoch 218/5000\n",
      "506/506 [==============================] - 0s 74us/step - loss: 85.6688 - mean_squared_error: 85.6688\n",
      "Epoch 219/5000\n",
      "506/506 [==============================] - 0s 48us/step - loss: 84.7067 - mean_squared_error: 84.7067\n",
      "Epoch 220/5000\n",
      "506/506 [==============================] - 0s 53us/step - loss: 85.2773 - mean_squared_error: 85.2773\n",
      "Epoch 221/5000\n",
      "506/506 [==============================] - 0s 54us/step - loss: 85.4952 - mean_squared_error: 85.4952\n",
      "Epoch 222/5000\n",
      "506/506 [==============================] - 0s 55us/step - loss: 85.4957 - mean_squared_error: 85.4957\n",
      "Epoch 223/5000\n",
      "506/506 [==============================] - 0s 52us/step - loss: 85.6457 - mean_squared_error: 85.6457\n",
      "Epoch 224/5000\n",
      "506/506 [==============================] - 0s 52us/step - loss: 84.8547 - mean_squared_error: 84.8547\n",
      "Epoch 225/5000\n",
      "506/506 [==============================] - 0s 47us/step - loss: 85.1250 - mean_squared_error: 85.1250\n",
      "Epoch 226/5000\n",
      "506/506 [==============================] - 0s 55us/step - loss: 85.1111 - mean_squared_error: 85.1111\n",
      "Epoch 227/5000\n",
      "506/506 [==============================] - 0s 62us/step - loss: 85.0247 - mean_squared_error: 85.0247\n",
      "Epoch 228/5000\n",
      "506/506 [==============================] - 0s 46us/step - loss: 84.9390 - mean_squared_error: 84.9390\n",
      "Epoch 229/5000\n",
      "506/506 [==============================] - 0s 53us/step - loss: 85.0400 - mean_squared_error: 85.0400\n",
      "Epoch 230/5000\n",
      "506/506 [==============================] - 0s 45us/step - loss: 85.1800 - mean_squared_error: 85.1800\n",
      "Epoch 231/5000\n",
      "506/506 [==============================] - 0s 70us/step - loss: 85.6167 - mean_squared_error: 85.6167\n",
      "Epoch 232/5000\n",
      "506/506 [==============================] - 0s 53us/step - loss: 85.9130 - mean_squared_error: 85.9130\n",
      "Epoch 233/5000\n",
      "506/506 [==============================] - 0s 52us/step - loss: 84.7820 - mean_squared_error: 84.7820\n",
      "Epoch 234/5000\n",
      "506/506 [==============================] - 0s 46us/step - loss: 84.7864 - mean_squared_error: 84.7864\n",
      "Epoch 235/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 84.9280 - mean_squared_error: 84.9280\n",
      "Epoch 236/5000\n",
      "506/506 [==============================] - 0s 58us/step - loss: 85.3473 - mean_squared_error: 85.3473\n",
      "Epoch 237/5000\n",
      "506/506 [==============================] - 0s 55us/step - loss: 84.8159 - mean_squared_error: 84.8159\n",
      "Epoch 238/5000\n",
      "506/506 [==============================] - 0s 59us/step - loss: 84.7411 - mean_squared_error: 84.7411\n",
      "Epoch 239/5000\n",
      "506/506 [==============================] - 0s 60us/step - loss: 84.7734 - mean_squared_error: 84.7734\n",
      "Epoch 240/5000\n",
      "506/506 [==============================] - 0s 74us/step - loss: 85.2056 - mean_squared_error: 85.2056\n",
      "Epoch 241/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 84.7248 - mean_squared_error: 84.7248\n",
      "Epoch 242/5000\n",
      "506/506 [==============================] - 0s 53us/step - loss: 84.6175 - mean_squared_error: 84.6175\n",
      "Epoch 243/5000\n",
      "506/506 [==============================] - 0s 63us/step - loss: 84.7081 - mean_squared_error: 84.7081\n",
      "Epoch 244/5000\n",
      "506/506 [==============================] - 0s 75us/step - loss: 86.0801 - mean_squared_error: 86.0801\n",
      "Epoch 245/5000\n",
      "506/506 [==============================] - 0s 68us/step - loss: 85.0040 - mean_squared_error: 85.0040\n",
      "Epoch 246/5000\n",
      "506/506 [==============================] - 0s 57us/step - loss: 85.3627 - mean_squared_error: 85.3627\n",
      "Epoch 247/5000\n",
      "506/506 [==============================] - 0s 73us/step - loss: 85.4428 - mean_squared_error: 85.4428\n",
      "Epoch 248/5000\n",
      "506/506 [==============================] - 0s 69us/step - loss: 85.2151 - mean_squared_error: 85.2151\n",
      "Epoch 249/5000\n",
      "506/506 [==============================] - 0s 62us/step - loss: 84.5661 - mean_squared_error: 84.5661\n",
      "Epoch 250/5000\n",
      "506/506 [==============================] - 0s 58us/step - loss: 85.4826 - mean_squared_error: 85.4826\n",
      "Epoch 251/5000\n",
      "506/506 [==============================] - 0s 49us/step - loss: 85.1821 - mean_squared_error: 85.1821\n",
      "Epoch 252/5000\n",
      "506/506 [==============================] - 0s 48us/step - loss: 84.7725 - mean_squared_error: 84.7725\n",
      "Epoch 253/5000\n",
      "506/506 [==============================] - 0s 73us/step - loss: 85.7255 - mean_squared_error: 85.7255\n",
      "Epoch 254/5000\n",
      "506/506 [==============================] - 0s 60us/step - loss: 85.7034 - mean_squared_error: 85.7034\n",
      "Epoch 255/5000\n",
      "506/506 [==============================] - 0s 48us/step - loss: 85.0612 - mean_squared_error: 85.0612\n",
      "Epoch 256/5000\n",
      "506/506 [==============================] - 0s 45us/step - loss: 85.8056 - mean_squared_error: 85.8056\n",
      "Epoch 257/5000\n",
      "506/506 [==============================] - 0s 43us/step - loss: 85.0539 - mean_squared_error: 85.0539\n",
      "Epoch 258/5000\n",
      "506/506 [==============================] - 0s 50us/step - loss: 84.9245 - mean_squared_error: 84.9245\n",
      "Epoch 259/5000\n",
      "506/506 [==============================] - 0s 35us/step - loss: 84.6080 - mean_squared_error: 84.6080\n",
      "Epoch 260/5000\n",
      "506/506 [==============================] - 0s 36us/step - loss: 84.6883 - mean_squared_error: 84.6883\n",
      "Epoch 261/5000\n",
      "506/506 [==============================] - 0s 38us/step - loss: 84.2552 - mean_squared_error: 84.2552\n",
      "Epoch 262/5000\n",
      "506/506 [==============================] - 0s 48us/step - loss: 84.7298 - mean_squared_error: 84.7298\n",
      "Epoch 263/5000\n",
      "506/506 [==============================] - 0s 30us/step - loss: 84.4470 - mean_squared_error: 84.4470\n",
      "Epoch 264/5000\n",
      "506/506 [==============================] - 0s 45us/step - loss: 84.3399 - mean_squared_error: 84.3399\n",
      "Epoch 265/5000\n",
      "506/506 [==============================] - 0s 47us/step - loss: 84.9714 - mean_squared_error: 84.9714\n",
      "Epoch 266/5000\n",
      "506/506 [==============================] - 0s 58us/step - loss: 84.3199 - mean_squared_error: 84.3199\n",
      "Epoch 267/5000\n",
      "506/506 [==============================] - 0s 63us/step - loss: 84.1250 - mean_squared_error: 84.1250\n",
      "Epoch 268/5000\n",
      "506/506 [==============================] - 0s 59us/step - loss: 85.5904 - mean_squared_error: 85.5904\n",
      "Epoch 269/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 85.5772 - mean_squared_error: 85.5772\n",
      "Epoch 270/5000\n",
      "506/506 [==============================] - 0s 80us/step - loss: 85.7229 - mean_squared_error: 85.7229\n",
      "Epoch 271/5000\n",
      "506/506 [==============================] - 0s 73us/step - loss: 85.2724 - mean_squared_error: 85.2724\n",
      "Epoch 272/5000\n",
      "506/506 [==============================] - 0s 54us/step - loss: 85.2410 - mean_squared_error: 85.2410\n",
      "Epoch 273/5000\n",
      "506/506 [==============================] - 0s 64us/step - loss: 85.0061 - mean_squared_error: 85.0061\n",
      "Epoch 274/5000\n",
      "506/506 [==============================] - 0s 76us/step - loss: 85.4766 - mean_squared_error: 85.4766\n",
      "Epoch 275/5000\n",
      "506/506 [==============================] - 0s 63us/step - loss: 85.2656 - mean_squared_error: 85.2656\n",
      "Epoch 276/5000\n",
      "506/506 [==============================] - 0s 53us/step - loss: 85.1145 - mean_squared_error: 85.1145\n",
      "Epoch 277/5000\n",
      "506/506 [==============================] - 0s 64us/step - loss: 85.5018 - mean_squared_error: 85.5018\n",
      "Epoch 278/5000\n",
      "506/506 [==============================] - 0s 67us/step - loss: 85.3132 - mean_squared_error: 85.3132\n",
      "Epoch 279/5000\n",
      "506/506 [==============================] - 0s 59us/step - loss: 85.3549 - mean_squared_error: 85.3549\n",
      "Epoch 280/5000\n",
      "506/506 [==============================] - 0s 52us/step - loss: 84.7881 - mean_squared_error: 84.7881\n",
      "Epoch 281/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 60us/step - loss: 84.9341 - mean_squared_error: 84.9341\n",
      "Epoch 282/5000\n",
      "506/506 [==============================] - 0s 64us/step - loss: 85.3827 - mean_squared_error: 85.3827\n",
      "Epoch 283/5000\n",
      "506/506 [==============================] - 0s 85us/step - loss: 85.2546 - mean_squared_error: 85.2546\n",
      "Epoch 284/5000\n",
      "506/506 [==============================] - 0s 71us/step - loss: 85.3267 - mean_squared_error: 85.3267\n",
      "Epoch 285/5000\n",
      "506/506 [==============================] - 0s 63us/step - loss: 84.6884 - mean_squared_error: 84.6884\n",
      "Epoch 286/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 84.8015 - mean_squared_error: 84.8015\n",
      "Epoch 287/5000\n",
      "506/506 [==============================] - 0s 74us/step - loss: 85.5370 - mean_squared_error: 85.5370\n",
      "Epoch 288/5000\n",
      "506/506 [==============================] - 0s 49us/step - loss: 84.8321 - mean_squared_error: 84.8321\n",
      "Epoch 289/5000\n",
      "506/506 [==============================] - 0s 46us/step - loss: 84.6124 - mean_squared_error: 84.6124\n",
      "Epoch 290/5000\n",
      "506/506 [==============================] - 0s 51us/step - loss: 84.9399 - mean_squared_error: 84.9399\n",
      "Epoch 291/5000\n",
      "506/506 [==============================] - 0s 60us/step - loss: 85.1516 - mean_squared_error: 85.1516\n",
      "Epoch 292/5000\n",
      "506/506 [==============================] - 0s 62us/step - loss: 84.8563 - mean_squared_error: 84.8563\n",
      "Epoch 293/5000\n",
      "506/506 [==============================] - 0s 59us/step - loss: 85.1439 - mean_squared_error: 85.1439\n",
      "Epoch 294/5000\n",
      "506/506 [==============================] - 0s 50us/step - loss: 85.0365 - mean_squared_error: 85.0365\n",
      "Epoch 295/5000\n",
      "506/506 [==============================] - 0s 50us/step - loss: 84.8047 - mean_squared_error: 84.8047\n",
      "Epoch 296/5000\n",
      "506/506 [==============================] - 0s 44us/step - loss: 85.6976 - mean_squared_error: 85.6976\n",
      "Epoch 297/5000\n",
      "506/506 [==============================] - 0s 71us/step - loss: 85.2605 - mean_squared_error: 85.2605\n",
      "Epoch 298/5000\n",
      "506/506 [==============================] - 0s 49us/step - loss: 85.0323 - mean_squared_error: 85.0323\n",
      "Epoch 299/5000\n",
      "506/506 [==============================] - 0s 46us/step - loss: 84.9834 - mean_squared_error: 84.9834\n",
      "Epoch 300/5000\n",
      "506/506 [==============================] - 0s 63us/step - loss: 85.1969 - mean_squared_error: 85.1969\n",
      "Epoch 301/5000\n",
      "506/506 [==============================] - 0s 82us/step - loss: 85.3409 - mean_squared_error: 85.3409\n",
      "Epoch 302/5000\n",
      "506/506 [==============================] - 0s 67us/step - loss: 85.2416 - mean_squared_error: 85.2416\n",
      "Epoch 303/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 85.0855 - mean_squared_error: 85.0855\n",
      "Epoch 304/5000\n",
      "506/506 [==============================] - 0s 55us/step - loss: 85.0274 - mean_squared_error: 85.0274\n",
      "Epoch 305/5000\n",
      "506/506 [==============================] - 0s 55us/step - loss: 84.9841 - mean_squared_error: 84.9841\n",
      "Epoch 306/5000\n",
      "506/506 [==============================] - 0s 62us/step - loss: 85.1165 - mean_squared_error: 85.1165\n",
      "Epoch 307/5000\n",
      "506/506 [==============================] - 0s 50us/step - loss: 85.0899 - mean_squared_error: 85.0899\n",
      "Epoch 308/5000\n",
      "506/506 [==============================] - 0s 46us/step - loss: 85.6860 - mean_squared_error: 85.6860\n",
      "Epoch 309/5000\n",
      "506/506 [==============================] - 0s 51us/step - loss: 85.3722 - mean_squared_error: 85.3722\n",
      "Epoch 310/5000\n",
      "506/506 [==============================] - 0s 56us/step - loss: 85.0859 - mean_squared_error: 85.0859\n",
      "Epoch 311/5000\n",
      "506/506 [==============================] - 0s 59us/step - loss: 85.0149 - mean_squared_error: 85.0149\n",
      "Epoch 312/5000\n",
      "506/506 [==============================] - 0s 54us/step - loss: 85.5866 - mean_squared_error: 85.5866\n",
      "Epoch 313/5000\n",
      "506/506 [==============================] - 0s 51us/step - loss: 84.6160 - mean_squared_error: 84.6160\n",
      "Epoch 314/5000\n",
      "506/506 [==============================] - 0s 57us/step - loss: 85.5699 - mean_squared_error: 85.5699\n",
      "Epoch 315/5000\n",
      "506/506 [==============================] - 0s 64us/step - loss: 84.8948 - mean_squared_error: 84.8948\n",
      "Epoch 316/5000\n",
      "506/506 [==============================] - 0s 55us/step - loss: 85.1080 - mean_squared_error: 85.1080\n",
      "Epoch 317/5000\n",
      "506/506 [==============================] - 0s 46us/step - loss: 85.0768 - mean_squared_error: 85.0768\n",
      "Epoch 318/5000\n",
      "506/506 [==============================] - 0s 55us/step - loss: 84.9349 - mean_squared_error: 84.9349\n",
      "Epoch 319/5000\n",
      "506/506 [==============================] - 0s 58us/step - loss: 84.7351 - mean_squared_error: 84.7351\n",
      "Epoch 320/5000\n",
      "506/506 [==============================] - 0s 68us/step - loss: 84.8769 - mean_squared_error: 84.8769\n",
      "Epoch 321/5000\n",
      "506/506 [==============================] - 0s 57us/step - loss: 84.6134 - mean_squared_error: 84.6134\n",
      "Epoch 322/5000\n",
      "506/506 [==============================] - 0s 48us/step - loss: 85.5191 - mean_squared_error: 85.5191\n",
      "Epoch 323/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 84.8455 - mean_squared_error: 84.8455\n",
      "Epoch 324/5000\n",
      "506/506 [==============================] - 0s 57us/step - loss: 84.7218 - mean_squared_error: 84.7218\n",
      "Epoch 325/5000\n",
      "506/506 [==============================] - 0s 52us/step - loss: 85.4436 - mean_squared_error: 85.4436\n",
      "Epoch 326/5000\n",
      "506/506 [==============================] - 0s 47us/step - loss: 84.9416 - mean_squared_error: 84.9416\n",
      "Epoch 327/5000\n",
      "506/506 [==============================] - 0s 49us/step - loss: 85.1525 - mean_squared_error: 85.1525\n",
      "Epoch 328/5000\n",
      "506/506 [==============================] - 0s 34us/step - loss: 84.6063 - mean_squared_error: 84.6063\n",
      "Epoch 329/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 84.4591 - mean_squared_error: 84.4591\n",
      "Epoch 330/5000\n",
      "506/506 [==============================] - 0s 40us/step - loss: 84.3519 - mean_squared_error: 84.3519\n",
      "Epoch 331/5000\n",
      "506/506 [==============================] - 0s 36us/step - loss: 85.1905 - mean_squared_error: 85.1905\n",
      "Epoch 332/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 85.9025 - mean_squared_error: 85.9025\n",
      "Epoch 333/5000\n",
      "506/506 [==============================] - 0s 63us/step - loss: 85.1852 - mean_squared_error: 85.1852\n",
      "Epoch 334/5000\n",
      "506/506 [==============================] - 0s 58us/step - loss: 85.1202 - mean_squared_error: 85.1202\n",
      "Epoch 335/5000\n",
      "506/506 [==============================] - 0s 51us/step - loss: 85.1678 - mean_squared_error: 85.1678\n",
      "Epoch 336/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 85.4030 - mean_squared_error: 85.4030\n",
      "Epoch 337/5000\n",
      "506/506 [==============================] - 0s 78us/step - loss: 85.0505 - mean_squared_error: 85.0505\n",
      "Epoch 338/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 85.3045 - mean_squared_error: 85.3045\n",
      "Epoch 339/5000\n",
      "506/506 [==============================] - 0s 55us/step - loss: 85.0607 - mean_squared_error: 85.0607\n",
      "Epoch 340/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 85.2910 - mean_squared_error: 85.2910\n",
      "Epoch 341/5000\n",
      "506/506 [==============================] - 0s 64us/step - loss: 85.0728 - mean_squared_error: 85.0728\n",
      "Epoch 342/5000\n",
      "506/506 [==============================] - 0s 74us/step - loss: 84.8269 - mean_squared_error: 84.8269\n",
      "Epoch 343/5000\n",
      "506/506 [==============================] - 0s 57us/step - loss: 84.5151 - mean_squared_error: 84.5151\n",
      "Epoch 344/5000\n",
      "506/506 [==============================] - 0s 52us/step - loss: 84.9824 - mean_squared_error: 84.9824\n",
      "Epoch 345/5000\n",
      "506/506 [==============================] - 0s 59us/step - loss: 84.2161 - mean_squared_error: 84.2161\n",
      "Epoch 346/5000\n",
      "506/506 [==============================] - 0s 68us/step - loss: 85.0528 - mean_squared_error: 85.0528\n",
      "Epoch 347/5000\n",
      "506/506 [==============================] - 0s 59us/step - loss: 84.6075 - mean_squared_error: 84.6075\n",
      "Epoch 348/5000\n",
      "506/506 [==============================] - 0s 59us/step - loss: 85.3505 - mean_squared_error: 85.3505\n",
      "Epoch 349/5000\n",
      "506/506 [==============================] - 0s 63us/step - loss: 84.8880 - mean_squared_error: 84.8880\n",
      "Epoch 350/5000\n",
      "506/506 [==============================] - 0s 76us/step - loss: 85.2261 - mean_squared_error: 85.2261\n",
      "Epoch 351/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 85.3702 - mean_squared_error: 85.3702\n",
      "Epoch 352/5000\n",
      "506/506 [==============================] - 0s 49us/step - loss: 85.0057 - mean_squared_error: 85.0057\n",
      "Epoch 353/5000\n",
      "506/506 [==============================] - 0s 54us/step - loss: 85.0804 - mean_squared_error: 85.0804\n",
      "Epoch 354/5000\n",
      "506/506 [==============================] - 0s 59us/step - loss: 84.9107 - mean_squared_error: 84.9107\n",
      "Epoch 355/5000\n",
      "506/506 [==============================] - 0s 54us/step - loss: 85.5194 - mean_squared_error: 85.5194\n",
      "Epoch 356/5000\n",
      "506/506 [==============================] - 0s 46us/step - loss: 85.1383 - mean_squared_error: 85.1383\n",
      "Epoch 357/5000\n",
      "506/506 [==============================] - 0s 52us/step - loss: 85.1880 - mean_squared_error: 85.1880\n",
      "Epoch 358/5000\n",
      "506/506 [==============================] - 0s 70us/step - loss: 85.0883 - mean_squared_error: 85.0883\n",
      "Epoch 359/5000\n",
      "506/506 [==============================] - 0s 51us/step - loss: 85.6324 - mean_squared_error: 85.6324\n",
      "Epoch 360/5000\n",
      "506/506 [==============================] - 0s 59us/step - loss: 84.7612 - mean_squared_error: 84.7612\n",
      "Epoch 361/5000\n",
      "506/506 [==============================] - 0s 54us/step - loss: 84.6326 - mean_squared_error: 84.6326\n",
      "Epoch 362/5000\n",
      "506/506 [==============================] - 0s 55us/step - loss: 84.9346 - mean_squared_error: 84.9346\n",
      "Epoch 363/5000\n",
      "506/506 [==============================] - 0s 65us/step - loss: 85.1272 - mean_squared_error: 85.1272\n",
      "Epoch 364/5000\n",
      "506/506 [==============================] - 0s 57us/step - loss: 85.4425 - mean_squared_error: 85.4425\n",
      "Epoch 365/5000\n",
      "506/506 [==============================] - 0s 64us/step - loss: 84.9985 - mean_squared_error: 84.9985\n",
      "Epoch 366/5000\n",
      "506/506 [==============================] - 0s 47us/step - loss: 84.9196 - mean_squared_error: 84.9196\n",
      "Epoch 367/5000\n",
      "506/506 [==============================] - 0s 54us/step - loss: 85.4997 - mean_squared_error: 85.4997\n",
      "Epoch 368/5000\n",
      "506/506 [==============================] - 0s 59us/step - loss: 85.0356 - mean_squared_error: 85.0356\n",
      "Epoch 369/5000\n",
      "506/506 [==============================] - 0s 55us/step - loss: 85.4109 - mean_squared_error: 85.4109\n",
      "Epoch 370/5000\n",
      "506/506 [==============================] - 0s 54us/step - loss: 84.8147 - mean_squared_error: 84.8147\n",
      "Epoch 371/5000\n",
      "506/506 [==============================] - 0s 50us/step - loss: 85.5061 - mean_squared_error: 85.5061\n",
      "Epoch 372/5000\n",
      "506/506 [==============================] - 0s 70us/step - loss: 85.3744 - mean_squared_error: 85.3744\n",
      "Epoch 373/5000\n",
      "506/506 [==============================] - 0s 49us/step - loss: 85.2461 - mean_squared_error: 85.2461\n",
      "Epoch 374/5000\n",
      "506/506 [==============================] - 0s 62us/step - loss: 85.9414 - mean_squared_error: 85.9414\n",
      "Epoch 375/5000\n",
      "506/506 [==============================] - 0s 51us/step - loss: 84.9860 - mean_squared_error: 84.9860\n",
      "Epoch 376/5000\n",
      "506/506 [==============================] - 0s 68us/step - loss: 85.5856 - mean_squared_error: 85.5856\n",
      "Epoch 377/5000\n",
      "506/506 [==============================] - 0s 62us/step - loss: 85.2628 - mean_squared_error: 85.2628\n",
      "Epoch 378/5000\n",
      "506/506 [==============================] - 0s 45us/step - loss: 84.7506 - mean_squared_error: 84.7506\n",
      "Epoch 379/5000\n",
      "506/506 [==============================] - 0s 45us/step - loss: 85.2636 - mean_squared_error: 85.2636\n",
      "Epoch 380/5000\n",
      "506/506 [==============================] - 0s 62us/step - loss: 85.0772 - mean_squared_error: 85.0772\n",
      "Epoch 381/5000\n",
      "506/506 [==============================] - 0s 43us/step - loss: 84.9477 - mean_squared_error: 84.9477\n",
      "Epoch 382/5000\n",
      "506/506 [==============================] - 0s 47us/step - loss: 83.8538 - mean_squared_error: 83.8538\n",
      "Epoch 383/5000\n",
      "506/506 [==============================] - 0s 58us/step - loss: 85.6319 - mean_squared_error: 85.6319\n",
      "Epoch 384/5000\n",
      "506/506 [==============================] - 0s 60us/step - loss: 84.6929 - mean_squared_error: 84.6929\n",
      "Epoch 385/5000\n",
      "506/506 [==============================] - 0s 60us/step - loss: 84.8580 - mean_squared_error: 84.8580\n",
      "Epoch 386/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 85.0482 - mean_squared_error: 85.0482\n",
      "Epoch 387/5000\n",
      "506/506 [==============================] - 0s 50us/step - loss: 85.4390 - mean_squared_error: 85.4390\n",
      "Epoch 388/5000\n",
      "506/506 [==============================] - 0s 39us/step - loss: 85.2515 - mean_squared_error: 85.2515\n",
      "Epoch 389/5000\n",
      "506/506 [==============================] - 0s 58us/step - loss: 84.9048 - mean_squared_error: 84.9048\n",
      "Epoch 390/5000\n",
      "506/506 [==============================] - 0s 35us/step - loss: 85.4900 - mean_squared_error: 85.4900\n",
      "Epoch 391/5000\n",
      "506/506 [==============================] - 0s 37us/step - loss: 85.1968 - mean_squared_error: 85.1968\n",
      "Epoch 392/5000\n",
      "506/506 [==============================] - 0s 45us/step - loss: 84.9025 - mean_squared_error: 84.9025\n",
      "Epoch 393/5000\n",
      "506/506 [==============================] - 0s 29us/step - loss: 85.4559 - mean_squared_error: 85.4559\n",
      "Epoch 394/5000\n",
      "506/506 [==============================] - 0s 58us/step - loss: 84.8163 - mean_squared_error: 84.8163\n",
      "Epoch 395/5000\n",
      "506/506 [==============================] - 0s 48us/step - loss: 84.9306 - mean_squared_error: 84.9306\n",
      "Epoch 396/5000\n",
      "506/506 [==============================] - 0s 46us/step - loss: 84.9987 - mean_squared_error: 84.9987\n",
      "Epoch 397/5000\n",
      "506/506 [==============================] - 0s 45us/step - loss: 84.7444 - mean_squared_error: 84.7444\n",
      "Epoch 398/5000\n",
      "506/506 [==============================] - 0s 43us/step - loss: 84.8669 - mean_squared_error: 84.8669\n",
      "Epoch 399/5000\n",
      "506/506 [==============================] - 0s 53us/step - loss: 85.0853 - mean_squared_error: 85.0853\n",
      "Epoch 400/5000\n",
      "506/506 [==============================] - 0s 43us/step - loss: 85.1125 - mean_squared_error: 85.1125\n",
      "Epoch 401/5000\n",
      "506/506 [==============================] - 0s 66us/step - loss: 85.0603 - mean_squared_error: 85.0603\n",
      "Epoch 402/5000\n",
      "506/506 [==============================] - 0s 78us/step - loss: 84.7569 - mean_squared_error: 84.7569\n",
      "Epoch 403/5000\n",
      "506/506 [==============================] - 0s 60us/step - loss: 85.2764 - mean_squared_error: 85.2764\n",
      "Epoch 404/5000\n",
      "506/506 [==============================] - 0s 43us/step - loss: 84.6954 - mean_squared_error: 84.6954\n",
      "Epoch 405/5000\n",
      "506/506 [==============================] - 0s 46us/step - loss: 84.7624 - mean_squared_error: 84.7624\n",
      "Epoch 406/5000\n",
      "506/506 [==============================] - 0s 51us/step - loss: 85.3811 - mean_squared_error: 85.3811\n",
      "Epoch 407/5000\n",
      "506/506 [==============================] - 0s 73us/step - loss: 85.0101 - mean_squared_error: 85.0101\n",
      "Epoch 408/5000\n",
      "506/506 [==============================] - 0s 63us/step - loss: 84.7591 - mean_squared_error: 84.7591\n",
      "Epoch 409/5000\n",
      "506/506 [==============================] - 0s 58us/step - loss: 85.2854 - mean_squared_error: 85.2854\n",
      "Epoch 410/5000\n",
      "506/506 [==============================] - 0s 57us/step - loss: 84.8419 - mean_squared_error: 84.8419\n",
      "Epoch 411/5000\n",
      "506/506 [==============================] - 0s 69us/step - loss: 85.1085 - mean_squared_error: 85.1085\n",
      "Epoch 412/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 85.0138 - mean_squared_error: 85.0138\n",
      "Epoch 413/5000\n",
      "506/506 [==============================] - 0s 70us/step - loss: 84.7308 - mean_squared_error: 84.7308\n",
      "Epoch 414/5000\n",
      "506/506 [==============================] - 0s 70us/step - loss: 85.1141 - mean_squared_error: 85.1141\n",
      "Epoch 415/5000\n",
      "506/506 [==============================] - 0s 59us/step - loss: 84.8894 - mean_squared_error: 84.8894\n",
      "Epoch 416/5000\n",
      "506/506 [==============================] - 0s 56us/step - loss: 85.0264 - mean_squared_error: 85.0264\n",
      "Epoch 417/5000\n",
      "506/506 [==============================] - 0s 66us/step - loss: 85.0151 - mean_squared_error: 85.0151\n",
      "Epoch 418/5000\n",
      "506/506 [==============================] - 0s 65us/step - loss: 85.1810 - mean_squared_error: 85.1810\n",
      "Epoch 419/5000\n",
      "506/506 [==============================] - 0s 43us/step - loss: 85.1758 - mean_squared_error: 85.1758\n",
      "Epoch 420/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 85.0854 - mean_squared_error: 85.0854\n",
      "Epoch 421/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 55us/step - loss: 85.0753 - mean_squared_error: 85.0753\n",
      "Epoch 422/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 84.9265 - mean_squared_error: 84.9265\n",
      "Epoch 423/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 84.6310 - mean_squared_error: 84.6310\n",
      "Epoch 424/5000\n",
      "506/506 [==============================] - 0s 47us/step - loss: 85.5256 - mean_squared_error: 85.5256\n",
      "Epoch 425/5000\n",
      "506/506 [==============================] - 0s 67us/step - loss: 85.1650 - mean_squared_error: 85.1650\n",
      "Epoch 426/5000\n",
      "506/506 [==============================] - 0s 70us/step - loss: 84.7339 - mean_squared_error: 84.7339\n",
      "Epoch 427/5000\n",
      "506/506 [==============================] - 0s 53us/step - loss: 85.3687 - mean_squared_error: 85.3687\n",
      "Epoch 428/5000\n",
      "506/506 [==============================] - 0s 51us/step - loss: 85.2038 - mean_squared_error: 85.2038\n",
      "Epoch 429/5000\n",
      "506/506 [==============================] - 0s 59us/step - loss: 85.2370 - mean_squared_error: 85.2370\n",
      "Epoch 430/5000\n",
      "506/506 [==============================] - 0s 67us/step - loss: 85.1194 - mean_squared_error: 85.1194\n",
      "Epoch 431/5000\n",
      "506/506 [==============================] - 0s 63us/step - loss: 85.0839 - mean_squared_error: 85.0839\n",
      "Epoch 432/5000\n",
      "506/506 [==============================] - 0s 63us/step - loss: 85.0744 - mean_squared_error: 85.0744\n",
      "Epoch 433/5000\n",
      "506/506 [==============================] - 0s 43us/step - loss: 84.9452 - mean_squared_error: 84.9452\n",
      "Epoch 434/5000\n",
      "506/506 [==============================] - 0s 46us/step - loss: 84.7722 - mean_squared_error: 84.7722\n",
      "Epoch 435/5000\n",
      "506/506 [==============================] - 0s 57us/step - loss: 84.3526 - mean_squared_error: 84.3526\n",
      "Epoch 436/5000\n",
      "506/506 [==============================] - 0s 62us/step - loss: 84.8934 - mean_squared_error: 84.8934\n",
      "Epoch 437/5000\n",
      "506/506 [==============================] - 0s 53us/step - loss: 85.1771 - mean_squared_error: 85.1771\n",
      "Epoch 438/5000\n",
      "506/506 [==============================] - 0s 56us/step - loss: 85.8505 - mean_squared_error: 85.8505\n",
      "Epoch 439/5000\n",
      "506/506 [==============================] - 0s 55us/step - loss: 85.2312 - mean_squared_error: 85.2312\n",
      "Epoch 440/5000\n",
      "506/506 [==============================] - 0s 78us/step - loss: 84.7058 - mean_squared_error: 84.7058\n",
      "Epoch 441/5000\n",
      "506/506 [==============================] - 0s 62us/step - loss: 84.8922 - mean_squared_error: 84.8922\n",
      "Epoch 442/5000\n",
      "506/506 [==============================] - 0s 63us/step - loss: 85.2877 - mean_squared_error: 85.2877\n",
      "Epoch 443/5000\n",
      "506/506 [==============================] - 0s 68us/step - loss: 85.3687 - mean_squared_error: 85.3687\n",
      "Epoch 444/5000\n",
      "506/506 [==============================] - 0s 66us/step - loss: 84.4699 - mean_squared_error: 84.4699\n",
      "Epoch 445/5000\n",
      "506/506 [==============================] - 0s 73us/step - loss: 84.8794 - mean_squared_error: 84.8794\n",
      "Epoch 446/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 85.9447 - mean_squared_error: 85.9447\n",
      "Epoch 447/5000\n",
      "506/506 [==============================] - 0s 48us/step - loss: 85.1875 - mean_squared_error: 85.1875\n",
      "Epoch 448/5000\n",
      "506/506 [==============================] - 0s 62us/step - loss: 85.2198 - mean_squared_error: 85.2198\n",
      "Epoch 449/5000\n",
      "506/506 [==============================] - 0s 63us/step - loss: 85.1721 - mean_squared_error: 85.1721\n",
      "Epoch 450/5000\n",
      "506/506 [==============================] - 0s 77us/step - loss: 84.9713 - mean_squared_error: 84.9713\n",
      "Epoch 451/5000\n",
      "506/506 [==============================] - 0s 64us/step - loss: 84.9216 - mean_squared_error: 84.9216\n",
      "Epoch 452/5000\n",
      "506/506 [==============================] - 0s 51us/step - loss: 85.2792 - mean_squared_error: 85.2792\n",
      "Epoch 453/5000\n",
      "506/506 [==============================] - 0s 39us/step - loss: 85.1854 - mean_squared_error: 85.1854\n",
      "Epoch 454/5000\n",
      "506/506 [==============================] - 0s 52us/step - loss: 84.8535 - mean_squared_error: 84.8535\n",
      "Epoch 455/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 85.2543 - mean_squared_error: 85.2543\n",
      "Epoch 456/5000\n",
      "506/506 [==============================] - 0s 56us/step - loss: 84.8198 - mean_squared_error: 84.8198\n",
      "Epoch 457/5000\n",
      "506/506 [==============================] - 0s 41us/step - loss: 85.1160 - mean_squared_error: 85.1160\n",
      "Epoch 458/5000\n",
      "506/506 [==============================] - 0s 36us/step - loss: 84.8453 - mean_squared_error: 84.8453\n",
      "Epoch 459/5000\n",
      "506/506 [==============================] - 0s 42us/step - loss: 85.0634 - mean_squared_error: 85.0634\n",
      "Epoch 460/5000\n",
      "506/506 [==============================] - 0s 37us/step - loss: 85.0207 - mean_squared_error: 85.0207\n",
      "Epoch 461/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 85.3412 - mean_squared_error: 85.3412\n",
      "Epoch 462/5000\n",
      "506/506 [==============================] - 0s 44us/step - loss: 85.1133 - mean_squared_error: 85.1133\n",
      "Epoch 463/5000\n",
      "506/506 [==============================] - 0s 56us/step - loss: 85.1258 - mean_squared_error: 85.1258\n",
      "Epoch 464/5000\n",
      "506/506 [==============================] - 0s 69us/step - loss: 85.2434 - mean_squared_error: 85.2434\n",
      "Epoch 465/5000\n",
      "506/506 [==============================] - 0s 71us/step - loss: 85.0385 - mean_squared_error: 85.0385\n",
      "Epoch 466/5000\n",
      "506/506 [==============================] - 0s 73us/step - loss: 84.8871 - mean_squared_error: 84.8871\n",
      "Epoch 467/5000\n",
      "506/506 [==============================] - 0s 65us/step - loss: 84.7723 - mean_squared_error: 84.7723\n",
      "Epoch 468/5000\n",
      "506/506 [==============================] - 0s 52us/step - loss: 84.8237 - mean_squared_error: 84.8237\n",
      "Epoch 469/5000\n",
      "506/506 [==============================] - 0s 55us/step - loss: 85.0156 - mean_squared_error: 85.0156\n",
      "Epoch 470/5000\n",
      "506/506 [==============================] - 0s 58us/step - loss: 85.3914 - mean_squared_error: 85.3914\n",
      "Epoch 471/5000\n",
      "506/506 [==============================] - 0s 49us/step - loss: 85.0241 - mean_squared_error: 85.0241\n",
      "Epoch 472/5000\n",
      "506/506 [==============================] - 0s 56us/step - loss: 85.3331 - mean_squared_error: 85.3331\n",
      "Epoch 473/5000\n",
      "506/506 [==============================] - 0s 48us/step - loss: 84.9645 - mean_squared_error: 84.9645\n",
      "Epoch 474/5000\n",
      "506/506 [==============================] - 0s 51us/step - loss: 84.9994 - mean_squared_error: 84.9994\n",
      "Epoch 475/5000\n",
      "506/506 [==============================] - 0s 45us/step - loss: 85.1748 - mean_squared_error: 85.1748\n",
      "Epoch 476/5000\n",
      "506/506 [==============================] - 0s 55us/step - loss: 85.2521 - mean_squared_error: 85.2521\n",
      "Epoch 477/5000\n",
      "506/506 [==============================] - 0s 68us/step - loss: 84.7670 - mean_squared_error: 84.7670\n",
      "Epoch 478/5000\n",
      "506/506 [==============================] - 0s 65us/step - loss: 85.1218 - mean_squared_error: 85.1218\n",
      "Epoch 479/5000\n",
      "506/506 [==============================] - 0s 56us/step - loss: 84.6921 - mean_squared_error: 84.6921\n",
      "Epoch 480/5000\n",
      "506/506 [==============================] - 0s 55us/step - loss: 84.8119 - mean_squared_error: 84.8119\n",
      "Epoch 481/5000\n",
      "506/506 [==============================] - 0s 49us/step - loss: 85.0991 - mean_squared_error: 85.0991\n",
      "Epoch 482/5000\n",
      "506/506 [==============================] - 0s 59us/step - loss: 85.1323 - mean_squared_error: 85.1323\n",
      "Epoch 483/5000\n",
      "506/506 [==============================] - 0s 40us/step - loss: 84.8712 - mean_squared_error: 84.8712\n",
      "Epoch 484/5000\n",
      "506/506 [==============================] - 0s 43us/step - loss: 85.0935 - mean_squared_error: 85.0935\n",
      "Epoch 485/5000\n",
      "506/506 [==============================] - 0s 45us/step - loss: 84.9640 - mean_squared_error: 84.9640\n",
      "Epoch 486/5000\n",
      "506/506 [==============================] - 0s 51us/step - loss: 84.7345 - mean_squared_error: 84.7345\n",
      "Epoch 487/5000\n",
      "506/506 [==============================] - 0s 69us/step - loss: 84.9644 - mean_squared_error: 84.9644\n",
      "Epoch 488/5000\n",
      "506/506 [==============================] - 0s 58us/step - loss: 84.7320 - mean_squared_error: 84.7320\n",
      "Epoch 489/5000\n",
      "506/506 [==============================] - 0s 54us/step - loss: 84.8364 - mean_squared_error: 84.8364\n",
      "Epoch 490/5000\n",
      "506/506 [==============================] - 0s 62us/step - loss: 84.7497 - mean_squared_error: 84.7497\n",
      "Epoch 491/5000\n",
      "506/506 [==============================] - 0s 79us/step - loss: 85.2048 - mean_squared_error: 85.2048\n",
      "Epoch 492/5000\n",
      "506/506 [==============================] - 0s 58us/step - loss: 84.9330 - mean_squared_error: 84.9330\n",
      "Epoch 493/5000\n",
      "506/506 [==============================] - 0s 57us/step - loss: 85.4924 - mean_squared_error: 85.4924\n",
      "Epoch 494/5000\n",
      "506/506 [==============================] - 0s 64us/step - loss: 84.7943 - mean_squared_error: 84.7943\n",
      "Epoch 495/5000\n",
      "506/506 [==============================] - 0s 66us/step - loss: 85.0604 - mean_squared_error: 85.0604\n",
      "Epoch 496/5000\n",
      "506/506 [==============================] - 0s 66us/step - loss: 84.1923 - mean_squared_error: 84.1923\n",
      "Epoch 497/5000\n",
      "506/506 [==============================] - 0s 63us/step - loss: 85.1488 - mean_squared_error: 85.1488\n",
      "Epoch 498/5000\n",
      "506/506 [==============================] - 0s 59us/step - loss: 84.8337 - mean_squared_error: 84.8337\n",
      "Epoch 499/5000\n",
      "506/506 [==============================] - 0s 71us/step - loss: 85.2645 - mean_squared_error: 85.2645\n",
      "Epoch 500/5000\n",
      "506/506 [==============================] - 0s 65us/step - loss: 85.4409 - mean_squared_error: 85.4409\n",
      "Epoch 501/5000\n",
      "506/506 [==============================] - 0s 58us/step - loss: 84.9371 - mean_squared_error: 84.9371\n",
      "Epoch 502/5000\n",
      "506/506 [==============================] - 0s 62us/step - loss: 84.9547 - mean_squared_error: 84.9547\n",
      "Epoch 503/5000\n",
      "506/506 [==============================] - 0s 49us/step - loss: 85.0047 - mean_squared_error: 85.0047\n",
      "Epoch 504/5000\n",
      "506/506 [==============================] - 0s 72us/step - loss: 84.5735 - mean_squared_error: 84.5735\n",
      "Epoch 505/5000\n",
      "506/506 [==============================] - 0s 68us/step - loss: 84.8755 - mean_squared_error: 84.8755\n",
      "Epoch 506/5000\n",
      "506/506 [==============================] - 0s 68us/step - loss: 85.4329 - mean_squared_error: 85.4329\n",
      "Epoch 507/5000\n",
      "506/506 [==============================] - 0s 55us/step - loss: 84.7517 - mean_squared_error: 84.7517\n",
      "Epoch 508/5000\n",
      "506/506 [==============================] - 0s 68us/step - loss: 85.4258 - mean_squared_error: 85.4258\n",
      "Epoch 509/5000\n",
      "506/506 [==============================] - 0s 72us/step - loss: 85.5339 - mean_squared_error: 85.5339\n",
      "Epoch 510/5000\n",
      "506/506 [==============================] - 0s 65us/step - loss: 85.4883 - mean_squared_error: 85.4883\n",
      "Epoch 511/5000\n",
      "506/506 [==============================] - 0s 51us/step - loss: 84.8812 - mean_squared_error: 84.8812\n",
      "Epoch 512/5000\n",
      "506/506 [==============================] - 0s 44us/step - loss: 84.9922 - mean_squared_error: 84.9922\n",
      "Epoch 513/5000\n",
      "506/506 [==============================] - 0s 50us/step - loss: 84.7731 - mean_squared_error: 84.7731\n",
      "Epoch 514/5000\n",
      "506/506 [==============================] - 0s 54us/step - loss: 85.0561 - mean_squared_error: 85.0561\n",
      "Epoch 515/5000\n",
      "506/506 [==============================] - 0s 63us/step - loss: 84.9043 - mean_squared_error: 84.9043\n",
      "Epoch 516/5000\n",
      "506/506 [==============================] - 0s 45us/step - loss: 85.2201 - mean_squared_error: 85.2201\n",
      "Epoch 517/5000\n",
      "506/506 [==============================] - 0s 58us/step - loss: 84.7844 - mean_squared_error: 84.7844\n",
      "Epoch 518/5000\n",
      "506/506 [==============================] - 0s 51us/step - loss: 85.0681 - mean_squared_error: 85.0681\n",
      "Epoch 519/5000\n",
      "506/506 [==============================] - 0s 57us/step - loss: 84.1412 - mean_squared_error: 84.1412\n",
      "Epoch 520/5000\n",
      "506/506 [==============================] - 0s 58us/step - loss: 85.0075 - mean_squared_error: 85.0075\n",
      "Epoch 521/5000\n",
      "506/506 [==============================] - 0s 68us/step - loss: 84.7103 - mean_squared_error: 84.7103\n",
      "Epoch 522/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 85.2229 - mean_squared_error: 85.2229\n",
      "Epoch 523/5000\n",
      "506/506 [==============================] - 0s 59us/step - loss: 85.4983 - mean_squared_error: 85.4983\n",
      "Epoch 524/5000\n",
      "506/506 [==============================] - 0s 49us/step - loss: 85.3085 - mean_squared_error: 85.3085\n",
      "Epoch 525/5000\n",
      "506/506 [==============================] - 0s 58us/step - loss: 85.3862 - mean_squared_error: 85.3862\n",
      "Epoch 526/5000\n",
      "506/506 [==============================] - 0s 65us/step - loss: 84.6461 - mean_squared_error: 84.6461\n",
      "Epoch 527/5000\n",
      "506/506 [==============================] - 0s 64us/step - loss: 84.9784 - mean_squared_error: 84.9784\n",
      "Epoch 528/5000\n",
      "506/506 [==============================] - 0s 57us/step - loss: 84.7976 - mean_squared_error: 84.7976\n",
      "Epoch 529/5000\n",
      "506/506 [==============================] - 0s 60us/step - loss: 84.7559 - mean_squared_error: 84.7559\n",
      "Epoch 530/5000\n",
      "506/506 [==============================] - 0s 72us/step - loss: 84.9012 - mean_squared_error: 84.9012\n",
      "Epoch 531/5000\n",
      "506/506 [==============================] - 0s 64us/step - loss: 84.5389 - mean_squared_error: 84.5389\n",
      "Epoch 532/5000\n",
      "506/506 [==============================] - 0s 62us/step - loss: 85.2468 - mean_squared_error: 85.2468\n",
      "Epoch 533/5000\n",
      "506/506 [==============================] - 0s 54us/step - loss: 84.8611 - mean_squared_error: 84.8611\n",
      "Epoch 534/5000\n",
      "506/506 [==============================] - 0s 59us/step - loss: 85.1048 - mean_squared_error: 85.1048\n",
      "Epoch 535/5000\n",
      "506/506 [==============================] - 0s 62us/step - loss: 84.6319 - mean_squared_error: 84.6319\n",
      "Epoch 536/5000\n",
      "506/506 [==============================] - 0s 68us/step - loss: 85.1256 - mean_squared_error: 85.1256\n",
      "Epoch 537/5000\n",
      "506/506 [==============================] - 0s 71us/step - loss: 85.0434 - mean_squared_error: 85.0434\n",
      "Epoch 538/5000\n",
      "506/506 [==============================] - 0s 50us/step - loss: 84.9542 - mean_squared_error: 84.9542\n",
      "Epoch 539/5000\n",
      "506/506 [==============================] - 0s 46us/step - loss: 84.6363 - mean_squared_error: 84.6363\n",
      "Epoch 540/5000\n",
      "506/506 [==============================] - 0s 54us/step - loss: 85.5070 - mean_squared_error: 85.5070\n",
      "Epoch 541/5000\n",
      "506/506 [==============================] - 0s 67us/step - loss: 85.3407 - mean_squared_error: 85.3407\n",
      "Epoch 542/5000\n",
      "506/506 [==============================] - 0s 78us/step - loss: 84.6764 - mean_squared_error: 84.6764\n",
      "Epoch 543/5000\n",
      "506/506 [==============================] - 0s 59us/step - loss: 84.9557 - mean_squared_error: 84.9557\n",
      "Epoch 544/5000\n",
      "506/506 [==============================] - 0s 55us/step - loss: 85.3978 - mean_squared_error: 85.3978\n",
      "Epoch 545/5000\n",
      "506/506 [==============================] - 0s 49us/step - loss: 84.8902 - mean_squared_error: 84.8902\n",
      "Epoch 546/5000\n",
      "506/506 [==============================] - 0s 66us/step - loss: 84.9339 - mean_squared_error: 84.9339\n",
      "Epoch 547/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 85.6350 - mean_squared_error: 85.6350\n",
      "Epoch 548/5000\n",
      "506/506 [==============================] - 0s 56us/step - loss: 85.0126 - mean_squared_error: 85.0126\n",
      "Epoch 549/5000\n",
      "506/506 [==============================] - 0s 53us/step - loss: 84.9165 - mean_squared_error: 84.9165\n",
      "Epoch 550/5000\n",
      "506/506 [==============================] - 0s 66us/step - loss: 85.2469 - mean_squared_error: 85.2469\n",
      "Epoch 551/5000\n",
      "506/506 [==============================] - 0s 62us/step - loss: 85.3298 - mean_squared_error: 85.3298\n",
      "Epoch 552/5000\n",
      "506/506 [==============================] - 0s 51us/step - loss: 84.4393 - mean_squared_error: 84.4393\n",
      "Epoch 553/5000\n",
      "506/506 [==============================] - 0s 51us/step - loss: 84.9972 - mean_squared_error: 84.9972\n",
      "Epoch 554/5000\n",
      "506/506 [==============================] - 0s 50us/step - loss: 85.1924 - mean_squared_error: 85.1924\n",
      "Epoch 555/5000\n",
      "506/506 [==============================] - 0s 70us/step - loss: 84.8038 - mean_squared_error: 84.8038\n",
      "Epoch 556/5000\n",
      "506/506 [==============================] - 0s 50us/step - loss: 85.3118 - mean_squared_error: 85.3118\n",
      "Epoch 557/5000\n",
      "506/506 [==============================] - 0s 38us/step - loss: 84.5082 - mean_squared_error: 84.5082\n",
      "Epoch 558/5000\n",
      "506/506 [==============================] - 0s 54us/step - loss: 85.2740 - mean_squared_error: 85.2740\n",
      "Epoch 559/5000\n",
      "506/506 [==============================] - 0s 64us/step - loss: 84.6809 - mean_squared_error: 84.6809\n",
      "Epoch 560/5000\n",
      "506/506 [==============================] - 0s 60us/step - loss: 85.3762 - mean_squared_error: 85.3762\n",
      "Epoch 561/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 53us/step - loss: 84.6124 - mean_squared_error: 84.6124\n",
      "Epoch 562/5000\n",
      "506/506 [==============================] - 0s 58us/step - loss: 84.9155 - mean_squared_error: 84.9155\n",
      "Epoch 563/5000\n",
      "506/506 [==============================] - 0s 66us/step - loss: 83.9864 - mean_squared_error: 83.9864\n",
      "Epoch 564/5000\n",
      "506/506 [==============================] - 0s 58us/step - loss: 85.3675 - mean_squared_error: 85.3675\n",
      "Epoch 565/5000\n",
      "506/506 [==============================] - 0s 54us/step - loss: 85.5496 - mean_squared_error: 85.5496\n",
      "Epoch 566/5000\n",
      "506/506 [==============================] - 0s 48us/step - loss: 85.0878 - mean_squared_error: 85.0878\n",
      "Epoch 567/5000\n",
      "506/506 [==============================] - 0s 46us/step - loss: 84.9054 - mean_squared_error: 84.9054\n",
      "Epoch 568/5000\n",
      "506/506 [==============================] - 0s 65us/step - loss: 84.8235 - mean_squared_error: 84.8235\n",
      "Epoch 569/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 84.8601 - mean_squared_error: 84.8601\n",
      "Epoch 570/5000\n",
      "506/506 [==============================] - 0s 53us/step - loss: 85.2488 - mean_squared_error: 85.2488\n",
      "Epoch 571/5000\n",
      "506/506 [==============================] - 0s 56us/step - loss: 85.1858 - mean_squared_error: 85.1858\n",
      "Epoch 572/5000\n",
      "506/506 [==============================] - 0s 55us/step - loss: 85.7306 - mean_squared_error: 85.7306\n",
      "Epoch 573/5000\n",
      "506/506 [==============================] - 0s 71us/step - loss: 85.1365 - mean_squared_error: 85.1365\n",
      "Epoch 574/5000\n",
      "506/506 [==============================] - 0s 41us/step - loss: 85.1728 - mean_squared_error: 85.1728\n",
      "Epoch 575/5000\n",
      "506/506 [==============================] - 0s 46us/step - loss: 84.9805 - mean_squared_error: 84.9805\n",
      "Epoch 576/5000\n",
      "506/506 [==============================] - 0s 49us/step - loss: 85.0374 - mean_squared_error: 85.0374\n",
      "Epoch 577/5000\n",
      "506/506 [==============================] - 0s 70us/step - loss: 84.7582 - mean_squared_error: 84.7582\n",
      "Epoch 578/5000\n",
      "506/506 [==============================] - 0s 64us/step - loss: 85.5105 - mean_squared_error: 85.5105\n",
      "Epoch 579/5000\n",
      "506/506 [==============================] - 0s 54us/step - loss: 85.3064 - mean_squared_error: 85.3064\n",
      "Epoch 580/5000\n",
      "506/506 [==============================] - 0s 53us/step - loss: 84.9920 - mean_squared_error: 84.9920\n",
      "Epoch 581/5000\n",
      "506/506 [==============================] - 0s 47us/step - loss: 85.0318 - mean_squared_error: 85.0318\n",
      "Epoch 582/5000\n",
      "506/506 [==============================] - 0s 70us/step - loss: 84.9203 - mean_squared_error: 84.9203\n",
      "Epoch 583/5000\n",
      "506/506 [==============================] - 0s 46us/step - loss: 85.0566 - mean_squared_error: 85.0566\n",
      "Epoch 584/5000\n",
      "506/506 [==============================] - 0s 50us/step - loss: 85.3086 - mean_squared_error: 85.3086\n",
      "Epoch 585/5000\n",
      "506/506 [==============================] - 0s 52us/step - loss: 85.0530 - mean_squared_error: 85.0530\n",
      "Epoch 586/5000\n",
      "506/506 [==============================] - 0s 58us/step - loss: 84.4914 - mean_squared_error: 84.4914\n",
      "Epoch 587/5000\n",
      "506/506 [==============================] - 0s 41us/step - loss: 85.1921 - mean_squared_error: 85.1921\n",
      "Epoch 588/5000\n",
      "506/506 [==============================] - 0s 38us/step - loss: 84.9855 - mean_squared_error: 84.9855\n",
      "Epoch 589/5000\n",
      "506/506 [==============================] - 0s 53us/step - loss: 85.0786 - mean_squared_error: 85.0786\n",
      "Epoch 590/5000\n",
      "506/506 [==============================] - 0s 63us/step - loss: 85.0333 - mean_squared_error: 85.0333\n",
      "Epoch 591/5000\n",
      "506/506 [==============================] - 0s 38us/step - loss: 85.2291 - mean_squared_error: 85.2291\n",
      "Epoch 592/5000\n",
      "506/506 [==============================] - 0s 48us/step - loss: 85.0306 - mean_squared_error: 85.0306\n",
      "Epoch 593/5000\n",
      "506/506 [==============================] - 0s 49us/step - loss: 85.1346 - mean_squared_error: 85.1346\n",
      "Epoch 594/5000\n",
      "506/506 [==============================] - 0s 53us/step - loss: 85.0157 - mean_squared_error: 85.0157\n",
      "Epoch 595/5000\n",
      "506/506 [==============================] - 0s 64us/step - loss: 84.8854 - mean_squared_error: 84.8854\n",
      "Epoch 596/5000\n",
      "506/506 [==============================] - 0s 60us/step - loss: 84.9842 - mean_squared_error: 84.9842\n",
      "Epoch 597/5000\n",
      "506/506 [==============================] - 0s 60us/step - loss: 85.2347 - mean_squared_error: 85.2347\n",
      "Epoch 598/5000\n",
      "506/506 [==============================] - 0s 75us/step - loss: 84.6959 - mean_squared_error: 84.6959\n",
      "Epoch 599/5000\n",
      "506/506 [==============================] - 0s 63us/step - loss: 85.0931 - mean_squared_error: 85.0931\n",
      "Epoch 600/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 85.1203 - mean_squared_error: 85.1203\n",
      "Epoch 601/5000\n",
      "506/506 [==============================] - 0s 69us/step - loss: 84.9623 - mean_squared_error: 84.9623\n",
      "Epoch 602/5000\n",
      "506/506 [==============================] - 0s 69us/step - loss: 84.8528 - mean_squared_error: 84.8528\n",
      "Epoch 603/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 85.0210 - mean_squared_error: 85.0210\n",
      "Epoch 604/5000\n",
      "506/506 [==============================] - 0s 62us/step - loss: 85.4549 - mean_squared_error: 85.4549\n",
      "Epoch 605/5000\n",
      "506/506 [==============================] - 0s 65us/step - loss: 85.5828 - mean_squared_error: 85.5828\n",
      "Epoch 606/5000\n",
      "506/506 [==============================] - 0s 83us/step - loss: 85.1068 - mean_squared_error: 85.1068\n",
      "Epoch 607/5000\n",
      "506/506 [==============================] - 0s 69us/step - loss: 84.8468 - mean_squared_error: 84.8468\n",
      "Epoch 608/5000\n",
      "506/506 [==============================] - 0s 65us/step - loss: 85.3986 - mean_squared_error: 85.3986\n",
      "Epoch 609/5000\n",
      "506/506 [==============================] - 0s 55us/step - loss: 85.7554 - mean_squared_error: 85.7554\n",
      "Epoch 610/5000\n",
      "506/506 [==============================] - 0s 51us/step - loss: 85.1334 - mean_squared_error: 85.1334\n",
      "Epoch 611/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 85.4283 - mean_squared_error: 85.4283\n",
      "Epoch 612/5000\n",
      "506/506 [==============================] - 0s 50us/step - loss: 84.7941 - mean_squared_error: 84.7941\n",
      "Epoch 613/5000\n",
      "506/506 [==============================] - 0s 48us/step - loss: 85.0166 - mean_squared_error: 85.0166\n",
      "Epoch 614/5000\n",
      "506/506 [==============================] - 0s 55us/step - loss: 84.8350 - mean_squared_error: 84.8350\n",
      "Epoch 615/5000\n",
      "506/506 [==============================] - 0s 69us/step - loss: 85.3530 - mean_squared_error: 85.3530\n",
      "Epoch 616/5000\n",
      "506/506 [==============================] - 0s 64us/step - loss: 84.8732 - mean_squared_error: 84.8732\n",
      "Epoch 617/5000\n",
      "506/506 [==============================] - 0s 48us/step - loss: 85.5919 - mean_squared_error: 85.5919\n",
      "Epoch 618/5000\n",
      "506/506 [==============================] - 0s 58us/step - loss: 85.1020 - mean_squared_error: 85.1020\n",
      "Epoch 619/5000\n",
      "506/506 [==============================] - 0s 72us/step - loss: 85.0668 - mean_squared_error: 85.0668\n",
      "Epoch 620/5000\n",
      "506/506 [==============================] - 0s 68us/step - loss: 85.0001 - mean_squared_error: 85.0001\n",
      "Epoch 621/5000\n",
      "506/506 [==============================] - 0s 58us/step - loss: 84.9869 - mean_squared_error: 84.9869\n",
      "Epoch 622/5000\n",
      "506/506 [==============================] - 0s 60us/step - loss: 85.3448 - mean_squared_error: 85.3448\n",
      "Epoch 623/5000\n",
      "506/506 [==============================] - 0s 54us/step - loss: 84.7413 - mean_squared_error: 84.7413\n",
      "Epoch 624/5000\n",
      "506/506 [==============================] - 0s 77us/step - loss: 85.2553 - mean_squared_error: 85.2553\n",
      "Epoch 625/5000\n",
      "506/506 [==============================] - 0s 66us/step - loss: 84.8712 - mean_squared_error: 84.8712\n",
      "Epoch 626/5000\n",
      "506/506 [==============================] - 0s 60us/step - loss: 85.1742 - mean_squared_error: 85.1742\n",
      "Epoch 627/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 84.9293 - mean_squared_error: 84.9293\n",
      "Epoch 628/5000\n",
      "506/506 [==============================] - 0s 70us/step - loss: 84.8975 - mean_squared_error: 84.8975\n",
      "Epoch 629/5000\n",
      "506/506 [==============================] - 0s 59us/step - loss: 85.2516 - mean_squared_error: 85.2516\n",
      "Epoch 630/5000\n",
      "506/506 [==============================] - 0s 48us/step - loss: 84.4600 - mean_squared_error: 84.4600\n",
      "Epoch 631/5000\n",
      "506/506 [==============================] - 0s 57us/step - loss: 84.7186 - mean_squared_error: 84.7186\n",
      "Epoch 632/5000\n",
      "506/506 [==============================] - 0s 79us/step - loss: 85.0211 - mean_squared_error: 85.0211\n",
      "Epoch 633/5000\n",
      "506/506 [==============================] - 0s 52us/step - loss: 85.1120 - mean_squared_error: 85.1120\n",
      "Epoch 634/5000\n",
      "506/506 [==============================] - 0s 59us/step - loss: 85.1935 - mean_squared_error: 85.1935\n",
      "Epoch 635/5000\n",
      "506/506 [==============================] - 0s 51us/step - loss: 84.9476 - mean_squared_error: 84.9476\n",
      "Epoch 636/5000\n",
      "506/506 [==============================] - 0s 70us/step - loss: 84.9757 - mean_squared_error: 84.9757\n",
      "Epoch 637/5000\n",
      "506/506 [==============================] - 0s 50us/step - loss: 85.0822 - mean_squared_error: 85.0822\n",
      "Epoch 638/5000\n",
      "506/506 [==============================] - 0s 42us/step - loss: 84.7716 - mean_squared_error: 84.7716\n",
      "Epoch 639/5000\n",
      "506/506 [==============================] - 0s 48us/step - loss: 85.2566 - mean_squared_error: 85.2566\n",
      "Epoch 640/5000\n",
      "506/506 [==============================] - 0s 56us/step - loss: 85.5930 - mean_squared_error: 85.5930\n",
      "Epoch 641/5000\n",
      "506/506 [==============================] - 0s 54us/step - loss: 85.2887 - mean_squared_error: 85.2887\n",
      "Epoch 642/5000\n",
      "506/506 [==============================] - 0s 62us/step - loss: 84.7400 - mean_squared_error: 84.7400\n",
      "Epoch 643/5000\n",
      "506/506 [==============================] - 0s 47us/step - loss: 85.0331 - mean_squared_error: 85.0331\n",
      "Epoch 644/5000\n",
      "506/506 [==============================] - 0s 44us/step - loss: 84.8263 - mean_squared_error: 84.8263\n",
      "Epoch 645/5000\n",
      "506/506 [==============================] - 0s 60us/step - loss: 84.7995 - mean_squared_error: 84.7995\n",
      "Epoch 646/5000\n",
      "506/506 [==============================] - 0s 44us/step - loss: 84.7763 - mean_squared_error: 84.7763\n",
      "Epoch 647/5000\n",
      "506/506 [==============================] - 0s 38us/step - loss: 85.3794 - mean_squared_error: 85.3794\n",
      "Epoch 648/5000\n",
      "506/506 [==============================] - 0s 44us/step - loss: 85.1992 - mean_squared_error: 85.1992\n",
      "Epoch 649/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 84.8043 - mean_squared_error: 84.8043\n",
      "Epoch 650/5000\n",
      "506/506 [==============================] - 0s 43us/step - loss: 84.9649 - mean_squared_error: 84.9649\n",
      "Epoch 651/5000\n",
      "506/506 [==============================] - 0s 41us/step - loss: 85.1290 - mean_squared_error: 85.1290\n",
      "Epoch 652/5000\n",
      "506/506 [==============================] - 0s 36us/step - loss: 85.0797 - mean_squared_error: 85.0797\n",
      "Epoch 653/5000\n",
      "506/506 [==============================] - 0s 32us/step - loss: 84.9903 - mean_squared_error: 84.9903\n",
      "Epoch 654/5000\n",
      "506/506 [==============================] - 0s 62us/step - loss: 85.4107 - mean_squared_error: 85.4107\n",
      "Epoch 655/5000\n",
      "506/506 [==============================] - 0s 49us/step - loss: 84.2887 - mean_squared_error: 84.2887\n",
      "Epoch 656/5000\n",
      "506/506 [==============================] - 0s 46us/step - loss: 85.7565 - mean_squared_error: 85.7565\n",
      "Epoch 657/5000\n",
      "506/506 [==============================] - 0s 41us/step - loss: 85.0065 - mean_squared_error: 85.0065\n",
      "Epoch 658/5000\n",
      "506/506 [==============================] - 0s 43us/step - loss: 84.7313 - mean_squared_error: 84.7313\n",
      "Epoch 659/5000\n",
      "506/506 [==============================] - 0s 52us/step - loss: 84.8326 - mean_squared_error: 84.8326\n",
      "Epoch 660/5000\n",
      "506/506 [==============================] - 0s 64us/step - loss: 84.7851 - mean_squared_error: 84.7851\n",
      "Epoch 661/5000\n",
      "506/506 [==============================] - 0s 57us/step - loss: 84.4938 - mean_squared_error: 84.4938\n",
      "Epoch 662/5000\n",
      "506/506 [==============================] - 0s 72us/step - loss: 84.8061 - mean_squared_error: 84.8061\n",
      "Epoch 663/5000\n",
      "506/506 [==============================] - 0s 67us/step - loss: 85.1291 - mean_squared_error: 85.1291\n",
      "Epoch 664/5000\n",
      "506/506 [==============================] - 0s 58us/step - loss: 85.3228 - mean_squared_error: 85.3228\n",
      "Epoch 665/5000\n",
      "506/506 [==============================] - 0s 57us/step - loss: 84.6370 - mean_squared_error: 84.6370\n",
      "Epoch 666/5000\n",
      "506/506 [==============================] - 0s 56us/step - loss: 85.4276 - mean_squared_error: 85.4276\n",
      "Epoch 667/5000\n",
      "506/506 [==============================] - 0s 72us/step - loss: 84.7960 - mean_squared_error: 84.7960\n",
      "Epoch 668/5000\n",
      "506/506 [==============================] - 0s 67us/step - loss: 85.4543 - mean_squared_error: 85.4543\n",
      "Epoch 669/5000\n",
      "506/506 [==============================] - 0s 63us/step - loss: 85.1439 - mean_squared_error: 85.1439\n",
      "Epoch 670/5000\n",
      "506/506 [==============================] - 0s 62us/step - loss: 84.8679 - mean_squared_error: 84.8679\n",
      "Epoch 671/5000\n",
      "506/506 [==============================] - 0s 63us/step - loss: 84.2755 - mean_squared_error: 84.2755\n",
      "Epoch 672/5000\n",
      "506/506 [==============================] - 0s 77us/step - loss: 84.5012 - mean_squared_error: 84.5012\n",
      "Epoch 673/5000\n",
      "506/506 [==============================] - 0s 54us/step - loss: 85.5569 - mean_squared_error: 85.5569\n",
      "Epoch 674/5000\n",
      "506/506 [==============================] - 0s 54us/step - loss: 85.1736 - mean_squared_error: 85.1736\n",
      "Epoch 675/5000\n",
      "506/506 [==============================] - 0s 64us/step - loss: 85.8000 - mean_squared_error: 85.8000\n",
      "Epoch 676/5000\n",
      "506/506 [==============================] - 0s 66us/step - loss: 84.8245 - mean_squared_error: 84.8245\n",
      "Epoch 677/5000\n",
      "506/506 [==============================] - 0s 57us/step - loss: 85.4497 - mean_squared_error: 85.4497\n",
      "Epoch 678/5000\n",
      "506/506 [==============================] - 0s 48us/step - loss: 85.1074 - mean_squared_error: 85.1074\n",
      "Epoch 679/5000\n",
      "506/506 [==============================] - 0s 49us/step - loss: 85.7975 - mean_squared_error: 85.7975\n",
      "Epoch 680/5000\n",
      "506/506 [==============================] - 0s 67us/step - loss: 85.1969 - mean_squared_error: 85.1969\n",
      "Epoch 681/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 84.9737 - mean_squared_error: 84.9737\n",
      "Epoch 682/5000\n",
      "506/506 [==============================] - 0s 62us/step - loss: 84.2692 - mean_squared_error: 84.2692\n",
      "Epoch 683/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 85.4417 - mean_squared_error: 85.4417\n",
      "Epoch 684/5000\n",
      "506/506 [==============================] - 0s 57us/step - loss: 84.9669 - mean_squared_error: 84.9669\n",
      "Epoch 685/5000\n",
      "506/506 [==============================] - 0s 52us/step - loss: 85.0907 - mean_squared_error: 85.0907\n",
      "Epoch 686/5000\n",
      "506/506 [==============================] - 0s 55us/step - loss: 84.9119 - mean_squared_error: 84.9119\n",
      "Epoch 687/5000\n",
      "506/506 [==============================] - 0s 59us/step - loss: 85.4236 - mean_squared_error: 85.4236\n",
      "Epoch 688/5000\n",
      "506/506 [==============================] - 0s 72us/step - loss: 85.0960 - mean_squared_error: 85.0960\n",
      "Epoch 689/5000\n",
      "506/506 [==============================] - 0s 70us/step - loss: 85.6211 - mean_squared_error: 85.6211\n",
      "Epoch 690/5000\n",
      "506/506 [==============================] - 0s 57us/step - loss: 85.4630 - mean_squared_error: 85.4630\n",
      "Epoch 691/5000\n",
      "506/506 [==============================] - 0s 46us/step - loss: 85.0027 - mean_squared_error: 85.0027\n",
      "Epoch 692/5000\n",
      "506/506 [==============================] - 0s 58us/step - loss: 85.1503 - mean_squared_error: 85.1503\n",
      "Epoch 693/5000\n",
      "506/506 [==============================] - 0s 65us/step - loss: 84.9806 - mean_squared_error: 84.9806\n",
      "Epoch 694/5000\n",
      "506/506 [==============================] - 0s 60us/step - loss: 84.9074 - mean_squared_error: 84.9074\n",
      "Epoch 695/5000\n",
      "506/506 [==============================] - 0s 83us/step - loss: 84.1275 - mean_squared_error: 84.1275\n",
      "Epoch 696/5000\n",
      "506/506 [==============================] - 0s 62us/step - loss: 85.4892 - mean_squared_error: 85.4892\n",
      "Epoch 697/5000\n",
      "506/506 [==============================] - 0s 69us/step - loss: 84.8374 - mean_squared_error: 84.8374\n",
      "Epoch 698/5000\n",
      "506/506 [==============================] - 0s 55us/step - loss: 85.7229 - mean_squared_error: 85.7229\n",
      "Epoch 699/5000\n",
      "506/506 [==============================] - 0s 59us/step - loss: 83.9450 - mean_squared_error: 83.9450\n",
      "Epoch 700/5000\n",
      "506/506 [==============================] - 0s 64us/step - loss: 84.0931 - mean_squared_error: 84.0931\n",
      "Epoch 701/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 66us/step - loss: 85.0464 - mean_squared_error: 85.0464\n",
      "Epoch 702/5000\n",
      "506/506 [==============================] - 0s 75us/step - loss: 85.1333 - mean_squared_error: 85.1333\n",
      "Epoch 703/5000\n",
      "506/506 [==============================] - 0s 63us/step - loss: 84.9138 - mean_squared_error: 84.9138\n",
      "Epoch 704/5000\n",
      "506/506 [==============================] - 0s 55us/step - loss: 86.4281 - mean_squared_error: 86.4281\n",
      "Epoch 705/5000\n",
      "506/506 [==============================] - 0s 59us/step - loss: 85.3473 - mean_squared_error: 85.3473\n",
      "Epoch 706/5000\n",
      "506/506 [==============================] - 0s 66us/step - loss: 84.4789 - mean_squared_error: 84.4789\n",
      "Epoch 707/5000\n",
      "506/506 [==============================] - 0s 59us/step - loss: 85.0255 - mean_squared_error: 85.0255\n",
      "Epoch 708/5000\n",
      "506/506 [==============================] - 0s 73us/step - loss: 85.2653 - mean_squared_error: 85.2653\n",
      "Epoch 709/5000\n",
      "506/506 [==============================] - 0s 60us/step - loss: 84.7243 - mean_squared_error: 84.7243\n",
      "Epoch 710/5000\n",
      "506/506 [==============================] - 0s 56us/step - loss: 84.7536 - mean_squared_error: 84.7536\n",
      "Epoch 711/5000\n",
      "506/506 [==============================] - 0s 68us/step - loss: 85.0770 - mean_squared_error: 85.0770\n",
      "Epoch 712/5000\n",
      "506/506 [==============================] - 0s 65us/step - loss: 85.4868 - mean_squared_error: 85.4868\n",
      "Epoch 713/5000\n",
      "506/506 [==============================] - 0s 57us/step - loss: 84.7760 - mean_squared_error: 84.7760\n",
      "Epoch 714/5000\n",
      "506/506 [==============================] - 0s 51us/step - loss: 85.2507 - mean_squared_error: 85.2507\n",
      "Epoch 715/5000\n",
      "506/506 [==============================] - 0s 69us/step - loss: 84.8326 - mean_squared_error: 84.8326\n",
      "Epoch 716/5000\n",
      "506/506 [==============================] - 0s 41us/step - loss: 85.1067 - mean_squared_error: 85.1067\n",
      "Epoch 717/5000\n",
      "506/506 [==============================] - 0s 35us/step - loss: 85.1139 - mean_squared_error: 85.1139\n",
      "Epoch 718/5000\n",
      "506/506 [==============================] - 0s 47us/step - loss: 85.2626 - mean_squared_error: 85.2626\n",
      "Epoch 719/5000\n",
      "506/506 [==============================] - 0s 47us/step - loss: 84.4346 - mean_squared_error: 84.4346\n",
      "Epoch 720/5000\n",
      "506/506 [==============================] - 0s 40us/step - loss: 84.8405 - mean_squared_error: 84.8405\n",
      "Epoch 721/5000\n",
      "506/506 [==============================] - 0s 69us/step - loss: 84.9692 - mean_squared_error: 84.9692\n",
      "Epoch 722/5000\n",
      "506/506 [==============================] - 0s 58us/step - loss: 85.4911 - mean_squared_error: 85.4911\n",
      "Epoch 723/5000\n",
      "506/506 [==============================] - 0s 45us/step - loss: 84.8635 - mean_squared_error: 84.8635\n",
      "Epoch 724/5000\n",
      "506/506 [==============================] - 0s 66us/step - loss: 84.7287 - mean_squared_error: 84.7287\n",
      "Epoch 725/5000\n",
      "506/506 [==============================] - 0s 68us/step - loss: 85.1936 - mean_squared_error: 85.1936\n",
      "Epoch 726/5000\n",
      "506/506 [==============================] - 0s 64us/step - loss: 85.0986 - mean_squared_error: 85.0986\n",
      "Epoch 727/5000\n",
      "506/506 [==============================] - 0s 84us/step - loss: 84.7897 - mean_squared_error: 84.7897\n",
      "Epoch 728/5000\n",
      "506/506 [==============================] - 0s 63us/step - loss: 84.7557 - mean_squared_error: 84.7557\n",
      "Epoch 729/5000\n",
      "506/506 [==============================] - 0s 65us/step - loss: 84.8337 - mean_squared_error: 84.8337\n",
      "Epoch 730/5000\n",
      "506/506 [==============================] - 0s 65us/step - loss: 85.0637 - mean_squared_error: 85.0637\n",
      "Epoch 731/5000\n",
      "506/506 [==============================] - 0s 70us/step - loss: 85.1385 - mean_squared_error: 85.1385\n",
      "Epoch 732/5000\n",
      "506/506 [==============================] - 0s 67us/step - loss: 85.6833 - mean_squared_error: 85.6833\n",
      "Epoch 733/5000\n",
      "506/506 [==============================] - 0s 63us/step - loss: 85.1485 - mean_squared_error: 85.1485\n",
      "Epoch 734/5000\n",
      "506/506 [==============================] - 0s 62us/step - loss: 84.9510 - mean_squared_error: 84.9510\n",
      "Epoch 735/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 84.6512 - mean_squared_error: 84.6512\n",
      "Epoch 736/5000\n",
      "506/506 [==============================] - 0s 60us/step - loss: 85.2903 - mean_squared_error: 85.2903\n",
      "Epoch 737/5000\n",
      "506/506 [==============================] - 0s 70us/step - loss: 85.5808 - mean_squared_error: 85.5808\n",
      "Epoch 738/5000\n",
      "506/506 [==============================] - 0s 71us/step - loss: 84.7466 - mean_squared_error: 84.7466\n",
      "Epoch 739/5000\n",
      "506/506 [==============================] - 0s 46us/step - loss: 84.8433 - mean_squared_error: 84.8433\n",
      "Epoch 740/5000\n",
      "506/506 [==============================] - 0s 58us/step - loss: 85.7025 - mean_squared_error: 85.7025\n",
      "Epoch 741/5000\n",
      "506/506 [==============================] - 0s 58us/step - loss: 84.5990 - mean_squared_error: 84.5990\n",
      "Epoch 742/5000\n",
      "506/506 [==============================] - 0s 68us/step - loss: 85.2569 - mean_squared_error: 85.2569\n",
      "Epoch 743/5000\n",
      "506/506 [==============================] - 0s 71us/step - loss: 84.9941 - mean_squared_error: 84.9941\n",
      "Epoch 744/5000\n",
      "506/506 [==============================] - 0s 60us/step - loss: 85.0077 - mean_squared_error: 85.0077\n",
      "Epoch 745/5000\n",
      "506/506 [==============================] - 0s 60us/step - loss: 85.1743 - mean_squared_error: 85.1743\n",
      "Epoch 746/5000\n",
      "506/506 [==============================] - 0s 68us/step - loss: 85.3058 - mean_squared_error: 85.3058\n",
      "Epoch 747/5000\n",
      "506/506 [==============================] - 0s 59us/step - loss: 84.8694 - mean_squared_error: 84.8694\n",
      "Epoch 748/5000\n",
      "506/506 [==============================] - 0s 63us/step - loss: 85.0664 - mean_squared_error: 85.0664\n",
      "Epoch 749/5000\n",
      "506/506 [==============================] - 0s 63us/step - loss: 85.3977 - mean_squared_error: 85.3977\n",
      "Epoch 750/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 84.7909 - mean_squared_error: 84.7909\n",
      "Epoch 751/5000\n",
      "506/506 [==============================] - 0s 66us/step - loss: 85.1910 - mean_squared_error: 85.1910\n",
      "Epoch 752/5000\n",
      "506/506 [==============================] - 0s 84us/step - loss: 84.8493 - mean_squared_error: 84.8493\n",
      "Epoch 753/5000\n",
      "506/506 [==============================] - 0s 66us/step - loss: 85.0491 - mean_squared_error: 85.0491\n",
      "Epoch 754/5000\n",
      "506/506 [==============================] - 0s 77us/step - loss: 84.9392 - mean_squared_error: 84.9392\n",
      "Epoch 755/5000\n",
      "506/506 [==============================] - 0s 59us/step - loss: 84.9526 - mean_squared_error: 84.9526\n",
      "Epoch 756/5000\n",
      "506/506 [==============================] - 0s 65us/step - loss: 84.9914 - mean_squared_error: 84.9914\n",
      "Epoch 757/5000\n",
      "506/506 [==============================] - 0s 64us/step - loss: 85.0195 - mean_squared_error: 85.0195\n",
      "Epoch 758/5000\n",
      "506/506 [==============================] - 0s 75us/step - loss: 85.1756 - mean_squared_error: 85.1756\n",
      "Epoch 759/5000\n",
      "506/506 [==============================] - 0s 77us/step - loss: 85.4699 - mean_squared_error: 85.4699\n",
      "Epoch 760/5000\n",
      "506/506 [==============================] - 0s 74us/step - loss: 85.7663 - mean_squared_error: 85.7663\n",
      "Epoch 761/5000\n",
      "506/506 [==============================] - 0s 65us/step - loss: 84.6640 - mean_squared_error: 84.6640\n",
      "Epoch 762/5000\n",
      "506/506 [==============================] - 0s 64us/step - loss: 84.7949 - mean_squared_error: 84.7949\n",
      "Epoch 763/5000\n",
      "506/506 [==============================] - 0s 69us/step - loss: 85.4048 - mean_squared_error: 85.4048\n",
      "Epoch 764/5000\n",
      "506/506 [==============================] - 0s 71us/step - loss: 85.5702 - mean_squared_error: 85.5702\n",
      "Epoch 765/5000\n",
      "506/506 [==============================] - 0s 60us/step - loss: 84.9354 - mean_squared_error: 84.9354\n",
      "Epoch 766/5000\n",
      "506/506 [==============================] - 0s 69us/step - loss: 85.5109 - mean_squared_error: 85.5109\n",
      "Epoch 767/5000\n",
      "506/506 [==============================] - 0s 60us/step - loss: 85.0776 - mean_squared_error: 85.0776\n",
      "Epoch 768/5000\n",
      "506/506 [==============================] - 0s 50us/step - loss: 85.0058 - mean_squared_error: 85.0058\n",
      "Epoch 769/5000\n",
      "506/506 [==============================] - 0s 42us/step - loss: 85.2969 - mean_squared_error: 85.2969\n",
      "Epoch 770/5000\n",
      "506/506 [==============================] - 0s 55us/step - loss: 84.8832 - mean_squared_error: 84.8832\n",
      "Epoch 771/5000\n",
      "506/506 [==============================] - 0s 52us/step - loss: 84.8576 - mean_squared_error: 84.8576\n",
      "Epoch 772/5000\n",
      "506/506 [==============================] - 0s 70us/step - loss: 84.9224 - mean_squared_error: 84.9224\n",
      "Epoch 773/5000\n",
      "506/506 [==============================] - 0s 55us/step - loss: 84.8505 - mean_squared_error: 84.8505\n",
      "Epoch 774/5000\n",
      "506/506 [==============================] - 0s 53us/step - loss: 85.3900 - mean_squared_error: 85.3900\n",
      "Epoch 775/5000\n",
      "506/506 [==============================] - 0s 55us/step - loss: 85.3227 - mean_squared_error: 85.3227\n",
      "Epoch 776/5000\n",
      "506/506 [==============================] - 0s 48us/step - loss: 84.9115 - mean_squared_error: 84.9115\n",
      "Epoch 777/5000\n",
      "506/506 [==============================] - 0s 52us/step - loss: 85.5220 - mean_squared_error: 85.5220\n",
      "Epoch 778/5000\n",
      "506/506 [==============================] - 0s 47us/step - loss: 85.1809 - mean_squared_error: 85.1809\n",
      "Epoch 779/5000\n",
      "506/506 [==============================] - 0s 56us/step - loss: 84.8795 - mean_squared_error: 84.8795\n",
      "Epoch 780/5000\n",
      "506/506 [==============================] - 0s 38us/step - loss: 84.4430 - mean_squared_error: 84.4430\n",
      "Epoch 781/5000\n",
      "506/506 [==============================] - 0s 47us/step - loss: 84.8322 - mean_squared_error: 84.8322\n",
      "Epoch 782/5000\n",
      "506/506 [==============================] - 0s 52us/step - loss: 85.0883 - mean_squared_error: 85.0883\n",
      "Epoch 783/5000\n",
      "506/506 [==============================] - 0s 56us/step - loss: 85.2008 - mean_squared_error: 85.2008\n",
      "Epoch 784/5000\n",
      "506/506 [==============================] - 0s 79us/step - loss: 85.4426 - mean_squared_error: 85.4426\n",
      "Epoch 785/5000\n",
      "506/506 [==============================] - 0s 68us/step - loss: 85.1575 - mean_squared_error: 85.1575\n",
      "Epoch 786/5000\n",
      "506/506 [==============================] - 0s 58us/step - loss: 85.3949 - mean_squared_error: 85.3949\n",
      "Epoch 787/5000\n",
      "506/506 [==============================] - 0s 57us/step - loss: 84.7248 - mean_squared_error: 84.7248\n",
      "Epoch 788/5000\n",
      "506/506 [==============================] - 0s 69us/step - loss: 84.5165 - mean_squared_error: 84.5165\n",
      "Epoch 789/5000\n",
      "506/506 [==============================] - 0s 60us/step - loss: 85.8223 - mean_squared_error: 85.8223\n",
      "Epoch 790/5000\n",
      "506/506 [==============================] - 0s 77us/step - loss: 84.5613 - mean_squared_error: 84.5613\n",
      "Epoch 791/5000\n",
      "506/506 [==============================] - 0s 53us/step - loss: 85.0764 - mean_squared_error: 85.0764\n",
      "Epoch 792/5000\n",
      "506/506 [==============================] - 0s 64us/step - loss: 84.9440 - mean_squared_error: 84.9440\n",
      "Epoch 793/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 85.1307 - mean_squared_error: 85.1307\n",
      "Epoch 794/5000\n",
      "506/506 [==============================] - 0s 60us/step - loss: 85.3595 - mean_squared_error: 85.3595\n",
      "Epoch 795/5000\n",
      "506/506 [==============================] - 0s 70us/step - loss: 85.3420 - mean_squared_error: 85.3420\n",
      "Epoch 796/5000\n",
      "506/506 [==============================] - 0s 58us/step - loss: 85.2258 - mean_squared_error: 85.2258\n",
      "Epoch 797/5000\n",
      "506/506 [==============================] - 0s 58us/step - loss: 85.1122 - mean_squared_error: 85.1122\n",
      "Epoch 798/5000\n",
      "506/506 [==============================] - 0s 60us/step - loss: 84.6405 - mean_squared_error: 84.6405\n",
      "Epoch 799/5000\n",
      "506/506 [==============================] - 0s 62us/step - loss: 85.1045 - mean_squared_error: 85.1045\n",
      "Epoch 800/5000\n",
      "506/506 [==============================] - 0s 86us/step - loss: 84.7714 - mean_squared_error: 84.7714\n",
      "Epoch 801/5000\n",
      "506/506 [==============================] - 0s 63us/step - loss: 84.6239 - mean_squared_error: 84.6239\n",
      "Epoch 802/5000\n",
      "506/506 [==============================] - 0s 60us/step - loss: 85.2767 - mean_squared_error: 85.2767\n",
      "Epoch 803/5000\n",
      "506/506 [==============================] - 0s 64us/step - loss: 85.0245 - mean_squared_error: 85.0245\n",
      "Epoch 804/5000\n",
      "506/506 [==============================] - 0s 77us/step - loss: 85.4369 - mean_squared_error: 85.4369\n",
      "Epoch 805/5000\n",
      "506/506 [==============================] - 0s 56us/step - loss: 84.9814 - mean_squared_error: 84.9814\n",
      "Epoch 806/5000\n",
      "506/506 [==============================] - 0s 83us/step - loss: 84.7946 - mean_squared_error: 84.7946\n",
      "Epoch 807/5000\n",
      "506/506 [==============================] - 0s 65us/step - loss: 84.6439 - mean_squared_error: 84.6439\n",
      "Epoch 808/5000\n",
      "506/506 [==============================] - 0s 60us/step - loss: 85.3649 - mean_squared_error: 85.3649\n",
      "Epoch 809/5000\n",
      "506/506 [==============================] - 0s 68us/step - loss: 84.8043 - mean_squared_error: 84.8043\n",
      "Epoch 810/5000\n",
      "506/506 [==============================] - 0s 68us/step - loss: 85.3300 - mean_squared_error: 85.3300\n",
      "Epoch 811/5000\n",
      "506/506 [==============================] - 0s 75us/step - loss: 85.1808 - mean_squared_error: 85.1808\n",
      "Epoch 812/5000\n",
      "506/506 [==============================] - 0s 68us/step - loss: 85.2849 - mean_squared_error: 85.2849\n",
      "Epoch 813/5000\n",
      "506/506 [==============================] - 0s 67us/step - loss: 84.8549 - mean_squared_error: 84.8549\n",
      "Epoch 814/5000\n",
      "506/506 [==============================] - 0s 49us/step - loss: 84.2107 - mean_squared_error: 84.2107\n",
      "Epoch 815/5000\n",
      "506/506 [==============================] - 0s 66us/step - loss: 85.4359 - mean_squared_error: 85.4359\n",
      "Epoch 816/5000\n",
      "506/506 [==============================] - 0s 56us/step - loss: 84.6240 - mean_squared_error: 84.6240\n",
      "Epoch 817/5000\n",
      "506/506 [==============================] - 0s 72us/step - loss: 84.8233 - mean_squared_error: 84.8233\n",
      "Epoch 818/5000\n",
      "506/506 [==============================] - 0s 64us/step - loss: 84.7857 - mean_squared_error: 84.7857\n",
      "Epoch 819/5000\n",
      "506/506 [==============================] - 0s 48us/step - loss: 85.0566 - mean_squared_error: 85.0566\n",
      "Epoch 820/5000\n",
      "506/506 [==============================] - 0s 60us/step - loss: 84.9009 - mean_squared_error: 84.9009\n",
      "Epoch 821/5000\n",
      "506/506 [==============================] - 0s 62us/step - loss: 84.3690 - mean_squared_error: 84.3690\n",
      "Epoch 822/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 85.4110 - mean_squared_error: 85.4110\n",
      "Epoch 823/5000\n",
      "506/506 [==============================] - 0s 79us/step - loss: 85.1906 - mean_squared_error: 85.1906\n",
      "Epoch 824/5000\n",
      "506/506 [==============================] - 0s 75us/step - loss: 85.1292 - mean_squared_error: 85.1292\n",
      "Epoch 825/5000\n",
      "506/506 [==============================] - 0s 58us/step - loss: 85.2271 - mean_squared_error: 85.2271\n",
      "Epoch 826/5000\n",
      "506/506 [==============================] - 0s 63us/step - loss: 85.2493 - mean_squared_error: 85.2493\n",
      "Epoch 827/5000\n",
      "506/506 [==============================] - 0s 81us/step - loss: 85.1880 - mean_squared_error: 85.1880\n",
      "Epoch 828/5000\n",
      "506/506 [==============================] - 0s 60us/step - loss: 85.9106 - mean_squared_error: 85.9106\n",
      "Epoch 829/5000\n",
      "506/506 [==============================] - 0s 72us/step - loss: 85.0184 - mean_squared_error: 85.0184\n",
      "Epoch 830/5000\n",
      "506/506 [==============================] - 0s 66us/step - loss: 85.3175 - mean_squared_error: 85.3175\n",
      "Epoch 831/5000\n",
      "506/506 [==============================] - 0s 49us/step - loss: 84.9374 - mean_squared_error: 84.9374\n",
      "Epoch 832/5000\n",
      "506/506 [==============================] - 0s 45us/step - loss: 84.7521 - mean_squared_error: 84.7521\n",
      "Epoch 833/5000\n",
      "506/506 [==============================] - 0s 40us/step - loss: 85.9284 - mean_squared_error: 85.9284\n",
      "Epoch 834/5000\n",
      "506/506 [==============================] - 0s 43us/step - loss: 85.3400 - mean_squared_error: 85.3400\n",
      "Epoch 835/5000\n",
      "506/506 [==============================] - 0s 32us/step - loss: 84.6397 - mean_squared_error: 84.6397\n",
      "Epoch 836/5000\n",
      "506/506 [==============================] - 0s 53us/step - loss: 85.2071 - mean_squared_error: 85.2071\n",
      "Epoch 837/5000\n",
      "506/506 [==============================] - 0s 67us/step - loss: 85.5312 - mean_squared_error: 85.5312\n",
      "Epoch 838/5000\n",
      "506/506 [==============================] - 0s 57us/step - loss: 85.3524 - mean_squared_error: 85.3524\n",
      "Epoch 839/5000\n",
      "506/506 [==============================] - 0s 43us/step - loss: 85.5963 - mean_squared_error: 85.5963\n",
      "Epoch 840/5000\n",
      "506/506 [==============================] - 0s 49us/step - loss: 85.2281 - mean_squared_error: 85.2281\n",
      "Epoch 841/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 44us/step - loss: 85.5057 - mean_squared_error: 85.5057\n",
      "Epoch 842/5000\n",
      "506/506 [==============================] - 0s 69us/step - loss: 85.2541 - mean_squared_error: 85.2541\n",
      "Epoch 843/5000\n",
      "506/506 [==============================] - 0s 85us/step - loss: 85.0925 - mean_squared_error: 85.0925\n",
      "Epoch 844/5000\n",
      "506/506 [==============================] - 0s 59us/step - loss: 85.0701 - mean_squared_error: 85.0701\n",
      "Epoch 845/5000\n",
      "506/506 [==============================] - 0s 65us/step - loss: 85.4431 - mean_squared_error: 85.4431\n",
      "Epoch 846/5000\n",
      "506/506 [==============================] - 0s 52us/step - loss: 84.5982 - mean_squared_error: 84.5982\n",
      "Epoch 847/5000\n",
      "506/506 [==============================] - 0s 51us/step - loss: 85.2270 - mean_squared_error: 85.2270\n",
      "Epoch 848/5000\n",
      "506/506 [==============================] - 0s 82us/step - loss: 84.3814 - mean_squared_error: 84.3814\n",
      "Epoch 849/5000\n",
      "506/506 [==============================] - 0s 77us/step - loss: 85.6030 - mean_squared_error: 85.6030\n",
      "Epoch 850/5000\n",
      "506/506 [==============================] - 0s 63us/step - loss: 84.9207 - mean_squared_error: 84.9207\n",
      "Epoch 851/5000\n",
      "506/506 [==============================] - 0s 52us/step - loss: 85.1606 - mean_squared_error: 85.1606\n",
      "Epoch 852/5000\n",
      "506/506 [==============================] - 0s 49us/step - loss: 84.5196 - mean_squared_error: 84.5196\n",
      "Epoch 853/5000\n",
      "506/506 [==============================] - 0s 56us/step - loss: 85.0348 - mean_squared_error: 85.0348\n",
      "Epoch 854/5000\n",
      "506/506 [==============================] - 0s 58us/step - loss: 84.9431 - mean_squared_error: 84.9431\n",
      "Epoch 855/5000\n",
      "506/506 [==============================] - 0s 92us/step - loss: 85.7173 - mean_squared_error: 85.7173\n",
      "Epoch 856/5000\n",
      "506/506 [==============================] - 0s 67us/step - loss: 86.4319 - mean_squared_error: 86.4319\n",
      "Epoch 857/5000\n",
      "506/506 [==============================] - 0s 70us/step - loss: 84.0116 - mean_squared_error: 84.0116\n",
      "Epoch 858/5000\n",
      "506/506 [==============================] - 0s 74us/step - loss: 85.2035 - mean_squared_error: 85.2035\n",
      "Epoch 859/5000\n",
      "506/506 [==============================] - 0s 73us/step - loss: 85.5599 - mean_squared_error: 85.5599\n",
      "Epoch 860/5000\n",
      "506/506 [==============================] - 0s 68us/step - loss: 84.7681 - mean_squared_error: 84.7681\n",
      "Epoch 861/5000\n",
      "506/506 [==============================] - 0s 74us/step - loss: 85.4218 - mean_squared_error: 85.4218\n",
      "Epoch 862/5000\n",
      "506/506 [==============================] - 0s 58us/step - loss: 84.7874 - mean_squared_error: 84.7874\n",
      "Epoch 863/5000\n",
      "506/506 [==============================] - 0s 66us/step - loss: 84.9071 - mean_squared_error: 84.9071\n",
      "Epoch 864/5000\n",
      "506/506 [==============================] - 0s 56us/step - loss: 85.0126 - mean_squared_error: 85.0126\n",
      "Epoch 865/5000\n",
      "506/506 [==============================] - 0s 65us/step - loss: 85.3316 - mean_squared_error: 85.3316\n",
      "Epoch 866/5000\n",
      "506/506 [==============================] - 0s 66us/step - loss: 86.1034 - mean_squared_error: 86.1034\n",
      "Epoch 867/5000\n",
      "506/506 [==============================] - 0s 77us/step - loss: 84.9592 - mean_squared_error: 84.9592\n",
      "Epoch 868/5000\n",
      "506/506 [==============================] - 0s 65us/step - loss: 85.2852 - mean_squared_error: 85.2852\n",
      "Epoch 869/5000\n",
      "506/506 [==============================] - 0s 54us/step - loss: 85.0593 - mean_squared_error: 85.0593\n",
      "Epoch 870/5000\n",
      "506/506 [==============================] - 0s 61us/step - loss: 85.1429 - mean_squared_error: 85.1429\n",
      "Epoch 871/5000\n",
      " 32/506 [>.............................] - ETA: 0s - loss: 121.2959 - mean_squared_error: 121.2959"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-46d3f58551a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_, y_, epochs=5000, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Kaggle Toxic Text Classification Example Learnboard](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/leaderboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference1: https://www.kaggle.com/eashish/bidirectional-gru-with-convolution\n",
    "\n",
    "Keras - Bidirectional LSTM baseline ( lb 0.069)\n",
    "\n",
    "Reference2:  https://www.kaggle.com/yekenot/textcnn-2d-convolution\n",
    "\n",
    "TextCNN 2D Convolution\n",
    "\n",
    "Reference3:  https://www.kaggle.com/ogrellier/wordbatch-fm-ftrl-using-mse-lb-0-9804\n",
    "\n",
    "FM_FTRL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 激活函数表达式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Softmax: $\\frac{1}{1+e^{-x}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Relu:\n",
    "$$ f(x)=\\left\\{\n",
    "\\begin{aligned}\n",
    "0 & & x<0 \\\\\n",
    "x & & x>=0\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Leaky Relu: max(0.1x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.tanh: tanh(x) = $\\frac{sinh(x)}{cosh(x)}$ = $\\frac{e^x - e^{-x}}{e^x + e^{-x}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.Swish: $\\frac{x}{1+e^{-x}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 目前Swish表现最好，Softmax其次，Relu最快"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: \n",
    "- [一文概览深度学习中的激活函数](https://www.jiqizhixin.com/articles/2017-11-02-26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [什么是多层感知器和反向传播？](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650720758&idx=1&sn=3004c425e0d427f4900a182d74bed31d&chksm=871b0d88b06c849e951469ae1ed54e5f66074d6322eb6681c85727bb8199154709c04c48c034&scene=21#wechat_redirect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化方法比较"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Karpathy做了一个这几个方法在MNIST上性能的比较，其结论是： \n",
    "- adagrad相比于sgd和momentum更加稳定，即不需要怎么调参。\n",
    "- 而精调的sgd和momentum系列方法无论是收敛速度还是precision都比adagrad要好一些。\n",
    "- 在精调参数下，一般Nesterov优于momentum优于sgd。\n",
    "- 而adagrad一方面不用怎么调参，另一方面其性能稳定优于其他方法。\n",
    "\n",
    "> 总之，Adagrad和Nesterov最好\n",
    "\n",
    "[Reference: 优化器比较](https://blog.csdn.net/blue_jjw/article/details/50650248)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
